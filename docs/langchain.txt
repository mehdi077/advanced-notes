### Install Core LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/install

Installs the main LangChain.js package and the core utilities. This is the primary package needed to start using LangChain.js.

```bash
npm install langchain @langchain/core
```

```bash
pnpm add langchain @langchain/core
```

```bash
yarn add langchain @langchain/core
```

```bash
bun add langchain @langchain/core
```

--------------------------------

### Create and Run Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Assembles the agent by combining the configured model, system prompt, and other components. This snippet shows the initial setup for agent creation.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  systemPrompt: systemPrompt,

```

--------------------------------

### Start Agent Server using Langgraph Dev

Source: https://docs.langchain.com/oss/javascript/langchain/studio

This command starts the Langchain Agent server for development. It allows you to view your agent in the Studio UI. Note that Safari may block localhost connections, requiring the use of the `--tunnel` flag for a secure connection.

```shell
langgraph dev

```

```shell
langgraph dev --tunnel

```

--------------------------------

### Install LangGraph CLI with In-Memory Support

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Installs the LangGraph CLI with in-memory support, which is necessary for setting up a local agent server. Python version 3.11 or higher is required for this installation. This command is executed in a shell environment.

```shell
pip install --upgrade "langgraph-cli[inmem]"
```

--------------------------------

### LangGraph Configuration File for Agent Setup

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Defines the configuration for a LangGraph application in a `langgraph.json` file. This JSON specifies project dependencies, lists the available graphs (linking the agent definition from `src/agent.py`), and points to the environment file (`.env`) for configuration settings. This file is used by the LangGraph CLI to set up and run the agent.

```json
{
  "dependencies": ["ப்பான."],
  "graphs": {
    "agent": "./src/agent.py:agent"
  },
  "env": ".env"
}
```

--------------------------------

### Create and Run Agent Chat UI Locally (Bash)

Source: https://docs.langchain.com/oss/javascript/langchain/ui

This snippet demonstrates how to create a new Agent Chat UI project using npx, navigate into the project directory, install dependencies, and start the development server. It's a quick way to set up a local instance for customization or testing.

```bash
# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```

--------------------------------

### Example: Using JSON Schema for Tool Input (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

An alternative way to define tool input schemas using JSON schema objects. Note that JSON schemas are not validated at runtime.

```typescript
const getWeather = tool(
  ({ city }) => `It's always sunny in ${city}!`,
  {
    name: "get_weather_for_location",
    description: "Get the weather for a given city",
    schema: {
      type: "object",
      properties: {
        city: {
          type: "string",
          description: "The city to get the weather for"
        }
      },
      required: ["city"]
    },
  }
);
```

--------------------------------

### Install LangChain.js and dependencies for SQL agent

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the necessary LangChain.js packages, TypeORM, SQLite3, and Zod for building the SQL agent. This setup is crucial for database interaction and data validation.

```bash
npm i langchain @langchain/core typeorm sqlite3 zod
```

```bash
yarn add langchain @langchain/core typeorm sqlite3 zod
```

```bash
pnpm add langchain @langchain/core typeorm sqlite3 zod
```

--------------------------------

### Install Project Dependencies using Pip

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Installs the project's Python dependencies using pip. The `-e` flag installs the package in editable mode, meaning changes to the source code will be reflected immediately without needing to reinstall. This command is typically run from the root of the LangGraph application directory.

```shell
pip install -e .
```

--------------------------------

### LangChain.js AI Agent with Tools and Memory

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

This TypeScript code sets up an AI agent using LangChain.js. It defines tools for getting weather and user location, configures a chat model, and specifies a structured response format. The agent is initialized with a system prompt, tools, response format, and a memory checkpointer. It then demonstrates invoking the agent for a weather query and a follow-up 'thank you' message, showcasing conversation context and tool usage.

```typescript
import { createAgent, tool, initChatModel } from "langchain";
import { MemorySaver, type Runtime } from "@langchain/langgraph";
import * as z from "zod";

// Define system prompt
const systemPrompt = `You are an expert weather forecaster, who speaks in puns.

  You have access to two tools:

  - get_weather_for_location: use this to get the weather for a specific location
  - get_user_location: use this to get the user's location

  If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;

// Define tools
const getWeather = tool(
  ({ city }) => `It's always sunny in ${city}!`,
  {
    name: "get_weather_for_location",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string(),
    }),
  }
);

const getUserLocation = tool(
  (_, config: Runtime<{ user_id: string}>) => {
    const { user_id } = config.context;
    return user_id === "1" ? "Florida" : "SF";
  },
  {
    name: "get_user_location",
    description: "Retrieve user information based on user ID",
    schema: z.object({}),
  }
);

// Configure model
const model = await initChatModel(
  "claude-sonnet-4-5-20250929",
  { temperature: 0 }
);

// Define response format
const responseFormat = z.object({
  punny_response: z.string(),
  weather_conditions: z.string().optional(),
});

// Set up memory
const checkpointer = new MemorySaver();

// Create agent
const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  systemPrompt: systemPrompt,
  tools: [getUserLocation, getWeather],
  responseFormat,
  checkpointer,
});

// Run agent
// `thread_id` is a unique identifier for a given conversation.
const config = {
  configurable: { thread_id: "1" },
  context: { user_id: "1" },
};

const response = await agent.invoke(
  { messages: [{ role: "user", content: "what is the weather outside?" }] },
  config
);
console.log(response.structuredResponse);
// {
//   punny_response: "Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!",
//   weather_conditions: "It's always sunny in Florida!"
// }

// Note that we can continue the conversation using the same `thread_id`.
const thankYouResponse = await agent.invoke(
  { messages: [{ role: "user", content: "thank you!" }] },
  config
);
console.log(thankYouResponse.structuredResponse);
// {
//   punny_response: "You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!",
//   weather_conditions: undefined
// }
```

--------------------------------

### Install Langchain.js Package

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Installs the necessary 'langchain' package for building Langchain.js applications. Supports installation via npm, yarn, or pnpm.

```bash
npm install langchain
```

```bash
yarn add langchain
```

```bash
pnpm add langchain
```

--------------------------------

### Extended Agentic RAG Example for LangGraph Docs (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/retrieval

This extended example shows a more complex Agentic RAG setup for querying LangGraph documentation. It includes a custom `fetchDocumentation` tool with domain restrictions and a detailed system prompt to guide the agent. The example fetches content from `llms.txt` and then invokes the agent with a user's question.

```typescript
import { tool, createAgent, HumanMessage } from "langchain";
import * as z from "zod";

const ALLOWED_DOMAINS = ["https://langchain-ai.github.io/"];
const LLMS_TXT = "https://langchain-ai.github.io/langgraph/llms.txt";

const fetchDocumentation = tool(
    async (input) => {
      if (!ALLOWED_DOMAINS.some((domain) => input.url.startsWith(domain))) {
        return `Error: URL not allowed. Must start with one of: ${ALLOWED_DOMAINS.join(", ")}`;
      }
      const response = await fetch(input.url);
      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }
      return response.text();
    },
    {
      name: "fetch_documentation",
      description: "Fetch and convert documentation from a URL",
      schema: z.object({
        url: z.string().describe("The URL of the documentation to fetch"),
      }),
    }
  );

const llmsTxtResponse = await fetch(LLMS_TXT);
const llmsTxtContent = await llmsTxtResponse.text();

const systemPrompt = `
  You are an expert TypeScript developer and technical assistant.
  Your primary role is to help users with questions about LangGraph and related tools.

  Instructions:

  1. If a user asks a question you're unsure about — or one that likely involves API usage,
     behavior, or configuration — you MUST use the `fetch_documentation` tool to consult the relevant docs.
  2. When citing documentation, summarize clearly and include relevant context from the content.
  3. Do not use any URLs outside of the allowed domain.
  4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.

  You can access official documentation from the following approved sources:

  ${llmsTxtContent}

  You MUST consult the documentation to get up to date documentation
  before answering a user's question about LangGraph.

  Your answers should be clear, concise, and technically accurate.
  `;

const tools = [fetchDocumentation];

const agent = createAgent({
    model: "claude-sonnet-4-0",
    tools,
    systemPrompt,
    name: "Agentic RAG",
  });

const response = await agent.invoke({
    messages: [
      new HumanMessage(
        "Write a short example of a langgraph agent using the " +
        "prebuilt create react agent. the agent should be able " +
        "to look up stock pricing information."
      ),
    ],
  });

console.log(response.messages.at(-1)?.content);
```

--------------------------------

### Synchronize Project Dependencies using Uv

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Synchronizes project dependencies using the `uv` tool. This command ensures that the installed packages match the project's requirements, potentially offering faster dependency management than pip. It is executed from the root of the LangGraph application directory.

```shell
uv sync
```

--------------------------------

### Clone and Run Agent Chat UI Locally (Bash)

Source: https://docs.langchain.com/oss/javascript/langchain/ui

This snippet shows how to clone the Agent Chat UI repository from GitHub, navigate into the cloned directory, install project dependencies using pnpm, and then start the development server. This method is useful for developers who want to work directly with the source code.

```bash
# Clone the repository
git clone https://github.com/langchain-ai/agent-chat-ui.git
cd agent-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```

--------------------------------

### Install @modelcontextprotocol/sdk

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Install the @modelcontextprotocol/sdk library using your preferred package manager (npm, pnpm, yarn, or bun). This is the foundational step for building MCP servers.

```bash
npm install @modelcontextprotocol/sdk
```

```bash
pnpm add @modelcontextprotocol/sdk
```

```bash
yarn add @modelcontextprotocol/sdk
```

```bash
bun add @modelcontextprotocol/sdk
```

--------------------------------

### Tool Execution Loop with Model and Tools

Source: https://docs.langchain.com/oss/javascript/langchain/models

This example illustrates the manual tool execution loop. It shows how to get tool calls from a model, execute the requested tool, and then pass the tool's results back to the model to generate a final response. This pattern is crucial when not using an agent.

```typescript
// Bind (potentially multiple) tools to the model
const modelWithTools = model.bindTools([get_weather])

// Step 1: Model generates tool calls
const messages = [{"role": "user", "content": "What's the weather in Boston?"}]
const ai_msg = await modelWithTools.invoke(messages)
messages.push(ai_msg)

// Step 2: Execute tools and collect results
for (const tool_call of ai_msg.tool_calls) {
    // Execute the tool with the generated arguments
    const tool_result = await get_weather.invoke(tool_call)
    messages.push(tool_result)
}

// Step 3: Pass results back to model for final response
const final_response = await modelWithTools.invoke(messages)
console.log(final_response.text)
// "The current weather in Boston is 72°F and sunny."
```

--------------------------------

### Install Pinecone Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/pinecone` package for integrating with Pinecone, a managed vector database service. Also requires the `@pinecone-database/pinecone` client library.

```bash
npm i @langchain/pinecone
```

```bash
yarn add @langchain/pinecone
```

```bash
pnpm add @langchain/pinecone
```

--------------------------------

### Install Chroma Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/community` package for integrating with the Chroma vector store. Chroma is a popular open-source vector database.

```bash
npm i @langchain/community
```

```bash
yarn add @langchain/community
```

```bash
pnpm add @langchain/community
```

--------------------------------

### Install Memory Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/classic` package required for the MemoryVectorStore. This is a lightweight, in-memory solution suitable for testing or small workloads.

```bash
npm i @langchain/classic
```

```bash
yarn add @langchain/classic
```

```bash
pnpm add @langchain/classic
```

--------------------------------

### Create Tools for Weather Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Implements tools for retrieving weather information and user location. It utilizes Zod for input schema validation and demonstrates runtime configuration for dynamic behavior.

```typescript
import { type Runtime } from "@langchain/langgraph";
import { tool } from "langchain";
import * as z from "zod";

const getWeather = tool(
  (input) => `It's always sunny in ${input.city}!`,
  {
    name: "get_weather_for_location",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string().describe("The city to get the weather for"),
    }),
  }
);

type AgentRuntime = Runtime<{ user_id: string }>;

const getUserLocation = tool(
  (_, config: AgentRuntime) => {
    const { user_id } = config.context;
    return user_id === "1" ? "Florida" : "SF";
  },
  {
    name: "get_user_location",
    description: "Retrieve user information based on user ID",
  }
);
```

--------------------------------

### Install Qdrant Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/qdrant` package for Qdrant vector store integration. Qdrant is a popular open-source vector similarity search engine.

```bash
npm i @langchain/qdrant
```

```bash
yarn add @langchain/qdrant
```

```bash
pnpm add @langchain/qdrant
```

--------------------------------

### Configure Language Model for Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Initializes a chat model with specific parameters like temperature, timeout, and max tokens for consistent and controlled responses.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel(
  "claude-sonnet-4-5-20250929",
  { temperature: 0.5, timeout: 10, maxTokens: 1000 }
);
```

--------------------------------

### Install @langchain/mcp-adapters

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Installs the @langchain/mcp-adapters library using various package managers (npm, pnpm, yarn, bun). This library is necessary for integrating MCP tools with LangChain agents.

```bash
npm install @langchain/mcp-adapters
```

```bash
pnpm add @langchain/mcp-adapters
```

```bash
yarn add @langchain/mcp-adapters
```

```bash
bun add @langchain/mcp-adapters
```

--------------------------------

### Install and Configure VertexAI Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/google-vertexai` package and sets the `GOOGLE_APPLICATION_CREDENTIALS` environment variable for VertexAI embeddings.

```bash
npm i @langchain/google-vertexai
yarn add @langchain/google-vertexai
pnpm add @langchain/google-vertexai
```

```bash
GOOGLE_APPLICATION_CREDENTIALS=credentials.json
```

--------------------------------

### Install Azure Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the Azure package for Langchain.js, supporting integration with Azure OpenAI services. Available for npm, pnpm, yarn, and bun.

```bash
npm install @langchain/azure
```

```bash
pnpm install @langchain/azure
```

```bash
yarn add @langchain/azure
```

```bash
bun add @langchain/azure
```

--------------------------------

### Install and Use Chroma Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/community` package and initializes a Chroma vector store. Chroma is a popular vector database that can be run locally or in a client-server mode.

```bash
npm i @langchain/community
yarn add @langchain/community
pnpm add @langchain/community
```

```typescript
import { Chroma } from "@langchain/community/vectorstores/chroma";

const vectorStore = new Chroma(embeddings, {

```

--------------------------------

### Install and Configure MistralAI Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/mistralai` package and sets the `MISTRAL_API_KEY` environment variable for MistralAI embeddings.

```bash
npm i @langchain/mistralai
yarn add @langchain/mistralai
pnpm add @langchain/mistralai
```

```bash
MISTRAL_API_KEY=your-api-key
```

--------------------------------

### Define System Prompt for Weather Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Defines the agent's role and behavior as an expert weather forecaster who speaks in puns. It specifies the available tools and how to handle location information.

```typescript
const systemPrompt = `You are an expert weather forecaster, who speaks in puns.\n\nYou have access to two tools:\n\n- get_weather_for_location: use this to get the weather for a specific location\n- get_user_location: use this to get the user's location\n\nIf a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get_user_location tool to find their location.`;
```

--------------------------------

### Install LangChain Community and PDF Parsing Packages

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary LangChain community package and the pdf-parse library for handling PDF documents. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/community pdf-parse
```

```bash
yarn add @langchain/community pdf-parse
```

```bash
pnpm add @langchain/community pdf-parse
```

--------------------------------

### Install LangChain.js Integrations

Source: https://docs.langchain.com/oss/javascript/langchain/install

Installs specific provider integration packages for LangChain.js, such as OpenAI or Anthropic. These packages extend LangChain's capabilities by connecting to different LLM providers and tools.

```bash
# Installing the OpenAI integration
npm install @langchain/openai
# Installing the Anthropic integration
npm install @langchain/anthropic
```

```bash
# Installing the OpenAI integration
pnpm install @langchain/openai
# Installing the Anthropic integration
pnpm install @langchain/anthropic
```

```bash
# Installing the OpenAI integration
yarn add @langchain/openai
# Installing the Anthropic integration
yarn add @langchain/anthropic
```

```bash
# Installing the OpenAI integration
bun add @langchain/openai
# Installing the Anthropic integration
bun add @langchain/anthropic
```

--------------------------------

### Install and Configure Cohere Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/cohere` package and sets the `COHERE_API_KEY` environment variable for Cohere embeddings.

```bash
npm i @langchain/cohere
yarn add @langchain/cohere
pnpm add @langchain/cohere
```

```bash
COHERE_API_KEY=your-api-key
```

--------------------------------

### Quick Start: LangChain Agent with Automatic LangSmith Tracing

Source: https://docs.langchain.com/oss/javascript/langchain/observability

This TypeScript example demonstrates creating a LangChain agent with 'create_agent' and running an invocation. All steps, including tool calls and model interactions, are automatically traced to LangSmith without additional code.

```typescript
import { createAgent } from "@langchain/agents";

function sendEmail(to: string, subject: string, body: string): string {
    // ... email sending logic
    return `Email sent to ${to}`;
}

function searchWeb(query: string): string {
    // ... web search logic
    return `Search results for: ${query}`;
}

const agent = createAgent({
    model: "gpt-4o",
    tools: [sendEmail, searchWeb],
    systemPrompt: "You are a helpful assistant that can send emails and search the web."
});

// Run the agent - all steps will be traced automatically
const response = await agent.invoke({
    messages: [{ role: "user", content: "Search for the latest AI news and email a summary to john@example.com" }]
});
```

--------------------------------

### ReAct Loop Example: Tool Use in Agent

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Illustrates the ReAct (Reasoning + Acting) pattern where an agent alternates between reasoning and tool calls to fulfill a user request. This example shows how an agent identifies popular headphones, checks inventory, and provides a final answer.

```text
================================ Human Message =================================

Find the most popular wireless headphones right now and check if they're in stock

================================== Ai Message ==================================
Tool Calls:
  search_products (call_abc123)
 Call ID: call_abc123
  Args:
    query: wireless headphones

================================= Tool Message =================================

Found 5 products matching "wireless headphones". Top 5 results: WH-1000XM5, ...

================================== Ai Message ==================================
Tool Calls:
  check_inventory (call_def456)
 Call ID: call_def456
  Args:
    product_id: WH-1000XM5

================================= Tool Message =================================

Product WH-1000XM5: 10 units in stock

================================== Ai Message ==================================

I found wireless headphones (model WH-1000XM5) with 10 units in stock...
```

--------------------------------

### Install Langgraph CLI

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the latest version of the Langgraph command-line interface globally. This is a prerequisite for running agents within the Langgraph environment.

```shell
npm i -g langgraph-cli@latest
```

--------------------------------

### Add Memory to Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Initializes a MemorySaver for managing conversational state. For production environments, a persistent checkpointer that saves to a database is recommended.

```typescript
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();
```

--------------------------------

### Extended Example: GitHub vs GitLab Tool Selection in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This extended example shows how to dynamically select between GitHub and GitLab tools based on runtime context. It defines specific tools for creating issues in each platform and uses middleware with `contextSchema` to filter the available tools. The agent is then invoked with a specific provider context to demonstrate the selection.

```typescript
import * as z from "zod";
import { createAgent, createMiddleware, tool, HumanMessage } from "langchain";

const githubCreateIssue = tool(
  async ({ repo, title }) => ({
    url: `https://github.com/${repo}/issues/1`,
    title,
  }),
  {
    name: "github_create_issue",
    description: "Create an issue in a GitHub repository",
    schema: z.object({ repo: z.string(), title: z.string() }),
  }
);

const gitlabCreateIssue = tool(
  async ({ project, title }) => ({
    url: `https://gitlab.com/${project}/-/issues/1`,
    title,
  }),
  {
    name: "gitlab_create_issue",
    description: "Create an issue in a GitLab project",
    schema: z.object({ project: z.string(), title: z.string() }),
  }
);

const allTools = [githubCreateIssue, gitlabCreateIssue];

const toolSelector = createMiddleware({
  name: "toolSelector",
  contextSchema: z.object({ provider: z.enum(["github", "gitlab"]) }),
  wrapModelCall: (request, handler) => {
    const provider = request.runtime.context.provider;
    const toolName = provider === "gitlab" ? "gitlab_create_issue" : "github_create_issue";
    const selectedTools = request.tools.filter((t) => t.name === toolName);
    const modifiedRequest = { ...request, tools: selectedTools };
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: allTools,
  middleware: [toolSelector],
});

// Invoke with GitHub context
await agent.invoke(
  {
    messages: [
      new HumanMessage("Open an issue titled 'Bug: where are the cats' in the repository `its-a-cats-game`"),
    ],
  },
  {
    context: { provider: "github" },
  }
);
```

--------------------------------

### Install Google GenAI Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the Google Generative AI package for Langchain.js using npm, pnpm, yarn, or bun. This enables integration with Google's Gemini models.

```bash
npm install @langchain/google-genai
```

```bash
pnpm install @langchain/google-genai
```

```bash
yarn add @langchain/google-genai
```

```bash
bun add @langchain/google-genai
```

--------------------------------

### Install Anthropic Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the Anthropic package for Langchain.js using common package managers (npm, pnpm, yarn, bun). This enables integration with Anthropic's chat models.

```bash
npm install @langchain/anthropic
```

```bash
pnpm install @langchain/anthropic
```

```bash
yarn add @langchain/anthropic
```

```bash
pnpm add @langchain/anthropic
```

--------------------------------

### System Message: Basic Instructions (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Example of creating a SystemMessage with basic instructions to prime the model's behavior. This is used to set the tone or provide initial context for the assistant.

```typescript
import { SystemMessage, HumanMessage, AIMessage } from "langchain";

const systemMsg = new SystemMessage("You are a helpful coding assistant.");

const messages = [
  systemMsg,
  new HumanMessage("How do I create a REST API?"),
];
const response = await model.invoke(messages);
```

--------------------------------

### Install and Use Memory Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/classic` package and initializes a `MemoryVectorStore`. This vector store operates in memory and is suitable for smaller datasets or testing.

```bash
npm i @langchain/classic
yarn add @langchain/classic
pnpm add @langchain/classic
```

```typescript
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);
```

--------------------------------

### Install OpenAI Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using OpenAI embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/openai
```

```bash
yarn add @langchain/openai
```

```bash
pnpm add @langchain/openai
```

--------------------------------

### Install AgentEvals and LangChain Core Packages

Source: https://docs.langchain.com/oss/javascript/langchain/test

Installs the AgentEvals package and the LangChain Core library using npm. These are essential dependencies for performing integration tests on agentic applications.

```bash
npm install agentevals @langchain/core
```

--------------------------------

### Inject User Writing Style Middleware for Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware injects a user's email writing style from a store into the LLM's prompt. It requires a `userId` from the runtime context and reads writing style preferences (tone, greeting, sign-off, example email) from a store. The retrieved style is formatted into a user message to guide the LLM's response generation, particularly for drafting emails. Dependencies include 'langchain' and 'zod'.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const injectWritingStyle = createMiddleware({
  name: "InjectWritingStyle",
  contextSchema,
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's writing style examples
    const store = request.runtime.store;  // [!code highlight]
    const writingStyle = await store.get(["writing_style"], userId);  // [!code highlight]

    if (writingStyle) {
      const style = writingStyle.value;
      // Build style guide from stored examples
      const styleContext = `Your writing style:
    - Tone: ${style.tone || 'professional'}
    - Typical greeting: "${style.greeting || 'Hi'}"
    - Typical sign-off: "${style.signOff || 'Best'}"
    - Example email you've written:
    ${style.exampleEmail || ''}`;

      // Append at end - models pay more attention to final messages
      const messages = [
        ...request.messages,
        { role: "user", content: styleContext }
      ];
      request = request.override({ messages });  // [!code highlight]
    }

    return handler(request);
  },
});
```

--------------------------------

### Streaming Semantic Events with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates using `streamEvents()` to stream semantic events from Langchain chat models. This method simplifies filtering by event type and aggregates the full message. The example logs the start input, individual text tokens as they are streamed, and the final complete message output.

```typescript
const stream = await model.streamEvents("Hello");
for await (const event of stream) {
    if (event.event === "on_chat_model_start") {
        console.log(`Input: ${event.data.input}`);
    }
    if (event.event === "on_chat_model_stream") {
        console.log(`Token: ${event.data.chunk.text}`);
    }
    if (event.event === "on_chat_model_end") {
        console.log(`Full message: ${event.data.output.text}`);
    }
}
```

--------------------------------

### Install OpenAI Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the OpenAI package for Langchain.js using various package managers like npm, pnpm, yarn, and bun. This package is required to integrate OpenAI chat models.

```bash
npm install @langchain/openai
```

```bash
pnpm install @langchain/openai
```

```bash
yarn add @langchain/openai
```

```bash
bun add @langchain/openai
```

--------------------------------

### Install and Configure AWS Bedrock Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/aws` package and sets the `BEDROCK_AWS_REGION` environment variable for AWS Bedrock embeddings.

```bash
npm i @langchain/aws
yarn add @langchain/aws
pnpm add @langchain/aws
```

```bash
BEDROCK_AWS_REGION=your-region
```

--------------------------------

### Install and Configure Azure OpenAI Embeddings

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Installs the `@langchain/openai` package and sets up environment variables for Azure OpenAI embeddings. Requires `AZURE_OPENAI_API_INSTANCE_NAME`, `AZURE_OPENAI_API_KEY`, and `AZURE_OPENAI_API_VERSION` to be set.

```bash
npm i @langchain/openai
yarn add @langchain/openai
pnpm add @langchain/openai
```

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

--------------------------------

### Install MistralAI Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using MistralAI embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/mistralai
```

```bash
yarn add @langchain/mistralai
```

```bash
pnpm add @langchain/mistralai
```

--------------------------------

### Implementing Dynamic System Prompts with Middleware

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Shows how to dynamically adjust the system prompt at runtime using middleware. This example uses Zod for schema validation and a middleware function to tailor the prompt based on the 'userRole' context, providing different instructions for 'expert' and 'beginner' users.

```typescript
import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.enum(["expert", "beginner"]),
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [/* ... */],
  contextSchema,
  middleware: [
    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {
      const userRole = runtime.context.userRole || "user";
      const basePrompt = "You are a helpful assistant.";

      if (userRole === "expert") {
        return `${basePrompt} Provide detailed technical responses.`;
      } else if (userRole === "beginner") {
        return `${basePrompt} Explain concepts simply and avoid jargon.`;
      }
      return basePrompt;
    }),
  ],
});

// The system prompt will be set dynamically based on context
const result = await agent.invoke(
  { messages: [{ role: "user", content: "Explain machine learning" }] },
  { context: { userRole: "expert" } }
);
```

--------------------------------

### Create Basic Agent with Claude and Weather Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

This snippet demonstrates how to create a basic AI agent using Langchain. It utilizes Claude Sonnet 4.5 as the language model and defines a 'getWeather' tool. The agent is configured to answer user questions by invoking the defined tools. Ensure ANTHROPIC_API_KEY environment variable is set.

```typescript
import { createAgent, tool } from "langchain";
import * as z from "zod";

const getWeather = tool(
  (input) => `It's always sunny in ${input.city}!`,
  {
    name: "get_weather",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string().describe("The city to get the weather for"),
    }),
  }
);

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  tools: [getWeather],
});

console.log(
  await agent.invoke({
    messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
  })
);

```

--------------------------------

### Install MongoDB Atlas Vector Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the `@langchain/mongodb` package for integrating with MongoDB Atlas Vector Search. This package requires the `mongodb` driver as well.

```bash
npm i @langchain/mongodb
```

```bash
yarn add @langchain/mongodb
```

```bash
pnpm add @langchain/mongodb
```

--------------------------------

### Install Cohere Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using Cohere embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/cohere
```

```bash
yarn add @langchain/cohere
```

```bash
pnpm add @langchain/cohere
```

--------------------------------

### Define Structured Response Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/quickstart

Defines a Zod object schema for the agent's response, ensuring predictable output by specifying fields like 'punny_response' and 'weather_conditions'.

```typescript
const responseFormat = z.object({
  punny_response: z.string(),
  weather_conditions: z.string().optional(),
});
```

--------------------------------

### Install Google VertexAI Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using Google VertexAI embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/google-vertexai
```

```bash
yarn add @langchain/google-vertexai
```

```bash
pnpm add @langchain/google-vertexai
```

--------------------------------

### Create a LangChain Agent with a Tool

Source: https://docs.langchain.com/oss/javascript/langchain/overview

This TypeScript example demonstrates how to create a LangChain agent. It defines a simple 'getWeather' tool using Zod for schema validation and then initializes an agent with a specific model and the defined tool. The agent is then invoked with a user query.

```typescript
import * as z from "zod";
// npm install @langchain/anthropic to call the model
import { createAgent, tool } from "langchain";

const getWeather = tool(
  ({ city }) => `It's always sunny in ${city}!`,
  {
    name: "get_weather",
    description: "Get the weather for a given city",
    schema: z.object({
      city: z.string(),
    }),
  },
);

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  tools: [getWeather],
});

console.log(
  await agent.invoke({
    messages: [{ role: "user", content: "What's the weather in Tokyo?" }],
  })
);
```

--------------------------------

### Math Server Implementation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

A TypeScript example demonstrating how to create a Math MCP server using stdio transport. This server exposes 'add' and 'multiply' tools, defining their input schemas and handling tool calls. It's suitable for local testing and environments where stdio is preferred.

```typescript
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";

const server = new Server(
    {
        name: "math-server",
        version: "0.1.0",
    },
    {
        capabilities: {
        tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "add",
            description: "Add two numbers",
            inputSchema: {
            type: "object",
            properties: {
                a: {
                type: "number",
                description: "First number",
                },
                b: {
                type: "number",
                description: "Second number",
                },
            },
            required: ["a", "b"],
            },
        },
        {
            name: "multiply",
            description: "Multiply two numbers",
            inputSchema: {
            type: "object",
            properties: {
                a: {
                type: "number",
                description: "First number",
                },
                b: {
                type: "number",
                description: "Second number",
                },
            },
            required: ["a", "b"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "add": {
        const { a, b } = request.params.arguments as { a: number; b: number };
        return {
            content: [
            {
                type: "text",
                text: String(a + b),
            },
            ],
        };
        }
        case "multiply": {
        const { a, b } = request.params.arguments as { a: number; b: number };
        return {
            content: [
            {
                type: "text",
                text: String(a * b),
            },
            ],
        };
        }
        default:
        throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

async function main() {
    const transport = new StdioServerTransport();
    await server.connect(transport);
    console.error("Math MCP server running on stdio");
}

main();

```

--------------------------------

### Install AWS Bedrock Package for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Installs the AWS package for Langchain.js using npm, pnpm, yarn, or bun. This is necessary for integrating with AWS Bedrock services, including Converse API.

```bash
npm install @langchain/aws
```

```bash
pnpm install @langchain/aws
```

```bash
yarn add @langchain/aws
```

```bash
bun add @langchain/aws
```

--------------------------------

### Personal Assistant Supervisor Example using TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript example demonstrates a multi-agent system where a supervisor agent coordinates specialized sub-agents for calendar and email tasks. It utilizes Langchain's tool and createAgent functionalities, along with Zod for schema validation. The example includes stubbed API tools for calendar event creation, email sending, and checking time slot availability, and wraps these specialized agents as tools for the supervisor. Dependencies include 'langchain', '@langchain/anthropic', and 'zod'.

```typescript
/**
 * Personal Assistant Supervisor Example
 *
 * This example demonstrates the tool calling pattern for multi-agent systems.
 * A supervisor agent coordinates specialized sub-agents (calendar and email)
 * that are wrapped as tools.
 */

import { tool, createAgent } from "langchain";
import { ChatAnthropic } from "@langchain/anthropic";
import { z } from "zod";

// =============================================================================
// Step 1: Define low-level API tools (stubbed)
// ============================================================================

const createCalendarEvent = tool(
  async ({ title, startTime, endTime, attendees, location }) => {
    // Stub: In practice, this would call Google Calendar API, Outlook API, etc.
    return `Event created: ${title} from ${startTime} to ${endTime} with ${attendees.length} attendees`;
  },
  {
    name: "create_calendar_event",
    description: "Create a calendar event. Requires exact ISO datetime format.",
    schema: z.object({
      title: z.string(),
      startTime: z.string().describe("ISO format: '2024-01-15T14:00:00'"),
      endTime: z.string().describe("ISO format: '2024-01-15T15:00:00'"),
      attendees: z.array(z.string()).describe("email addresses"),
      location: z.string().optional().default(""),
    }),
  }
);

const sendEmail = tool(
  async ({ to, subject, body, cc }) => {
    // Stub: In practice, this would call SendGrid, Gmail API, etc.
    return `Email sent to ${to.join(", ")} - Subject: ${subject}`;
  },
  {
    name: "send_email",
    description:
      "Send an email via email API. Requires properly formatted addresses.",
    schema: z.object({
      to: z.array(z.string()).describe("email addresses"),
      subject: z.string(),
      body: z.string(),
      cc: z.array(z.string()).optional().default([]),
    }),
  }
);

const getAvailableTimeSlots = tool(
  async ({ attendees, date, durationMinutes }) => {
    // Stub: In practice, this would query calendar APIs
    return ["09:00", "14:00", "16:00"];
  },
  {
    name: "get_available_time_slots",
    description:
      "Check calendar availability for given attendees on a specific date.",
    schema: z.object({
      attendees: z.array(z.string()),
      date: z.string().describe("ISO format: '2024-01-15'"),
      durationMinutes: z.number(),
    }),
  }
);

// =============================================================================
// Step 2: Create specialized sub-agents
// ============================================================================

const llm = new ChatAnthropic({
  model: "claude-haiku-4-5-20251001",
});

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: `
  You are a calendar scheduling assistant.
  Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm')
  into proper ISO datetime formats.
  Use get_available_time_slots to check availability when needed.
  Use create_calendar_event to schedule events.
  Always confirm what was scheduled in your final response.
    `.trim(),
});

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: `
  You are an email assistant.
  Compose professional emails based on natural language requests.
  Extract recipient information and craft appropriate subject lines and body text.
  Use send_email to send the message.
  Always confirm what was sent in your final response.
    `.trim(),
});

// =============================================================================
// Step 3: Wrap sub-agents as tools for the supervisor
// ============================================================================

const scheduleEvent = tool(
  async ({ request }) => {
    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: request }],
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "schedule_event",
    description: `
  Schedule calendar events using natural language.

  Use this when the user wants to create, modify, or check calendar appointments.
  Handles date/time parsing, availability checking, and event creation.

  Input: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2pm')
      `,
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

const manageEmail = tool(
  async ({ request }) => {
    const result = await emailAgent.invoke({
      messages: [{ role: "user", content: request }],
    });

```

--------------------------------

### Setting a Static System Prompt for an Agent

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Demonstrates how to initialize an agent with a static system prompt string. This prompt guides the agent's overall behavior and response style. If no system prompt is provided, the agent infers its task from the messages.

```typescript
const agent = createAgent({
  model,
  tools,
  systemPrompt: "You are a helpful assistant. Be concise and accurate.",
});
```

--------------------------------

### Create Calendar Agent with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This code snippet demonstrates how to initialize a calendar agent using Langchain. It defines a system prompt to guide the agent's behavior, specifying its role as a scheduling assistant. It also configures the agent with necessary tools such as 'create_calendar_event' and 'get_available_time_slots', and specifies the language model to be used.

```typescript
import { createAgent } from "langchain";

const CALENDAR_AGENT_PROMPT = `
You are a calendar scheduling assistant.
Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm')
into proper ISO datetime formats.
Use get_available_time_slots to check availability when needed.
Use create_calendar_event to schedule events.
Always confirm what was scheduled in your final response.
`.trim();

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: CALENDAR_AGENT_PROMPT,
});
```

--------------------------------

### LangChain.js Video Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Provides examples of creating HumanMessage objects with video content for LangChain chat models in JavaScript. Demonstrates inputting video via base64 encoding or provider file IDs. Verify model support for video processing.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this video." },
    {
      type: "video",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this video." },
    { type: "video", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Install LangChain Dependencies (Bash)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This snippet provides commands to install the necessary LangChain.js dependencies using different package managers: npm, yarn, and pnpm. It includes the core 'langchain' package along with community and textsplitter modules required for RAG applications.

```bash
npm i langchain @langchain/community @langchain/textsplitters

```

```bash
yarn add langchain @langchain/community @langchain/textsplitters

```

```bash
pnpm add langchain @langchain/community @langchain/textsplitters

```

--------------------------------

### Install AWS Bedrock Embeddings for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Installs the necessary package for using AWS Bedrock embeddings with Langchain.js. Supports npm, yarn, and pnpm package managers.

```bash
npm i @langchain/aws
```

```bash
yarn add @langchain/aws
```

```bash
pnpm add @langchain/aws
```

--------------------------------

### Invoking an Agent

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Provides a basic example of how to invoke an agent. The agent is called with a user message, and it processes this message according to its configuration, including its system prompt and available tools.

```typescript
await agent.invoke({
  messages: [{ role: "user", content: "What's the weather in San Francisco?" }],
})
```

--------------------------------

### LangChain Tool Calling with Zod Schema for Product Review Analysis

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example demonstrates how to use `toolStrategy` with a Zod schema to define the structure for analyzing product reviews. It shows the import of necessary LangChain and Zod components, the definition of the `ProductReview` schema, and the creation of an agent configured to output structured JSON. The output is the structured analysis of a given review.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductReview = z.object({
    rating: z.number().min(1).max(5).optional(),
    sentiment: z.enum(["positive", "negative"]),
    keyPoints: z.array(z.string()).describe("The key points of the review. Lowercase, 1-3 words each."),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy(ProductReview)
})

result = agent.invoke({
    "messages": [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

--------------------------------

### Initialize AWS Bedrock Converse Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Provides an example of initializing an AWS Bedrock chat model using the `initChatModel` function. This requires proper AWS credential configuration, which should be followed from the provided documentation link.

```typescript
import { initChatModel } from "langchain";

// Follow the steps here to configure your credentials:

```

--------------------------------

### Create and Use VectorStoreRetriever

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This example shows how to create a Retriever from a VectorStore using the 'asRetriever' method. It configures the retriever with a specific search type ('mmr') and search arguments. The 'batch' method is then used to invoke the retriever with multiple queries.

```typescript
const retriever = vectorStore.asRetriever({
  searchType: "mmr",
  searchKwargs: {
    fetchK: 1,
  },
});

await retriever.batch([
  "When was Nike incorporated?",
  "What was Nike's revenue in 2023?",
]);
```

--------------------------------

### Initialize Chat Model with Parameters

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to initialize a chat model with specific runtime parameters using the `initChatModel` function. This example sets the `temperature`, `timeout`, and `max_tokens` for the "claude-sonnet-4-5-20250929" model, allowing for fine-tuned response generation.

```typescript
const model = await initChatModel(
    "claude-sonnet-4-5-20250929",
    { temperature: 0.7, timeout: 30, max_tokens: 1000 }
)
```

--------------------------------

### Run the SQL Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Executes a sample query using the created agent. It demonstrates how to stream the agent's responses, logging each step (human input, tool calls, and AI output) to the console. This allows observation of the agent's reasoning and execution process.

```typescript
const question = "Which genre, on average, has the longest tracks?";
const stream = await agent.stream(
  { messages: [{ role: "user", content: question }] },
  { streamMode: "values" }
);
for await (const step of stream) {
  const message = step.messages.at(-1);
  console.log(`${message.role}: ${JSON.stringify(message.content, null, 2)}`);
}

```

--------------------------------

### Initialize Azure Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Provides code examples for initializing an Azure OpenAI chat model via `initChatModel` or `AzureChatOpenAI`. Requires API key, endpoint, and API version, along with the model name (e.g., 'azure_openai:gpt-4.1').

```typescript
import { initChatModel } from "langchain";

process.env.AZURE_OPENAI_API_KEY = "your-api-key";
process.env.AZURE_OPENAI_ENDPOINT = "your-endpoint";
process.env.OPENAI_API_VERSION = "your-api-version";

const model = await initChatModel("azure_openai:gpt-4.1");
```

```typescript
import { AzureChatOpenAI } from "@langchain/openai";

const model = new AzureChatOpenAI({
  model: "gpt-4.1",
  azureOpenAIApiKey: "your-api-key",
  azureOpenAIApiEndpoint: "your-endpoint",
  azureOpenAIApiVersion: "your-api-version"
});
```

--------------------------------

### Test Supervisor Agent with Single-Domain Request in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Demonstrates how to use the supervisor agent to handle a simple, single-domain request. The example shows sending a query to the supervisor agent, streaming the response, and logging the formatted messages. This tests the agent's ability to correctly route a request to the appropriate tool and process its output.

```typescript
const query = "Schedule a team standup for tomorrow at 9am";

const stream = await supervisorAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Create Agent with Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Demonstrates how to create an agent using `create_agent` and include custom or built-in middleware. This example shows passing `summarizationMiddleware` and `humanInTheLoopMiddleware` to the `middleware` option. Ensure LangChain and necessary middleware are imported.

```typescript
import {
  createAgent,
  summarizationMiddleware,
  humanInTheLoopMiddleware,
} from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [summarizationMiddleware, humanInTheLoopMiddleware],
});
```

--------------------------------

### Instantiate and Use AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to instantiate an AIMessage object, which represents the output of a model invocation. AIMessages can contain multimodal data, tool calls, and provider-specific metadata. This example also demonstrates invoking a model and logging the type of the response.

```typescript
const response = await model.invoke("Explain AI");
console.log(typeof response);  // AIMessage
```

--------------------------------

### Production Checkpointer with PostgreSQL (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This code example shows how to configure a production-ready checkpointer for short-term memory persistence using a PostgreSQL database. It utilizes `PostgresSaver` and requires a database connection string.

```typescript
import { PostgresSaver } from "@langchain/langgraph-checkpoint-postgres";

const DB_URI = "postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable";
const checkpointer = PostgresSaver.fromConnString(DB_URI);
```

--------------------------------

### Setup Agentic RAG Chain with Dynamic System Prompt - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Configures an agent to use a dynamic system prompt that incorporates retrieved documents for context. It uses middleware to fetch relevant documents based on the last user query and injects them into the system message before model inference. This approach allows the LLM to decide when to search.

```typescript
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";
import { SystemMessage } from "@langchain/core/messages";

const agent = createAgent({
  model,
  tools: [],
  middleware: [
    dynamicSystemPromptMiddleware(async (state) => {
        const lastQuery = state.messages[state.messages.length - 1].content;

        const retrievedDocs = await vectorStore.similaritySearch(lastQuery, 2);

        const docsContent = retrievedDocs
        .map((doc) => doc.pageContent)
        .join("\n\n");

        // Build system message
        const systemMessage = new SystemMessage(
        `You are a helpful assistant. Use the following context in your response:\n\n${docsContent}`
        );

        // Return system + existing messages
        return [systemMessage, ...state.messages];
    })
  ]
});

```

--------------------------------

### LangChain.js Audio Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to use HumanMessage objects for audio inputs in LangChain.js. Examples cover audio data provided as base64 encoded strings or through provider-managed file IDs. Compatibility depends on the specific chat model's capabilities.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this audio." },
    {
      type: "audio",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this audio." },
    { type: "audio", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Basic Agent with Short-Term Memory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This snippet demonstrates the fundamental setup of an agent with short-term memory using a `MemorySaver` checkpointer. It shows how to invoke the agent with initial messages and a thread ID for persistence.

```typescript
import { createAgent } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const agent = createAgent({
    model: "claude-sonnet-4-5-20250929",
    tools: [],
    checkpointer,
});

await agent.invoke(
    { messages: [{ role: "user", content: "hi! i am Bob" }] },
    { configurable: { thread_id: "1" } }
);
```

--------------------------------

### LangChain.js PDF Document Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to construct HumanMessage objects for PDF document inputs in LangChain.js. This includes examples for providing documents via URL, base64 encoding, or a file ID. Note that specific providers like OpenAI may require additional keys for file handling.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From URL
const messageFromUrl = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this document." },
    { type: "file", source_type: "url", url: "https://example.com/path/to/document.pdf" },
  ],
});

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this document." },
    {
      type: "file",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this document." },
    { type: "file", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Define a Simple Email Sending Agent in Python

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Defines a Python function `send_email` to simulate sending an email and then creates an agent using `langchain.agents.create_agent`. This agent is configured to use the GPT-4o model and is given a system prompt instructing it to always use the `send_email` tool. The agent is intended for use with Studio for local debugging and interaction.

```python
from langchain.agents import create_agent

def send_email(to: str, subject: str, body: str):
    """Send an email"""
    email = {
        "to": to,
        "subject": subject,
        "body": body
    }
    # ... email sending logic

    return f"Email sent to {to}"

agent = create_agent(
    "gpt-4o",
    tools=[send_email],
    system_prompt="You are an email assistant. Always use the send_email tool.",
)
```

--------------------------------

### Initialize Qdrant Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the QdrantVectorStore from an existing collection. Requires the Qdrant service URL and the name of the collection to connect to.

```typescript
import { QdrantVectorStore } from "@langchain/qdrant";

const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {
  url: process.env.QDRANT_URL,
  collectionName: "langchainjs-testing",

```

--------------------------------

### Initialize and Interact with SQLite Database (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Initializes a SqlDatabase wrapper using Langchain from a TypeORM DataSource configured for SQLite. It provides functions to get the database instance and retrieve table schema information. Requires '@langchain/classic/sql_db' and 'typeorm' packages.

```typescript
import { SqlDatabase } from "@langchain/classic/sql_db";
import { DataSource } from "typeorm";

let db: SqlDatabase | undefined;
async function getDb() {
  if (!db) {
    const dbPath = await resolveDbFile();
    const datasource = new DataSource({ type: "sqlite", database: dbPath });
    db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });
  }
  return db;
}

async function getSchema() {
  const db = await getDb();
  return await db.getTableInfo();
}
```

--------------------------------

### Dynamically Select Tools with Middleware in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This example demonstrates how to create a middleware to dynamically select a relevant subset of tools for an agent at runtime. It improves performance and accuracy by reducing the number of tools the model needs to consider. All available tools must be registered upfront, and the middleware filters them based on the request's state and runtime context.

```typescript
import { createAgent, createMiddleware } from "langchain";

const toolSelectorMiddleware = createMiddleware({
  name: "ToolSelector",
  wrapModelCall: (request, handler) => {
    // Select a small, relevant subset of tools based on state/context
    const relevantTools = selectRelevantTools(request.state, request.runtime);
    const modifiedRequest = { ...request, tools: relevantTools };
    return handler(modifiedRequest);
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: allTools, // All available tools need to be registered upfront
  // Middleware can be used to select a smaller subset that's relevant for the given run.
  middleware: [toolSelectorMiddleware],
});
```

--------------------------------

### Test Supervisor Agent with Multi-Domain Request in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Illustrates how the supervisor agent handles a complex, multi-domain request that requires coordinating actions across different tools. The example sends a query involving both scheduling a meeting and sending an email reminder. It then streams and logs the output, showcasing the agent's capability to sequence and synthesize results from multiple sub-agents.

```typescript
const query =
  "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
  "and send them an email reminder about reviewing the new mockups.";

const stream = await supervisorAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### LangChain Tool Calling with Union Types for Multiple Schemas

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example shows how to configure `toolStrategy` to handle multiple potential output structures by providing an array of Zod schemas. The agent can then return a structured response conforming to either the `ProductReview` schema or the `CustomerComplaint` schema. The code demonstrates defining both schemas and initializing an agent that can process inputs relevant to either structure.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductReview = z.object({
    rating: z.number().min(1).max(5).optional(),
    sentiment: z.enum(["positive", "negative"]),
    keyPoints: z.array(z.string()).describe("The key points of the review. Lowercase, 1-3 words each."),
});

const CustomerComplaint = z.object({
    issueType: z.enum(["product", "service", "shipping", "billing"]),
    severity: z.enum(["low", "medium", "high"]),
    description: z.string().describe("Brief description of the complaint"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy([ProductReview, CustomerComplaint])
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

--------------------------------

### Construct RAG Agent with Retrieval Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This code snippet shows how to construct a RAG agent using Langchain.js. It takes a list of tools (including the previously defined `retrieve` tool) and a `SystemMessage` that guides the agent's behavior. The `createAgent` function is used to instantiate the agent, specifying the model and the system prompt.

```typescript
import { createAgent } from "langchain";
import { SystemMessage } from "@langchain/core/messages";

const tools = [retrieve];
const systemPrompt = new SystemMessage(
    "You have access to a tool that retrieves context from a blog post. " +
    "Use the tool to help answer user queries."
)

const agent = createAgent({ model: "gpt-5", tools, systemPrompt });
```

--------------------------------

### Example Usage Metadata JSON

Source: https://docs.langchain.com/oss/javascript/langchain/messages

A sample JSON object representing the structure of `usage_metadata` that can be found within an AIMessage. It includes details on input, output, and total token counts, along with token details for reasoning and caching.

```json
{
  "output_tokens": 304,
  "input_tokens": 8,
  "total_tokens": 312,
  "input_token_details": {
    "cache_read": 0
  },
  "output_token_details": {
    "reasoning": 256
  }
}
```

--------------------------------

### Agent with Provider Strategy (Zod Schema)

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Example of creating an agent that uses a Zod schema for structured output via the `providerStrategy`. This approach is highly reliable when the model provider supports native structured output.

```typescript
import * as z from "zod";
import { createAgent, providerStrategy } from "langchain";

const ContactInfo = z.object({
    name: z.string().describe("The name of the person"),
    email: z.string().describe("The email address of the person"),
    phone: z.string().describe("The phone number of the person"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: providerStrategy(ContactInfo)
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
});

result.structuredResponse;
// { name: "John Doe", email: "john@example.com", phone: "(555) 123-4567" }
```

--------------------------------

### Setup Two-Step RAG Chain with Source Document Handling - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Illustrates setting up a two-step RAG chain that always performs a search and includes retrieved documents in the application state. This involves defining a state schema that includes a 'context' field for documents and using a pre-model hook middleware to populate this field and inject context into the prompt.

```typescript
import { createMiddleware, Document, createAgent } from "langchain";
import { MessagesZodSchema } from "@langchain/langgraph";

const StateSchema = z.object({
  messages: MessagesZodSchema,
  context: z.array(z.custom<Document>()),
})

const retrieveDocumentsMiddleware = createMiddleware({
  stateSchema: StateSchema,
  beforeModel: async (state) => {

```

--------------------------------

### Similarity Search in Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This example shows how to perform a similarity search on a vector store using a string query. It retrieves documents that are semantically similar to the provided question. The output includes the most similar Document object.

```typescript
const results1 = await vectorStore.similaritySearch(
  "When was Nike incorporated?"
);

console.log(results1[0]);
```

--------------------------------

### Configure LangSmith API Key Environment Variable

Source: https://docs.langchain.com/oss/javascript/langchain/studio

Sets the `LANGSMITH_API_KEY` environment variable in a `.env` file. This API key is essential for authenticating with LangSmith services, which are used by Studio for tracing and debugging agent behavior. It is critical to ensure this file is not committed to version control.

```bash
LANGSMITH_API_KEY=lsv2...
```

--------------------------------

### Invoke a Chat Model

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a chat model to get a response. The `invoke` method takes a string prompt as input and returns the model's generated message. This is a fundamental operation for interacting with chat models.

```typescript
const response = await model.invoke("Why do parrots talk?");
```

--------------------------------

### Combine Multiple Guardrails in Langchain Agent

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

This example demonstrates how to stack multiple middleware functions to create layered protection for a Langchain agent. Guardrails execute in the order they are added to the middleware array, allowing for sequential application of different validation and control mechanisms.

```typescript
import { createAgent, piiRedactionMiddleware, humanInTheLoopMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, sendEmailTool],
  middleware: [
    // Layer 1: Deterministic input filter (before agent)
    contentFilterMiddleware(["hack", "exploit"]),

    // Layer 2: PII protection (before and after model)
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToInput: true,
    }),
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToOutput: true,
    }),

    // Layer 3: Human approval for sensitive tools
    humanInTheLoopMiddleware({
      interruptOn: {
        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },
      }
    }),

    // Layer 4: Model-based safety check (after agent)
    safetyGuardrailMiddleware(),
  ],
});
```

--------------------------------

### Batch requests to a model with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Batching a collection of independent requests to a model can significantly improve performance and reduce costs by allowing parallel processing. This example demonstrates sending multiple prompts to a model and logging the responses.

```typescript
const responses = await model.batch([
  "Why do parrots have colorful feathers?",
  "How do airplanes fly?",
  "What is quantum computing?",
  "Why do parrots have colorful feathers?",
  "How do airplanes fly?",
  "What is quantum computing?",
]);
for (const response of responses) {
  console.log(response);
}
```

--------------------------------

### LangChain.js Schema Validation Error Example

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This snippet demonstrates how LangChain.js provides specific error feedback when structured output, defined by a Zod schema, fails to match the expected format. It shows an example where the model attempts to generate output that violates the schema constraints, and the subsequent tool response indicates the validation error.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductRating = z.object({
    rating: z.number().min(1).max(5).describe("Rating from 1-5"),
    comment: z.string().describe("Review comment"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy(ProductRating),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content: "Parse this: Amazing product, 10/10!",
        },
    ],
});

console.log(result);
```

--------------------------------

### Weather Server Implementation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

A TypeScript example for creating a Weather MCP server using SSE (Server-Sent Events) transport with Express.js. This server defines a 'get_weather' tool and handles requests to provide weather information for a given location. It's suitable for web-based applications.

```typescript
import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { SSEServerTransport } from "@modelcontextprotocol/sdk/server/sse.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import express from "express";

const app = express();
app.use(express.json());

const server = new Server(
    {
        name: "weather-server",
        version: "0.1.0",
    },
    {
        capabilities: {
        tools: {},
        },
    }
);

server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
        {
            name: "get_weather",
            description: "Get weather for location",
            inputSchema: {
            type: "object",
            properties: {
                location: {
                type: "string",
                description: "Location to get weather for",
                },
            },
            required: ["location"],
            },
        },
        ],
    };
});

server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "get_weather": {
        const { location } = request.params.arguments as { location: string };
        return {
            content: [
            {
                type: "text",
                text: `It's always sunny in ${location}`,
            },
            ],
        };
        }
        default:
        throw new Error(`Unknown tool: ${request.params.name}`);
    }
});

app.post("/mcp", async (req, res) => {
    const transport = new SSEServerTransport("/mcp", res);
    await server.connect(transport);
});

const PORT = process.env.PORT || 8000;
app.listen(PORT, () => {

```

--------------------------------

### Model Invocation with Configuration

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a model with various configuration options using the `RunnableConfig` object.

```APIDOC
## POST /models/invoke

### Description
Invokes a language model with a given prompt and optional runtime configuration.

### Method
POST

### Endpoint
`/models/invoke`

### Parameters
#### Request Body
- **prompt** (string) - Required - The input prompt for the model.
- **config** (object) - Optional - Configuration object for the invocation.
  - **runName** (string) - Optional - A custom name for this specific run.
  - **tags** (string[]) - Optional - Labels for categorization and filtering.
  - **metadata** (object) - Optional - Custom key-value pairs for additional context.
  - **callbacks** (CallbackHandler[]) - Optional - Handlers for monitoring execution events.
  - **maxConcurrency** (number) - Optional - Maximum parallel calls for batch operations.
  - **recursion_limit** (number) - Optional - Maximum recursion depth for chains.

### Request Example
```json
{
  "prompt": "Tell me a joke",
  "config": {
    "runName": "joke_generation",
    "tags": ["humor", "demo"],
    "metadata": {"user_id": "123"},
    "callbacks": [my_callback_handler]
  }
}
```

### Response
#### Success Response (200)
- **text** (string) - The model's generated response.

#### Response Example
```json
{
  "text": "Why don't scientists trust atoms? Because they make up everything!"
}
```
```

--------------------------------

### LangChain Tool Calling with JSON Schema for Product Review Analysis

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This code snippet illustrates using `toolStrategy` with a JSON schema for structured product review analysis. It defines a `productReviewSchema` in JSON Schema format and then initializes an agent with this schema. The example shows how to invoke the agent and log the resulting structured output, which is a JSON object representing the review's analysis.

```typescript
import { createAgent, toolStrategy } from "langchain";

const productReviewSchema = {
    "type": "object",
    "description": "Analysis of a product review.",
    "properties": {
        "rating": {
            "type": ["integer", "null"],
            "description": "The rating of the product (1-5)",
            "minimum": 1,
            "maximum": 5
        },
        "sentiment": {
            "type": "string",
            "enum": ["positive", "negative"],
            "description": "The sentiment of the review"
        },
        "key_points": {
            "type": "array",
            "items": {"type": "string"},
            "description": "The key points of the review"
        }
    },
    "required": ["sentiment", "key_points"]
}

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy(productReviewSchema)
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

--------------------------------

### Configure Tool Call Limits in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Demonstrates how to set up global and tool-specific limits for tool calls using the `toolCallLimitMiddleware` in Langchain.js. It shows examples of setting thread limits, run limits, and different exit behaviors for agent execution.

```typescript
import { createAgent, toolCallLimitMiddleware } from "langchain";

// Global limit: max 20 calls per thread, 10 per run
const globalLimiter = toolCallLimitMiddleware({
  threadLimit: 20,
  runLimit: 10,
});

// Tool-specific limit with default "continue" behavior
const searchLimiter = toolCallLimitMiddleware({
  toolName: "search",
  threadLimit: 5,
  runLimit: 3,
});

// Thread limit only (no per-run limit)
const databaseLimiter = toolCallLimitMiddleware({
  toolName: "query_database",
  threadLimit: 10,
});

// Strict enforcement with "error" behavior
const webScraperLimiter = toolCallLimitMiddleware({
  toolName: "scrape_webpage",
  runLimit: 2,
  exitBehavior: "error",
});

// Immediate termination with "end" behavior
const criticalToolLimiter = toolCallLimitMiddleware({
  toolName: "delete_records",
  runLimit: 1,
  exitBehavior: "end",
});

// Use multiple limiters together
const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, databaseTool, scraperTool],
  middleware: [globalLimiter, searchLimiter, databaseLimiter, webScraperLimiter],
});
```

--------------------------------

### Streaming Tool Calls in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Explains how to stream responses from a model that supports tool calls. It demonstrates receiving `ToolCallChunk` objects as they are generated, allowing for progressive display of tool call information. The example also shows how to accumulate these chunks into complete tool calls.

```typescript
const stream = await modelWithTools.stream(
    "What's the weather in Boston and Tokyo?"
)
for await (const chunk of stream) {
    // Tool call chunks arrive progressively
    if (chunk.tool_call_chunks) {
        for (const tool_chunk of chunk.tool_call_chunks) {
        console.log(`Tool: ${tool_chunk.get('name', '')}`)
        console.log(`Args: ${tool_chunk.get('args', '')}`)
        }
    }
}

// Output:
// Tool: get_weather
// Args:
// Tool:
// Args: {"loc
// Tool:
// Args: ation": "BOS"}
// Tool: get_time
// Args:
// Tool:
// Args: {"timezone": "Tokyo"}

```

```typescript
let full: AIMessageChunk | null = null
const stream = await modelWithTools.stream("What's the weather in Boston?")
for await (const chunk of stream) {
    full = full ? full.concat(chunk) : chunk
    console.log(full.contentBlocks)
}

```

--------------------------------

### Stream Agent Progress with Updates Mode in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

This example demonstrates how to stream agent progress using the `stream` method with `streamMode: 'updates'`. It shows how to capture state updates after each agent step, including LLM node, Tool node, and final AI response. Dependencies include `zod` for schema definition and `langchain` for agent and tool creation.

```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "updates" }
)) {
    const [step, content] = Object.entries(chunk)[0];
    console.log(`step: ${step}`);
    console.log(`content: ${JSON.stringify(content, null, 2)}`);
}
/**
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         // ...
 *         "tool_calls": [
 *           {
 *             "name": "get_weather",
 *             "args": {
 *               "city": "San Francisco"
 *             },
 *             "type": "tool_call",
 *             "id": "call_0qLS2Jp3MCmaKJ5MAYtr4jJd"
 *           }
 *         ],
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: tools
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The weather in San Francisco is always sunny!",
 *         "name": "get_weather",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 * step: model
 * content: {
 *   "messages": [
 *     {
 *       "kwargs": {
 *         "content": "The latest update says: The weather in San Francisco is always sunny!\n\nIf you'd like real-time details (current temperature, humidity, wind, and today's forecast), I can pull the latest data for you. Want me to fetch that?",
 *         // ...
 *       }
 *     }
 *   ]
 * }
 */
```

--------------------------------

### Initialize Google Gemini Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Demonstrates initializing a Google Gemini chat model using `initChatModel` or `ChatGoogleGenerativeAI`. Requires the Google API key and specifies the model (e.g., 'google-genai:gemini-2.5-flash-lite').

```typescript
import { initChatModel } from "langchain";

process.env.GOOGLE_API_KEY = "your-api-key";

const model = await initChatModel("google-genai:gemini-2.5-flash-lite");
```

```typescript
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

const model = new ChatGoogleGenerativeAI({
  model: "gemini-2.5-flash-lite",
  apiKey: "your-api-key"
});
```

--------------------------------

### Text Content Block Example (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates the structure of a standard text content block. It includes the 'type' property, which must be 'text', the 'text' content, and an optional 'annotations' array for associated metadata.

```typescript
{
    type: "text",
    text: "Hello world",
    annotations: []
}
```

--------------------------------

### Define a Tool for Searching Orders in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Defines a tool named 'search_orders' using Langchain.js and Zod for schema validation. This tool searches user orders based on provided status and limit, guiding the model's reasoning for order-related queries.

```typescript
import {
  tool
} from "@langchain/core/tools";
import {
  z
} from "zod";

const searchOrders = tool(
  async ({ userId, status, limit = 10 }) => {
    // Implementation here
  },
  {
    name: "search_orders",
    description: `Search for user orders by status.

    Use this when the user asks about order history or wants to check
    order status. Always filter by the provided status.`,
    schema: z.object({
      userId: z.string().describe("Unique identifier for the user"),
      status: z.enum(["pending", "shipped", "delivered"]).describe("Order status to filter by"),
      limit: z.number().default(10).describe("Maximum number of results to return"),
    }),
  }
);

```

--------------------------------

### Streaming Agent Responses in LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/agents

This example shows how to stream agent responses incrementally instead of waiting for a final output. It iterates over the streamed chunks, logs agent messages, and identifies tool calls as they occur, providing real-time feedback to the user.

```typescript
const stream = await agent.stream(
  {
    messages: [{
      role: "user",
      content: "Search for AI news and summarize the findings"
    }],
  },
  { streamMode: "values" }
);

for await (const chunk of stream) {
  // Each chunk contains the full state at that point
  const latestMessage = chunk.messages.at(-1);
  if (latestMessage?.content) {
    console.log(`Agent: ${latestMessage.content}`);
  } else if (latestMessage?.tool_calls) {
    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);
    console.log(`Calling tools: ${toolCallNames.join(", ")}`);
  }
}
```

--------------------------------

### Define Fetch URL Tool and Create Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/retrieval

This snippet demonstrates how to define a simple tool for fetching content from a URL and then create an agent using this tool. The agent is configured with a model, the defined tool, and a system prompt.

```typescript
import { tool, createAgent } from "langchain";

const fetchUrl = tool(
    (url: string) => {
        return `Fetched content from ${url}`;
    },
    { name: "fetch_url", description: "Fetch text content from a URL" }
);

const agent = createAgent({
    model: "claude-sonnet-4-0",
    tools: [fetchUrl],
    systemPrompt,
});
```

--------------------------------

### Parallel Tool Calls in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Illustrates how to invoke a model that can call multiple tools in parallel. It shows how to bind tools, invoke the model with a multi-part query, and process the resulting tool calls. The example also includes how to disable parallel calls.

```typescript
const modelWithTools = model.bind_tools([get_weather])

const response = await modelWithTools.invoke(
    "What's the weather in Boston and Tokyo?"
)


// The model may generate multiple tool calls
console.log(response.tool_calls)
// [
//   { name: 'get_weather', args: { location: 'Boston' }, id: 'call_1' },
//   { name: 'get_time', args: { location: 'Tokyo' }, id: 'call_2' } 
// ]


// Execute all tools (can be done in parallel with async)
const results = []
for (const tool_call of response.tool_calls || []) {
    if (tool_call.name === 'get_weather') {
        const result = await get_weather.invoke(tool_call)
        results.push(result)
    }
}

```

```python
model.bind_tools([get_weather], parallel_tool_calls=False)
```

--------------------------------

### Reasoning Content Block Example (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates the format for a reasoning content block, used to represent model reasoning steps. It requires a 'type' property set to 'reasoning' and a 'reasoning' string detailing the thought process.

```typescript
{
    type: "reasoning",
    reasoning: "The user is asking about..."
}
```

--------------------------------

### Pass Conversational Context to Sub-Agent in LangGraphJS

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript example demonstrates how to pass additional conversational context to sub-agents within LangGraphJS. It utilizes `getCurrentTaskInput` to access the message history and constructs a more detailed prompt for the `calendarAgent`.

```typescript
import { getCurrentTaskInput } from "@langchain/langgraph";
import type { InternalAgentState } from "langchain";
import { HumanMessage } from "@langchain/core/messages";

const scheduleEvent = tool(
  async ({ request }, config) => {
    // Customize context received by sub-agent
    // Access full thread messages from the config
    const currentMessages = getCurrentTaskInput<InternalAgentState>(config).messages;

    const originalUserMessage = currentMessages.find(HumanMessage.isInstance);

    const prompt = `
    You are assisting with the following user inquiry:

    ${originalUserMessage?.content || "No context available"}

    You are tasked with the following sub-request:

    ${request}
        `.trim();

    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: prompt }],
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### LangChain.js Image Input Examples

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to create HumanMessage objects with image content for LangChain chat models. Supports images from URLs, base64 encoded data, and provider-managed file IDs. Ensure the model supports image inputs.

```typescript
import { HumanMessage } from "@langchain/core/messages";

// From URL
const messageFromUrl = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this image." },
    {
      type: "image",
      source_type: "url",
      url: "https://example.com/path/to/image.jpg"
    },
  ],
});

// From base64 data
const messageFromBase64 = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this image." },
    {
      type: "image",
      source_type: "base64",
      data: "AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...",
    },
  ],
});

// From provider-managed File ID
const messageFromId = new HumanMessage({
  content: [
    { type: "text", text: "Describe the content of this image." },
    { type: "image", source_type: "id", id: "file-abc123" },
  ],
});
```

--------------------------------

### Generate Structured Output with Zod Schema in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet demonstrates how to define an output schema using Zod and then use it with a LangChain model to get structured output. The output is automatically validated against the Zod schema. It requires the 'zod' library.

```typescript
import * as z from "zod";

const Movie = z.object({
  title: z.string().describe("The title of the movie"),
  year: z.number().describe("The year the movie was released"),
  director: z.string().describe("The director of the movie"),
  rating: z.number().describe("The movie's rating out of 10"),
});

const modelWithStructure = model.withStructuredOutput(Movie);

const response = await modelWithStructure.invoke("Provide details about the movie Inception");
console.log(response);
// {
//   title: "Inception",
//   year: 2010,
//   director: "Christopher Nolan",
//   rating: 8.8,
// }
```

--------------------------------

### Initializing Chat Model with Standard Content Serialization (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Provides an example of initializing a chat model in TypeScript with the `outputVersion: "v1"` option to ensure that message content is serialized using LangChain's standard content block format for external applications.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel(
  "gpt-5-nano",
  { outputVersion: "v1" }
);

// Alternatively, set the environment variable LC_OUTPUT_VERSION = "v1"
```

--------------------------------

### Constructing Human Messages with Different Content Types (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to create `HumanMessage` objects in TypeScript with different content formats: a simple string, a provider-native format (OpenAI example with text and image URL), and a list of LangChain's standard content blocks.

```typescript
import { HumanMessage } from "langchain/dist/messages/ai.js";

// String content
const humanMessageFromString = new HumanMessage("Hello, how are you?");

// Provider-native format (e.g., OpenAI)
const humanMessageFromProviderNative = new HumanMessage({
  content: [
    { type: "text", text: "Hello, how are you?" },
    {
      type: "image_url",
      image_url: { url: "https://example.com/image.jpg" },
    },
  ],
});

// List of standard content blocks
const humanMessageFromStandardBlocks = new HumanMessage({
  contentBlocks: [
    { type: "text", text: "Hello, how are you?" },
    { type: "image", url: "https://example.com/image.jpg" },
  ],
});
```

--------------------------------

### Intelligent Tool Selection with LLM Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This example shows how to configure an agent to use `llmToolSelectorMiddleware` for intelligent tool selection. It's beneficial when an agent has many tools, helping to reduce token usage and improve model focus by filtering irrelevant tools. The middleware allows specifying a cheaper model for selection, limiting the number of tools, and always including specific tools.

```typescript
import { createAgent, llmToolSelectorMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [tool1, tool2, tool3, tool4, tool5, ...], // Many tools
  middleware: [
    llmToolSelectorMiddleware({
      model: "gpt-4o-mini", // Use cheaper model for selection
      maxTools: 3, // Limit to 3 most relevant tools
      alwaysInclude: ["search"], // Always include certain tools
    }),
  ],
});

```

--------------------------------

### Get Raw AI Message with Structured Output in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet demonstrates how to retrieve both the parsed structured output and the raw AIMessage object, which includes metadata like token counts. This is achieved by setting `includeRaw: true` when calling `withStructuredOutput`.

```typescript
import * as z from "zod";

const Movie = z.object({
  title: z.string().describe("The title of the movie"),
  year: z.number().describe("The year the movie was released"),
  director: z.string().describe("The director of the movie"),
  rating: z.number().describe("The movie's rating out of 10"),
});

const modelWithStructure = model.withStructuredOutput(Movie, { includeRaw: true });

const response = await modelWithStructure.invoke("Provide details about the movie Inception");
console.log(response);
// {
//   raw: AIMessage { ... },
//   parsed: { title: "Inception", ... }
// }
```

--------------------------------

### Get User Preferences from Store in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This tool retrieves user preferences from the store using a provided preference key. It accesses the user ID from the runtime context and then queries the store for persisted preferences. This allows tools to personalize responses based on user settings.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const getPreference = tool(
  async ({ preferenceKey }, { runtime }) => {
    const userId = runtime.context.userId;

    // Read from Store: get existing preferences
    const store = runtime.store;
    const existingPrefs = await store.get(["preferences"], userId);

    if (existingPrefs) {
      const value = existingPrefs.value?.[preferenceKey];
      return value ? `${preferenceKey}: ${value}` : `No preference set for ${preferenceKey}`;
    } else {
      return "No preferences found";
    }
  },
  {
    name: "get_preference",
    description: "Get user preference from Store",
    schema: z.object({
      preferenceKey: z.string(),
    }),
  }
);

```

--------------------------------

### Define Customer Support Ticket Schema with Zod

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Defines a structured schema for customer support tickets using Zod. It specifies fields for category, priority, summary, and customer sentiment, along with their types and descriptions. This schema guides models to output data in a predictable format.

```typescript
import { z } from "zod";

const customerSupportTicket = z.object({
  category: z.enum(["billing", "technical", "account", "product"]).describe(
    "Issue category"
  ),
  priority: z.enum(["low", "medium", "high", "critical"]).describe(
    "Urgency level"
  ),
  summary: z.string().describe(
    "One-sentence summary of the customer's issue"
  ),
  customerSentiment: z.enum(["frustrated", "neutral", "satisfied"]).describe(
    "Customer's emotional tone"
  ),
}).describe("Structured ticket information extracted from customer message");
```

--------------------------------

### Define a Tool to Get Available Time Slots (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines a Langchain tool named 'get_available_time_slots' using Zod for schema validation. This tool is a stub that simulates checking calendar availability for given attendees on a specific date and returns available time slots.

```typescript
import { tool } from "langchain";
import { z } from "zod";

const getAvailableTimeSlots = tool(
  async ({ attendees, date, durationMinutes }) => {
    // Stub: In practice, this would query calendar APIs
    return ["09:00", "14:00", "16:00"];
  },
  {
    name: "get_available_time_slots",
    description: "Check calendar availability for given attendees on a specific date.",
    schema: z.object({
      attendees: z.array(z.string()),
      date: z.string().describe("ISO format: '2024-01-15'"),
      durationMinutes: z.number(),
    }),
  }
);

```

--------------------------------

### Initialize OpenAI Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Demonstrates two methods to initialize an OpenAI chat model in Langchain.js: using the `initChatModel` utility function and instantiating the `ChatOpenAI` class directly. Both methods require setting the OpenAI API key and specify the desired model (e.g., 'gpt-4.1').

```typescript
import { initChatModel } from "langchain";

process.env.OPENAI_API_KEY = "your-api-key";

const model = await initChatModel("gpt-4.1");
```

```typescript
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4.1",
  apiKey: "your-api-key"
});
```

--------------------------------

### Retrieve Complete Reasoning Output from LangChain.js Models

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a LangChain.js model to get a complete response and then extract all reasoning steps from its content blocks. This method collects all 'reasoning' type content blocks and joins their 'reasoning' property to form a single string output, providing a consolidated view of the model's thought process.

```typescript
const response = await model.invoke("Why do parrots have colorful feathers?");
const reasoningSteps = response.contentBlocks.filter(b => b.type === "reasoning");
console.log(reasoningSteps.map(step => step.reasoning).join(" "));
```

--------------------------------

### Summarization Middleware Setup in LangChain (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This snippet demonstrates how to integrate the `summarizationMiddleware` into a LangChain agent. The middleware automatically summarizes older messages when the conversation exceeds a token limit, replacing them with a summary in the state. It requires specifying the summarization model, a token threshold for triggering summarization, and the number of recent messages to retain.

```typescript
import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      maxTokensBeforeSummary: 4000, // Trigger summarization at 4000 tokens
      messagesToKeep: 20, // Keep last 20 messages after summary
    }),
  ],
});
```

--------------------------------

### Configure MistralAI API Key

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the MISTRAL_API_KEY environment variable for authenticating with the MistralAI API.

```bash
MISTRAL_API_KEY=your-api-key
```

--------------------------------

### Create Supervisor Agent in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines and creates a supervisor agent using Langchain.js. This agent is configured with a language model, a set of high-level tools (scheduling and email management), and a system prompt to guide its decision-making process. It acts as the central orchestrator for handling user requests that may involve multiple sub-agent actions.

```typescript
const SUPERVISOR_PROMPT = `
You are a helpful personal assistant.
You can schedule calendar events and send emails.
Break down user requests into appropriate tool calls and coordinate the results.
When a request involves multiple actions, use multiple tools in sequence.
`.trim();

const supervisorAgent = createAgent({
  model: llm,
  tools: [scheduleEvent, manageEmail],
  systemPrompt: SUPERVISOR_PROMPT,
});
```

--------------------------------

### Index Data and Create Retrieval Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This snippet loads content from a URL using Cheerio, splits it into manageable chunks using RecursiveCharacterTextSplitter, indexes these chunks into a vector store, and defines a retrieval tool. The tool searches the vector store for relevant documents based on a query and returns their serialized content and the documents themselves. Dependencies include cheerio, langchain, and @langchain/community.

```typescript
import "cheerio";
import { createAgent, tool } from "langchain";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import * as z from "zod";

// Load and chunk contents of blog
const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector
  }
);

const docs = await cheerioLoader.load();

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200
});
const allSplits = await splitter.splitDocuments(docs);

// Index chunks
await vectorStore.addDocuments(allSplits)

// Construct a tool for retrieving context
const retrieveSchema = z.object({ query: z.string() });

const retrieve = tool(
  async ({ query }) => {
    const retrievedDocs = await vectorStore.similaritySearch(query, 2);
    const serialized = retrievedDocs
      .map(
        (doc) => `Source: ${doc.metadata.source}\nContent: ${doc.pageContent}`
      )
      .join("\n");
    return [serialized, retrievedDocs];
  },
  {
    name: "retrieve",
    description: "Retrieve information related to a query.",
    schema: retrieveSchema,
    responseFormat: "content_and_artifact",
  }
);

const agent = createAgent({ model: "gpt-5", tools: [retrieve] });

```

--------------------------------

### Create Custom Content Filter Middleware for LangChain Agents

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

Shows how to build a custom middleware function in LangChain that intercepts agent requests before execution. This example implements a content filter to block requests containing specific banned keywords, returning an AIMessage and halting further processing.

```typescript
import { createMiddleware, AIMessage } from "langchain";

const contentFilterMiddleware = (bannedKeywords: string[]) => {
  const keywords = bannedKeywords.map(kw => kw.toLowerCase());

  return createMiddleware({
    name: "ContentFilterMiddleware",
    beforeAgent: (state) => {
      // Get the first user message
      if (!state.messages || state.messages.length === 0) {
        return;
      }

      const firstMessage = state.messages[0];
      if (firstMessage._getType() !== "human") {
        return;
      }

      const content = firstMessage.content.toString().toLowerCase();

      // Check for banned keywords
      for (const keyword of keywords) {
        if (content.includes(keyword)) {
          // Block execution before any processing
          return {
            messages: [
              new AIMessage(
                "I cannot process requests containing inappropriate content. Please rephrase your request."
              )
            ],
            jumpTo: "end",
          };
        }
      }

      return;
    },
  });
};

// Use the custom guardrail
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, calculatorTool],
  middleware: [
    contentFilterMiddleware(["hack", "exploit", "malware"]),
  ],
});

// This request will be blocked before any processing
const result = await agent.invoke({
  messages: [{ role: "user", content: "How do I hack into a database?" }]
});

```

--------------------------------

### LangSmith Vitest/Jest Integration for Trajectory Accuracy Evaluation

Source: https://docs.langchain.com/oss/javascript/langchain/test

Integrate LangSmith with Vitest or Jest to run evaluations for trajectory accuracy. This approach uses specific LangSmith modules for test descriptions and assertions, logging outputs directly to LangSmith. It requires installing `langsmith/vitest` or `langsmith/jest` and `agentevals`.

```typescript
import * as ls from "langsmith/vitest";
// import * as ls from "langsmith/jest";

import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";

const trajectoryEvaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT,
});

ls.describe("trajectory accuracy", () => {
  ls.test("accurate trajectory", {
    inputs: {
      messages: [
        {
          role: "user",
          content: "What is the weather in SF?"
        }
      ]
    },
    referenceOutputs: {
      messages: [
        new HumanMessage("What is the weather in SF?"),
        new AIMessage({
          content: "",
          tool_calls: [
            { id: "call_1", name: "get_weather", args: { city: "SF" } }
          ]
        }),
        new ToolMessage({
          content: "It's 75 degrees and sunny in SF.",
          tool_call_id: "call_1"
        }),
        new AIMessage("The weather in SF is 75 degrees and sunny."),
      ],
    },
  }, async ({ inputs, referenceOutputs }) => {
    const result = await agent.invoke({
      messages: [new HumanMessage("What is the weather in SF?")]
    });

    ls.logOutputs({ messages: result.messages });

    await trajectoryEvaluator({
      inputs,
      outputs: result.messages,
      referenceOutputs,
    });
  });
});
```

--------------------------------

### Initialize FAISS Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the FaissStore with embeddings. FAISS is optimized for large-scale similarity search.

```typescript
import { FaissStore } from "@langchain/community/vectorstores/faiss";

const vectorStore = new FaissStore(embeddings, {});
```

--------------------------------

### Reject Tool Call with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Employ the 'reject' decision type to prevent a tool call from executing and provide feedback. The 'message' field in the decision object contains the explanation for rejection, which is added to the conversation history to guide the agent.

```typescript
await agent.invoke(
    new Command({
        // Decisions are provided as a list, one per action under review.
        // The order of decisions must match the order of actions
        // listed in the `__interrupt__` request.
        resume: {
            decisions: [
                {
                    type: "reject",
                    // An explanation about why the action was rejected
                    message: "No, this is wrong because ..., instead do this ...",
                }
            ]
        }
    }),
    config  // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Constructing AIMessage from Streamed Chunks

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to aggregate streamed AIMessageChunk objects into a single AIMessage. The code iterates through the stream, concatenating each chunk to a running `full` message. This allows the complete response to be treated as a single message, suitable for conversational history or further processing. The example logs the accumulating text and the final content blocks.

```typescript
let full: AIMessageChunk | null = null;
for await (const chunk of stream) {
  full = full ? full.concat(chunk) : chunk;
  console.log(full.text);
}

console.log(full.contentBlocks);
// [{"type": "text", "text": "The sky is typically blue..."}]
```

--------------------------------

### Initialize Pinecone Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the PineconeStore, connecting to a specified Pinecone index. Requires Pinecone API key and index name, along with optional configurations like `maxConcurrency`.

```typescript
import { PineconeStore } from "@langchain/pinecone";
import { Pinecone as PineconeClient } from "@pinecone-database/pinecone";

const pinecone = new PineconeClient({
  apiKey: process.env.PINECONE_API_KEY,
});
const pineconeIndex = pinecone.Index("your-index-name");

const vectorStore = new PineconeStore(embeddings, {
  pineconeIndex,
  maxConcurrency: 5,
});
```

--------------------------------

### LangChain.js Handle Specific Exceptions for Structured Output Errors

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example shows how to handle specific exceptions, such as `ToolInputParsingException`, when dealing with structured output errors. The `handleError` function can selectively return custom messages for certain error types while letting others propagate, allowing for fine-grained control over error responses.

```typescript
import { ToolInputParsingException } from "@langchain/core/tools";

const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        return error.message;
    }
)

// Only validation errors get retried with default message:
// { role: "tool", content: "Error: Failed to parse structured output for tool 'ProductRating': ...\n Please fix your mistakes." }
```

--------------------------------

### Access Short-Term Memory in Middleware for Dynamic Prompts (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example shows how to leverage short-term memory within middleware to construct dynamic prompts. It defines a context schema, a `getWeather` tool, and an agent. The agent's `prompt` function utilizes the `context.userName` to personalize the system message, addressing the user by name. The agent is then invoked with user messages and specific context, demonstrating how the prompt is dynamically generated based on the provided context.

```typescript
import * as z from "zod";
import { createAgent, tool, SystemMessage } from "langchain";

const contextSchema = z.object({
    userName: z.string(),
});

const getWeather = tool(
    async ({ city }, config) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get user info",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getWeather],
    contextSchema,
    prompt: (state, config) => {
        return [
        new SystemMessage(
            `You are a helpful assistant. Address the user as ${config.context?.userName}.`
        ),
        ...state.messages,
    };
});

const result = await agent.invoke(
    {
        messages: [{ role: "user", content: "What is the weather in SF?" }],
    },
    {
        context: {
        userName: "John Smith",
        },
    }
);

for (const message of result.messages) {
    console.log(message);
}
/**
 * HumanMessage {
 *   "content": "What is the weather in SF?",
 *   // ...
 * }
 * AIMessage {
 *   // ...
 *   "tool_calls": [
 *     {
 *       "name": "get_weather",
 *       "args": {
 *         "city": "San Francisco"
 *       },
 *       "type": "tool_call",
 *       "id": "call_tCidbv0apTpQpEWb3O2zQ4Yx"
 *     }
 *   ],
 *   // ...
 * }
 * ToolMessage {
 *   "content": "The weather in San Francisco is always sunny!",
 *   "tool_call_id": "call_tCidbv0apTpQpEWb3O2zQ4Yx"
 *   // ...
 * }
 * AIMessage {
 *   "content": "John Smith, here's the latest: The weather in San Francisco is always sunny!\n\nIf you'd like more details (temperature, wind, humidity) or a forecast for the next few days, I can pull that up. What would you like?",
 *   // ...
 * }
 */
```

--------------------------------

### Pass Conversational Context to Sub-agents in LangGraph.js

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript example shows how to enrich sub-agents with additional conversational context beyond the immediate request. It utilizes `getCurrentTaskInput` to access the full message history and constructs a more informative prompt for the `calendarAgent`, enabling it to understand historical context for tasks like scheduling. Dependencies include `@langchain/langgraph` and `@langchain/core/messages`.

```typescript
import { getCurrentTaskInput } from "@langchain/langgraph";
import type { InternalAgentState } from "langchain";
import { HumanMessage } from "@langchain/core/messages";

const scheduleEvent = tool(
  async ({ request }, config) => {
    // Customize context received by sub-agent
    // Access full thread messages from the config
    const currentMessages = getCurrentTaskInput<InternalAgentState>(config).messages;

    const originalUserMessage = currentMessages.find(HumanMessage.isInstance);

    const prompt = `
    You are assisting with the following user inquiry:

    ${originalUserMessage?.content || "No context available"}

    You are tasked with the following sub-request:

    ${request}
    `.trim();

    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: prompt }],
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### Run Agent and Handle Human Review Interruption (Python)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

This code demonstrates how to run the configured agent and handle the interruption that occurs when the `HumanInTheLoopMiddleware` pauses execution. It prints the pending tool request details when an interruption occurs.

```python
question = "Which genre on average has the longest tracks?"
config = {"configurable": {"thread_id": "1"}}

for step in agent.stream(
    {"messages": [{"role": "user", "content": question}]},
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        print("INTERRUPTED:")
        interrupt = step["__interrupt__"][0]
        for request in interrupt.value["action_requests"]:
            print(request["description"])
    else:
        pass
```

--------------------------------

### Langgraph Configuration File

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Defines the configuration for a Langgraph agent, specifying dependencies, graph entry points, and environment file. This JSON file is essential for initializing and running the agent.

```json
{
  "dependencies": [""],
  "graphs": {
      "agent": "./sqlAgent.ts:agent",
      "graph": "./sqlAgentLanggraph.ts:graph"
  },
  "env": ".env"
}
```

--------------------------------

### Integrate Message Deletion into Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example shows how to integrate the `deleteMessages` function as a `postModelHook` within a LangGraph agent. It defines an agent using `createAgent` and configures it to use `MemorySaver` as a checkpointer. The `deleteMessages` function is called after the model generates a response to manage the message history.

```typescript
import { RemoveMessage } from "@langchain/core/messages";
import { AgentState, createAgent } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const deleteMessages = (state: AgentState) => {
    const messages = state.messages;
    if (messages.length > 2) {
        // remove the earliest two messages
        return {
        messages: messages
            .slice(0, 2)
            .map((m) => new RemoveMessage({ id: m.id! })),
        };
    }
    return {};
};

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [],
    prompt: "Please be concise and to the point.",
    postModelHook: deleteMessages,
    checkpointer: new MemorySaver(),
});

const config = { configurable: { thread_id: "1" } };

const streamA = await agent.stream(
    { messages: [{ role: "user", content: "hi! I'm bob" }] },
    { ...config, streamMode: "values" }
);
for await (const event of streamA) {
    const messageDetails = event.messages.map((message) => [
        message.getType(),
        message.content,
    ]);
    console.log(messageDetails);
}

const streamB = await agent.stream(
    {
        messages: [{ role: "user", content: "what's my name?" }],
    },
    { ...config, streamMode: "values" }
);
for await (const event of streamB) {
    const messageDetails = event.messages.map((message) => [
        message.getType(),
        message.content,
    ]);
    console.log(messageDetails);
}
```

--------------------------------

### Define a Basic Tool with Zod Schema - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Demonstrates how to define a basic tool using the `tool` function from the `langchain` package and `zod` for input schema definition. This tool, `searchDatabase`, takes a query and a limit as input.

```typescript
import * as z from "zod"
import { tool } from "langchain"

const searchDatabase = tool(
  ({ query, limit }) => `Found ${limit} results for '${query}'`,
  {
    name: "search_database",
    description: "Search the customer database for records matching the query.",
    schema: z.object({
      query: z.string().describe("Search terms to look for"),
      limit: z.number().describe("Maximum number of results to return"),
    }),
  }
);
```

--------------------------------

### Modify Agent Short-Term Memory with Tools (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example demonstrates how to update an agent's short-term memory by returning state updates from a tool. It defines a custom state schema with messages and a userName, and then creates two tools: `updateUserInfo` to fetch and set user information, and `greet` to use that information. The `createAgent` function is used to build the agent, which is then invoked with initial messages and context. The `updateUserInfo` tool returns a `Command` object containing `update` instructions to modify the agent's state, including the `userName` and the message history.

```typescript
import * as z from "zod";
import { tool, createAgent } from "langchain";
import { MessagesZodState, Command } from "@langchain/langgraph";

const CustomState = z.object({
    messages: MessagesZodState.shape.messages,
    userName: z.string().optional(),
});

const updateUserInfo = tool(
    async (_, config) => {
        const userId = config.context?.userId;
        const name = userId === "user_123" ? "John Smith" : "Unknown user";
        return new Command({
        update: {
            userName: name,
            // update the message history
            messages: [
            {
                role: "tool",
                content: "Successfully looked up user information",
                tool_call_id: config.toolCall?.id,
            },
            ],
        },
        });
    },
    {
        name: "update_user_info",
        description: "Look up and update user info.",
        schema: z.object({}),
    }
);

const greet = tool(
    async (_, config) => {
        const userName = config.context?.userName;
        return `Hello ${userName}!`;
    },
    {
        name: "greet",
        description: "Use this to greet the user once you found their info.",
        schema: z.object({}),
    }
);

const agent = createAgent({
    model, // Assuming 'model' is defined elsewhere
    tools: [updateUserInfo, greet],
    stateSchema: CustomState,
});

await agent.invoke(
    { messages: [{ role: "user", content: "greet the user" }] },
    { context: { userId: "user_123" } }
);
```

--------------------------------

### LangChain JavaScript Error Handling for Multiple Structured Outputs

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Illustrates LangChain's automatic error handling for scenarios where a model incorrectly generates multiple structured outputs. The agent identifies the error, provides feedback in a `ToolMessage`, and prompts the model to retry, ensuring robust structured data extraction. This example uses 'zod' for defining schemas and 'langchain' for agent creation.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ContactInfo = z.object({
    name: z.string().describe("Person's name"),
    email: z.string().describe("Email address"),
});

const EventDetails = z.object({
    event_name: z.string().describe("Name of the event"),
    date: z.string().describe("Event date"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy([ContactInfo, EventDetails]),
});

const result = await agent.invoke({
    messages: [
        {
        role: "user",
        content:
            "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th",
        },
    ],
});

console.log(result);

/**
 * {
 *   messages: [
 *     { role: "user", content: "Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_1" }, { name: "EventDetails", args: { event_name: "Tech Conference", date: "March 15th" }, id: "call_2" } ] },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_1", name: "ContactInfo" },
 *     { role: "tool", content: "Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected.\n Please fix your mistakes.", tool_call_id: "call_2", name: "EventDetails" },
 *     { role: "assistant", content: "", tool_calls: [ { name: "ContactInfo", args: { name: "John Doe", email: "john@email.com" }, id: "call_3" } ] },
 *     { role: "tool", content: "Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'}", tool_call_id: "call_3", name: "ContactInfo" }
 *   ],
 *   structuredResponse: { name: "John Doe", email: "john@email.com" }
 * }
 */
```

--------------------------------

### Custom State Schema Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Demonstrates how to extend an agent's state with custom properties using middleware. This example defines a `stateSchema` using Zod for `modelCallCount` and `userId`, and uses `beforeModel` and `afterModel` hooks to interact with these custom state properties. TypeScript enforces the presence of these properties when invoking the agent.

```typescript
import { createMiddleware, createAgent, HumanMessage } from "langchain";
import * as z from "zod";

// Middleware with custom state requirements
const callCounterMiddleware = createMiddleware({
  name: "CallCounterMiddleware",
  stateSchema: z.object({
    modelCallCount: z.number().default(0),
    userId: z.string().optional(),
  }),
  beforeModel: (state) => {
    // Access custom state properties
    if (state.modelCallCount > 10) {
      return { jumpTo: "end" };
    }
    return;
  },
  afterModel: (state) => {
    // Update custom state
    return { modelCallCount: state.modelCallCount + 1 };
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [callCounterMiddleware] as const,
});

// TypeScript enforces required state properties
const result = await agent.invoke({
  messages: [new HumanMessage("Hello")],
  modelCallCount: 0, // Optional due to default value
  userId: "user-123", // Optional
});
```

--------------------------------

### Execute Agent with User Query (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This snippet demonstrates how to execute the previously created LangChain.js agent with a user's input query. It initializes an agent input object with a user message and then streams the agent's execution steps, printing each message from the agent's response. This allows for observing the agent's reasoning process and final output.

```typescript
let inputMessage = `What is Task Decomposition?`;

let agentInputs = { messages: [{ role: "user", content: inputMessage }] };

for await (const step of await agent.stream(agentInputs, {
  streamMode: "values",
})) {
  const lastMessage = step.messages[step.messages.length - 1];
  prettyPrint(lastMessage);
  console.log("-----\n");
}

```

--------------------------------

### LangChain.js Disable Error Handling for Structured Output

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This example shows how to completely disable error handling for structured output by setting `handleError` to `false`. When error handling is disabled, all errors will be raised directly, providing the raw error information without any intermediary processing or custom messages.

```typescript
const responseFormat = toolStrategy(ProductRating, {
    handleError: false  // All errors raised
)
```

--------------------------------

### LangChain.js Custom Error Message for Structured Output

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This code illustrates how to provide a custom error message when structured output fails validation. By passing a string to the `handleError` option within `toolStrategy`, you can define a user-friendly message that replaces the default error feedback, guiding the user on how to correct their input.

```typescript
const responseFormat = toolStrategy(ProductRating, {
    handleError: "Please provide a valid rating between 1-5 and include a comment."
})

// Error message becomes:
// { role: "tool", content: "Please provide a valid rating between 1-5 and include a comment." }
```

--------------------------------

### Summarize Agent Messages with Summarization Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This code demonstrates how to use `summarizationMiddleware` to automatically summarize the message history of a Langchain agent. It configures the middleware to use a specific model (`gpt-4o-mini`), set a token threshold for summarization (`maxTokensBeforeSummary`), and define the number of messages to keep (`messagesToKeep`). The example invokes the agent multiple times and then retrieves the final response, showing how summarization helps manage conversation context. Dependencies include `langchain` and `@langchain/langgraph`.

```typescript
import { createAgent, summarizationMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const checkpointer = new MemorySaver();

const agent = createAgent({
  model: "gpt-4o",
  tools: [],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      maxTokensBeforeSummary: 4000,
      messagesToKeep: 20,
    }),
  ],
  checkpointer,
});

const config = { configurable: { thread_id: "1" } };
await agent.invoke({ messages: "hi, my name is bob" }, config);
await agent.invoke({ messages: "write a short poem about cats" }, config);
await agent.invoke({ messages: "now do the same but for dogs" }, config);
const finalResponse = await agent.invoke({ messages: "what's my name?" }, config);

console.log(finalResponse.messages.at(-1)?.content);
// Your name is Bob!
```

--------------------------------

### Pass Conversation History to Subagent via State in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent

This example illustrates how to pass the full conversation history to a subagent by accessing the main agent's state. It customizes the tool definition to retrieve messages and other state keys from `getCurrentTaskInput<AgentState>()` and passes them to the subagent's `invoke` method. This allows for more context-aware subagent execution.

```typescript
import { createAgent, tool, AgentState, ToolMessage } from "langchain";
import { Command } from "@langchain/langgraph";
import * as z from "zod";

// Example of passing the full conversation history to the sub agent via the state.
const callSubagent1 = tool(
  async ({query}) => {
    const state = getCurrentTaskInput<AgentState>();
    // Apply any logic needed to transform the messages into a suitable input
    const subAgentInput = someLogic(query, state.messages);
    const result = await subagent1.invoke({
      messages: subAgentInput,
      // You could also pass other state keys here as needed.
      // Make sure to define these in both the main and subagent's
      // state schemas.
      exampleStateKey: state.exampleStateKey
    });
    return result.messages.at(-1)?.content;
  },
  {
    name: "subagent1_name",
    description: "subagent1_description",
  }
);

```

--------------------------------

### Configure Middleware to Jump to 'end' in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript example demonstrates how to configure a middleware function, `conditionalMiddleware`, to conditionally jump to the end of the agent's execution. The `afterModel` hook checks `someCondition(state)`; if true, it returns an object with `jumpTo: "end"`. This allows for controlled early termination based on specific state conditions.

```typescript
import { createMiddleware } from "langchain";

const conditionalMiddleware = createMiddleware({
  name: "ConditionalMiddleware",
  afterModel: (state) => {
    if (someCondition(state)) {
      return { jumpTo: "end" };
    }
    return;
  },
});
```

--------------------------------

### Create Supervisor Agent and Manage Email Tool

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines the 'manage_email' tool for sending emails via natural language and initializes the supervisor agent with scheduling and email tools. The supervisor agent uses a system prompt to understand its role as a personal assistant that can coordinate tool calls.

```javascript
const lastMessage = result.messages[result.messages.length - 1];
      return lastMessage.text;
    },
    {
      name: "manage_email",
      description: `
  Send emails using natural language.

  Use this when the user wants to send notifications, reminders, or any email communication.
  Handles recipient extraction, subject generation, and email composition.

  Input: Natural language email request (e.g., 'send them a reminder about the meeting')
      `.trim(),
      schema: z.object({
        request: z.string().describe("Natural language email request"),
      }),
    }
  );

  // ============================================================================
  // Step 4: Create the supervisor agent
  // ============================================================================

  const supervisorAgent = createAgent({
    model: llm,
    tools: [scheduleEvent, manageEmail],
    systemPrompt: `
  You are a helpful personal assistant.
  You can schedule calendar events and send emails.
  Break down user requests into appropriate tool calls and coordinate the results.
  When a request involves multiple actions, use multiple tools in sequence.
    `.trim(),
  });
```

--------------------------------

### Access Agent Short-Term Memory in a Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This example shows how to access an agent's short-term memory (state) from within a tool using the `tool_runtime` parameter. The `stateSchema` defines the structure of the memory. The `getUserInfo` tool is defined using `langchain`'s `tool` utility, and it accesses the `userId` from the `config.context`. The agent is created with this tool and the state schema. When invoked, the agent passes context including `userId`, which the tool can then use. Dependencies include `zod` and `langchain`.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";

const stateSchema = z.object({
    userId: z.string(),
});

const getUserInfo = tool(
    async (_, config) => {
        const userId = config.context?.userId;
        return { userId };
    },
    {
        name: "get_user_info",
        description: "Get user info",
        schema: z.object({}),
    }
);

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [getUserInfo],
    stateSchema,
});

const result = await agent.invoke(
    {
        messages: [{ role: "user", content: "what's my name?" }],
    },
    {
        context: {
        userId: "user_123",
        },
    }
);

console.log(result.messages.at(-1)?.content);
// Outputs: "User is John Smith."
```

--------------------------------

### Initialize Chroma Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the Chroma vector store with embeddings and collection configuration. Requires a `collectionName` to specify the target collection within Chroma.

```typescript
import { Chroma } from "@langchain/community/vectorstores/chroma";

const vectorStore = new Chroma(embeddings, {
  collectionName: "a-test-collection",
});
```

--------------------------------

### Configure Google VertexAI Credentials

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the GOOGLE_APPLICATION_CREDENTIALS environment variable to point to the JSON key file for authentication with Google Cloud.

```bash
GOOGLE_APPLICATION_CREDENTIALS=credentials.json
```

--------------------------------

### Context Management with Context Editing Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript snippet illustrates how to manage conversation context in Langchain agents using `contextEditingMiddleware`. It's ideal for long conversations requiring cleanup, such as removing failed tool attempts or implementing custom context management. The example shows how to clear old tool uses based on a token limit.

```typescript
import { createAgent, contextEditingMiddleware, ClearToolUsesEdit } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    contextEditingMiddleware({
      edits: [
        new ClearToolUsesEdit({ maxTokens: 1000 }), // Clear old tool uses
      ],
    }),
  ],
});

```

--------------------------------

### Execute Supervisor Agent with User Request

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Demonstrates how to use the supervisor agent to process a complex user request that requires coordinating multiple tools (scheduling and email). It streams the agent's execution steps and logs the formatted messages to the console.

```javascript
// ============================================================================
  // Step 5: Use the supervisor
  // ============================================================================
  
  // Example: User request requiring both calendar and email coordination
  const userRequest =
    "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
    "and send them an email reminder about reviewing the new mockups.";

  console.log("User Request:", userRequest);
  console.log(`\n${"=".repeat(80)}\n`);

  const stream = await supervisorAgent.stream({
    messages: [{ role: "user", content: userRequest }],
  });

  for await (const step of stream) {
    for (const update of Object.values(step)) {
      if (update && typeof update === "object" && "messages" in update) {
        for (const message of update.messages) {
          console.log(message.toFormattedString());
        }
      }
    }
  }
```
```

--------------------------------

### Load MCP Tools Statelessly with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Demonstrates how to load MCP tools for stateless operations using the MultiServerMCPClient. This requires initializing the client and then loading tools associated with a specific server.

```typescript
import { loadMCPTools } from "@langchain/mcp-adapters/tools.js";
import { MultiServerMCPClient } from "@langchain/mcp-adapters/client.js";

const client = new MultiServerMCPClient({
  servers: [
    {
      name: "weather",
      url: "http://localhost:8080",
    },
  ],
});

const tools = await loadMCPTools(client, "weather");
console.log(`Weather MCP server running on port ${PORT}`);
});
```

--------------------------------

### Create LangChain Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Initializes a ReAct agent using `createAgent`. This agent is configured with a specific language model (`gpt-5`), a list of tools (including `executeSql`), and a custom system prompt obtained from `getSystemPrompt`. This sets up the agent for interactive SQL analysis.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-5",
  tools: [executeSql],
  systemPrompt: getSystemPrompt,
});

```

--------------------------------

### Initialize PGVector Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the PGVectorStore using `initialize`. This method is used to set up the connection and configuration for interacting with a PostgreSQL database containing vector data.

```typescript
import { PGVectorStore } from "@langchain/community/vectorstores/pgvector";

const vectorStore = await PGVectorStore.initialize(embeddings, {})
```

--------------------------------

### Initialize AWS Bedrock Chat Model (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Initializes a chat model using AWS Bedrock with the 'bedrock:gpt-4.1' identifier. Ensure AWS credentials are configured according to Bedrock documentation.

```javascript
const model = await initChatModel("bedrock:gpt-4.1");
```

--------------------------------

### Create LangChain Agent with RAG Middleware in JavaScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This JavaScript code demonstrates how to create a LangChain agent using the `createAgent` function. It configures the agent with a language model, an empty list of tools, and the `retrieveDocumentsMiddleware` to enable Retrieval-Augmented Generation capabilities.

```javascript
const agent = createAgent({
    model,
    tools: [],
    middleware: [retrieveDocumentsMiddleware],
  });
```

--------------------------------

### Initialize Qdrant Vector Store with Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Initializes the Qdrant vector store from an existing collection. This requires the embeddings object, the Qdrant URL, and the collection name.

```typescript
import { QdrantVectorStore } from "@langchain/qdrant";

const vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {
  url: process.env.QDRANT_URL,
  collectionName: "langchainjs-testing",
});
```

--------------------------------

### Load Web Content with CheerioWebBaseLoader

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Loads content from a specified URL using the CheerioWebBaseLoader, targeting paragraph elements. It returns a list of Document objects, with the first document's page content length logged to the console. This is the first step in preparing data for indexing.

```typescript
import "cheerio";
import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";

const pTagSelector = "p";
const cheerioLoader = new CheerioWebBaseLoader(
  "https://lilianweng.github.io/posts/2023-06-23-agent/",
  {
    selector: pTagSelector,
  }
);

const docs = await cheerioLoader.load();

console.assert(docs.length === 1);
console.log(`Total characters: ${docs[0].pageContent.length}`);
```

--------------------------------

### Configure Cohere API Key

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the COHERE_API_KEY environment variable for authenticating with the Cohere API.

```bash
COHERE_API_KEY=your-api-key
```

--------------------------------

### Invoke Model with Configuration (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates invoking a model with various configuration options. This includes setting a custom `runName`, `tags` for categorization, `metadata` for additional context, and `callbacks` for event handling. These configurations are useful for debugging, logging, and tracking invocations.

```typescript
const response = await model.invoke(
    "Tell me a joke",
    {
        runName: "joke_generation",      // Custom name for this run
        tags: ["humor", "demo"],          // Tags for categorization
        metadata: {"user_id": "123"},     // Custom metadata
        callbacks: [my_callback_handler], // Callback handlers
    }
)
```

--------------------------------

### Create and Test Email Agent in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines an email agent prompt and creates an agent using a language model and the `sendEmail` tool. It then tests the agent with a natural language request to send a reminder email, streams the response, and logs the formatted messages. The agent demonstrates inferring recipients, crafting professional emails, and confirming the action.

```typescript
const EMAIL_AGENT_PROMPT = `
You are an email assistant.
Compose professional emails based on natural language requests.
Extract recipient information and craft appropriate subject lines and body text.
Use send_email to send the message.
Always confirm what was sent in your final response.
`.trim();

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: EMAIL_AGENT_PROMPT,
});

const query = "Send the design team a reminder about reviewing the new mockups";

const stream = await emailAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Initialize Anthropic Chat Model in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Shows how to initialize an Anthropic chat model using `initChatModel` or the `ChatAnthropic` class. Requires the Anthropic API key and specifies the model name (e.g., 'claude-sonnet-4-5-20250929').

```typescript
import { initChatModel } from "langchain";

process.env.ANTHROPIC_API_KEY = "your-api-key";

const model = await initChatModel("claude-sonnet-4-5-20250929");
```

```typescript
import { ChatAnthropic } from "@langchain/anthropic";

const model = new ChatAnthropic({
  model: "claude-sonnet-4-5-20250929",
  apiKey: "your-api-key"
});
```

--------------------------------

### Create Agent with Model Instance - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Initializes a Langchain.js agent using a pre-configured model instance. This provides granular control over model parameters like temperature, max tokens, and timeouts. It requires the `langchain` and the specific provider package (e.g., `@langchain/openai`).

```typescript
import { createAgent } from "langchain";
import { ChatOpenAI } from "@langchain/openai";

const model = new ChatOpenAI({
  model: "gpt-4o",
  temperature: 0.1,
  maxTokens: 1000,
  timeout: 30
});

const agent = createAgent({
  model,
  tools: []
});
```

--------------------------------

### Enable LangSmith Tracing with Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Set these environment variables to enable automatic tracing of LangChain agents to LangSmith. Ensure you replace '<your-api-key>' with your actual LangSmith API key.

```bash
export LANGSMITH_TRACING=true
export LANGSMITH_API_KEY=<your-api-key>
```

--------------------------------

### Set Up LangSmith Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/test

Configure your environment variables to enable LangSmith tracing. This is a prerequisite for logging evaluator results and utilizing LangSmith's features for experiment tracking.

```bash
export LANGSMITH_API_KEY="your_langsmith_api_key"
export LANGSMITH_TRACING="true"
```

--------------------------------

### Test Calendar Agent with Natural Language Query

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This code snippet shows how to test the initialized calendar agent. It sends a natural language scheduling request to the agent and streams the response. The output is processed to display the agent's actions, including tool calls for checking availability and creating events, and the final natural language confirmation.

```typescript
const query = "Schedule a team meeting next Tuesday at 2pm for 1 hour";

const stream = await calendarAgent.stream({
  messages: [{ role: "user", content: query }]
});

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Dynamic System Prompt based on User Preferences (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Creates a system prompt that incorporates user preferences, such as communication style, retrieved from a long-term store. It uses Zod for context schema validation and requires 'langchain' and 'zod' packages.

```typescript
import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

type Context = z.infer<typeof contextSchema>;

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  contextSchema,
  middleware: [
    dynamicSystemPromptMiddleware<Context>(async (state, runtime) => {
      const userId = runtime.context.userId;

      // Read from Store: get user preferences
      const store = runtime.store;
      const userPrefs = await store.get(["preferences"], userId);

      let base = "You are a helpful assistant.";

      if (userPrefs) {
        const style = userPrefs.value?.communicationStyle || "balanced";
        base += `\nUser prefers ${style} responses.`;
      }

      return base;
    }),
  ],
});
```

--------------------------------

### Use Multiple MCP Servers with LangChain Agent

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Connects to multiple MCP servers using `MultiServerMCPClient` from @langchain/mcp-adapters. It demonstrates setting up connections via 'stdio' for local tools and 'sse' for remote tools, then retrieves tools and creates an agent to invoke them.

```typescript
import { MultiServerMCPClient } from "@langchain/mcp-adapters";
import { ChatAnthropic } from "@langchain/anthropic";
import { createAgent } from "langchain";

const client = new MultiServerMCPClient({
    math: {
        transport: "stdio",  // Local subprocess communication
        command: "node",
        // Replace with absolute path to your math_server.js file
        args: ["/path/to/math_server.js"],
    },
    weather: {
        transport: "sse",  // Server-Sent Events for streaming
        // Ensure you start your weather server on port 8000
        url: "http://localhost:8000/mcp",
    },
});

const tools = await client.getTools();
const agent = createAgent({
    model: "claude-sonnet-4-5-20250929",
    tools,
});

const mathResponse = await agent.invoke({
    messages: [{ role: "user", content: "what's (3 + 5) x 12?" }],
});

const weatherResponse = await agent.invoke({
    messages: [{ role: "user", content: "what is the weather in nyc?" }],
});
```

--------------------------------

### Dynamic System Prompt based on Runtime Context (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Configures a system prompt dynamically based on runtime context, such as user role and deployment environment. It uses Zod for context schema definition and requires 'langchain' and 'zod' packages.

```typescript
import * as z from "zod";
import { createAgent, dynamicSystemPromptMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.string(),
  deploymentEnv: z.string(),
});

type Context = z.infer<typeof contextSchema>;

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  contextSchema,
  middleware: [
    dynamicSystemPromptMiddleware<Context>((state, runtime) => {
      // Read from Runtime Context: user role and environment
      const userRole = runtime.context.userRole;
      const env = runtime.context.deploymentEnv;

      let base = "You are a helpful assistant.";

      if (userRole === "admin") {
        base += "\nYou have admin access. You can perform all operations.";
      } else if (userRole === "viewer") {
        base += "\nYou have read-only access. Guide users to read operations only.";
      }

      if (env === "production") {
        base += "\nBe extra careful with any data modifications.";
      }

      return base;
    }),
  ],
});
```

--------------------------------

### Tool Strategy Configuration

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Configure a `ToolStrategy` to enable tool calling for structured output generation. This strategy is compatible with most modern models that support tool calling.

```APIDOC
## Tool Strategy Configuration

### Description
Configure a `ToolStrategy` to enable tool calling for structured output generation. This strategy is compatible with most modern models that support tool calling.

### Method
`toolStrategy<StructuredResponseT>`

### Endpoint
N/A (Function within LangChain library)

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **responseFormat** (JsonSchemaFormat | ZodSchema<StructuredResponseT> | Array<ZodSchema<StructuredResponseT> | JsonSchemaFormat>) - Required - The schema defining the structured output format. Supports Zod Schema and JSON Schema.
- **options** (ToolStrategyOptions) - Optional - Configuration options for the tool strategy.
  - **options.toolMessageContent** (string) - Custom content for the tool message returned when structured output is generated. Defaults to a message showing the structured response data.
  - **options.handleError** (boolean | (error: ToolStrategyError) => string | Promise<string>) - Options parameter containing an optional `handleError` parameter for customizing the error handling strategy.
    - `true`: Catch all errors with default error template (default).
    - `False`: No retry, let exceptions propagate.
    - `(error: ToolStrategyError) => string | Promise<string>`: retry with the provided message or throw the error.

### Request Example
```ts
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const ProductReview = z.object({
    rating: z.number().min(1).max(5).optional(),
    sentiment: z.enum(["positive", "negative"]),
    keyPoints: z.array(z.string()).describe("The key points of the review. Lowercase, 1-3 words each."),
});

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: toolStrategy(ProductReview)
})

result = agent.invoke({
    "messages": [{"role": "user", "content": "Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'"}]
})

console.log(result.structuredResponse);
// { "rating": 5, "sentiment": "positive", "keyPoints": ["fast shipping", "expensive"] }
```

### Response
#### Success Response (200)
- **structuredResponse** (object) - The structured output generated by the model based on the provided schema.

#### Response Example
```json
{
  "rating": 5,
  "sentiment": "positive",
  "keyPoints": ["fast shipping", "expensive"]
}
```
```

--------------------------------

### Test RAG Agent with Multi-Step Query (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This code snippet demonstrates how to test a RAG agent constructed with Langchain.js. It defines an input message that requires iterative retrieval steps. The agent's `stream` method is called with the input, and the output messages are logged to the console, showing the agent's process of querying the retrieval tool and formulating an answer.

```typescript
let inputMessage = `What is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.`;

let agentInputs = { messages: [{ role: "user", content: inputMessage }] };

const stream = await agent.stream(agentInputs, {
  streamMode: "values",
});
for await (const step of stream) {
  const lastMessage = step.messages[step.messages.length - 1];
  console.log(`[${lastMessage.role}]: ${lastMessage.content}`);
  console.log("-----\n");
}
```

--------------------------------

### Configure Azure OpenAI Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets environment variables required for Azure OpenAI embeddings, including the instance name, API key, and API version.

```bash
AZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>
AZURE_OPENAI_API_KEY=<YOUR_KEY>
AZURE_OPENAI_API_VERSION="2024-02-01"
```

--------------------------------

### Initialize Chat Model and Invoke with Messages (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates initializing a chat model and invoking it with a list of SystemMessage and HumanMessage objects. This is useful for setting up basic conversational context.

```typescript
import { initChatModel, HumanMessage, SystemMessage } from "langchain";

const model = await initChatModel("gpt-5-nano");

const systemMsg = new SystemMessage("You are a helpful assistant.");
const humanMsg = new HumanMessage("Hello, how are you?");

const messages = [systemMsg, humanMsg];
const response = await model.invoke(messages);  // Returns AIMessage
```

--------------------------------

### Download and Resolve SQLite Database Path (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Downloads the Chinook SQLite database from a GCS bucket if it does not exist locally. It saves the database to the local file system and returns its path. Requires 'node:fs/promises' and 'node:path' modules.

```typescript
import fs from "node:fs/promises";
import path from "node:path";

const url = "https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db";
const localPath = path.resolve("Chinook.db");

async function resolveDbPath() {
  if (await fs.exists(localPath)) {
    return localPath;
  }
  const resp = await fetch(url);
  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);
  const buf = Buffer.from(await resp.arrayBuffer());
  await fs.writeFile(localPath, buf);
  return localPath;
}
```

--------------------------------

### Dynamic Model Selection with Middleware in Javascript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Demonstrates selecting between different models (e.g., 'gpt-4o-mini', 'gpt-4o') at runtime based on conversation state, such as message count. This allows for cost optimization and sophisticated routing logic. Requires 'langchain' package.

```typescript
import { ChatOpenAI } from "@langchain/openai";
import { createAgent, createMiddleware } from "langchain";

const basicModel = new ChatOpenAI({ model: "gpt-4o-mini" });
const advancedModel = new ChatOpenAI({ model: "gpt-4o" });

const dynamicModelSelection = createMiddleware({
  name: "DynamicModelSelection",
  wrapModelCall: (request, handler) => {
    // Choose model based on conversation complexity
    const messageCount = request.messages.length;

    return handler({
        ...request,
        model: messageCount > 10 ? advancedModel : basicModel,
    });
  },
});

const agent = createAgent({
  model: "gpt-4o-mini", // Base model (used when messageCount ≤ 10)
  tools,
  middleware: [dynamicModelSelection] as const,
});
```

--------------------------------

### Stream Custom Updates with LangChain Stream Writer (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Illustrates how to use `config.streamWriter` to send custom, real-time updates from tools as they execute. This is valuable for providing immediate user feedback on tool progress. It requires the `langchain` package and uses Zod for schema definition.

```typescript
import * as z from "zod";
import { tool } from "langchain";

const getWeather = tool(
  ({ city }, config) => {
    const writer = config.streamWriter;

    // Stream custom updates as the tool executes
    writer(`Looking up data for city: ${city}`);
    writer(`Acquired data for city: ${city}`);

    return `It's always sunny in ${city}!`;
  },
  {
    name: "get_weather",
    description: "Get weather for a given city.",
    schema: z.object({
      city: z.string(),
    }),
  }
);

```

--------------------------------

### Select Model Based on Runtime Context (Cost/Environment)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware selects a language model based on information available in the runtime context, such as cost tier and environment. It defines a Zod schema for context (costTier, environment) and uses predefined models (premium, standard, budget). The model selection logic prioritizes a 'premium' cost tier in a 'production' environment, falls back to 'budget' if specified, and defaults to 'standard' otherwise.

```typescript
import * as z from "zod";
import { createMiddleware, initChatModel } from "langchain";

const contextSchema = z.object({
  costTier: z.string(),
  environment: z.string(),
});

// Initialize models once outside the middleware
const premiumModel = initChatModel("claude-sonnet-4-5-20250929");
const standardModel = initChatModel("gpt-4o");
const budgetModel = initChatModel("gpt-4o-mini");

const contextBasedModel = createMiddleware({
  name: "ContextBasedModel",
  contextSchema,
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: cost tier and environment
    const costTier = request.runtime.context.costTier;  // [!code highlight]
    const environment = request.runtime.context.environment;  // [!code highlight]

    let model;

    if (environment === "production" && costTier === "premium") {
      model = premiumModel;
    } else if (costTier === "budget") {
      model = budgetModel;
    } else {
      model = standardModel;
    }

    return handler({ ...request, model });  // [!code highlight]
  },
});
```

--------------------------------

### Use Langchain ClientSession for Stateful MCP Tool Usage

Source: https://docs.langchain.com/oss/javascript/langchain/mcp

Illustrates setting up a stateful server connection using `client.session()` for maintaining context between tool calls. This is essential for servers that require persistent state.

```typescript
import { loadMCPTools } from "@langchain/mcp-adapters/tools.js";
import { MultiServerMCPClient } from "@langchain/mcp-adapters/client.js";

const client = new MultiServerMCPClient({...});
const session = await client.session("math");
const tools = await loadMCPTools(session);
```

--------------------------------

### Access and Update Memory with LangChain Store (JavaScript)

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Demonstrates how to utilize the LangChain store for persisting data across conversations. It shows how to retrieve user information using `store.get` and save information using `store.put`. This requires the `langchain` and `@langchain/openai` packages and utilizes Zod for schema validation.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";
import { InMemoryStore } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";

const store = new InMemoryStore();

// Access memory
const getUserInfo = tool(
  async ({ user_id }) => {
    const value = await store.get(["users"], user_id);
    console.log("get_user_info", user_id, value);
    return value;
  },
  {
    name: "get_user_info",
    description: "Look up user info.",
    schema: z.object({
      user_id: z.string(),
    }),
  }
);

// Update memory
const saveUserInfo = tool(
  async ({ user_id, name, age, email }) => {
    console.log("save_user_info", user_id, name, age, email);
    await store.put(["users"], user_id, { name, age, email });
    return "Successfully saved user info.";
  },
  {
    name: "save_user_info",
    description: "Save user info.",
    schema: z.object({
      user_id: z.string(),
      name: z.string(),
      age: z.number(),
      email: z.string(),
    }),
  }
);

const agent = createAgent({
  model: new ChatOpenAI({ model: "gpt-4o" }),
  tools: [getUserInfo, saveUserInfo],
  store,
});

// First session: save user info
await agent.invoke({
  messages: [
    {
      role: "user",
      content: "Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev",
    },
  ],
});

// Second session: get user info
const result = await agent.invoke({
  messages: [
    { role: "user", content: "Get user info for user with id 'abc123'" },
  ],
});

console.log(result);
// Here is the user info for user with ID "abc123":
// - Name: Foo
// - Age: 25
// - Email: foo@langchain.dev

```

--------------------------------

### Define Agent System Prompt (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Generates a `SystemMessage` for an agent, defining its role as a 'careful SQLite analyst'. It includes the database schema, rules for query execution (read-only, limit rows, error handling), and instructions on how to interact with the `execute_sql` tool.

```typescript
import { SystemMessage } from "langchain";

const getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.

Authoritative schema (do not invent columns/tables):
${await getSchema()}

Rules:
- Think step-by-step.
- When you need data, call the tool `execute_sql` with ONE SELECT query.
- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.
- Limit to 5 rows unless user explicitly asks otherwise.
- If the tool returns 'Error:', revise the SQL and try again.
- Limit the number of attempts to 5.
- If you are not successful after 5 attempts, return a note to the user.
- Prefer explicit column lists; avoid SELECT *.
`);

```

--------------------------------

### Implement RAG Tool for Vector Store Search (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This code snippet demonstrates how to implement a custom tool for a RAG agent in Langchain.js. It defines a Zod schema for the query, creates a tool that wraps a vector store's `similaritySearch` method, and specifies the tool's name, description, schema, and response format. The tool retrieves documents based on a query and serializes their source and content, also returning the raw documents as artifacts.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";

const retrieveSchema = z.object({ query: z.string() });

const retrieve = tool(
  async ({ query }) => {
    const retrievedDocs = await vectorStore.similaritySearch(query, 2);
    const serialized = retrievedDocs
      .map(
        (doc) => `Source: ${doc.metadata.source}\nContent: ${doc.pageContent}`
      )
      .join("\n");
    return [serialized, retrievedDocs];
  },
  {
    name: "retrieve",
    description: "Retrieve information related to a query.",
    schema: retrieveSchema,
    responseFormat: "content_and_artifact",
  }
);
```

--------------------------------

### Dynamic Response Format Selection based on User Preferences in Store

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Uses middleware to select response formats based on user preferences stored in a data store. It retrieves user preferences, checks for a 'responseStyle' (defaulting to 'concise'), and applies either a 'verbose' or 'concise' schema accordingly.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const verboseResponse = z.object({
  answer: z.string().describe("Detailed answer"),
  sources: z.array(z.string()).describe("Sources used"),
});

const conciseResponse = z.object({
  answer: z.string().describe("Brief answer"),
});

const storeBasedOutput = createMiddleware({
  name: "StoreBasedOutput",
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's preferred response style
    const store = request.runtime.store;  // [!code highlight]
    const userPrefs = await store.get(["preferences"], userId);  // [!code highlight]

    if (userPrefs) {
      const style = userPrefs.value?.responseStyle || "concise";
      if (style === "verbose") {
        request.responseFormat = verboseResponse;  // [!code highlight]
      } else {
        request.responseFormat = conciseResponse;  // [!code highlight]
      }
    }

    return handler(request);
  },
});
```

--------------------------------

### Implement Agent Task Planning with To-Do List Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This snippet demonstrates how to equip Langchain agents with task planning and tracking capabilities using the `todoListMiddleware`. It defines custom tools for file operations and testing, then creates an agent that utilizes the middleware to break down complex requests into manageable steps. The agent's progress is tracked via the `result.todos` output.

```typescript
import { createAgent, HumanMessage, todoListMiddleware, tool } from "langchain";
import * as z from "zod";

const readFile = tool(
  async ({ filePath }) => {
    // Read file implementation
    return "file contents";
  },
  {
    name: "read_file",
    description: "Read contents of a file",
    schema: z.object({ filePath: z.string() }),
  }
);

const writeFile = tool(
  async ({ filePath, content }) => {
    // Write file implementation
    return `Wrote ${content.length} characters to ${filePath}`;
  },
  {
    name: "write_file",
    description: "Write content to a file",
    schema: z.object({ filePath: z.string(), content: z.string() }),
  }
);

const runTests = tool(
  async ({ testPath }) => {
    // Run tests implementation
    return "All tests passed!";
  },
  {
    name: "run_tests",
    description: "Run tests and return results",
    schema: z.object({ testPath: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [readFile, writeFile, runTests],
  middleware: [todoListMiddleware()] as const,
});

const result = await agent.invoke({
  messages: [
    new HumanMessage(
      "Refactor the authentication module to use async/await and ensure all tests pass"
    ),
  ],
});

// The agent will use write_todos to plan and track:
// 1. Read current authentication module code
// 2. Identify functions that need async conversion
// 3. Refactor functions to async/await
// 4. Update function calls throughout codebase
// 5. Run tests and fix any failures

console.log(result.todos); // Track the agent's progress through each step

```

--------------------------------

### Configure AWS Bedrock Chat Model (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Instantiates a ChatBedrockConverse model for AWS Bedrock with a specified model name and region. Requires '@langchain/aws' package and AWS credential configuration.

```typescript
import { ChatBedrockConverse } from "@langchain/aws";

// Follow the steps here to configure your credentials:
// https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

const model = new ChatBedrockConverse({
  model: "gpt-4.1",
  region: "us-east-2"
});
```

--------------------------------

### Set LangSmith Environment Variables for Tracing

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Configures environment variables to enable tracing in LangSmith, which is crucial for inspecting and debugging complex LangChain applications. Requires setting LANGSMITH_TRACING to 'true' and providing a LANGSMITH_API_KEY.

```shell
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

--------------------------------

### Initialize OpenAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the OpenAIEmbeddings class from the @langchain/openai package. Requires specifying the model to use, such as 'text-embedding-3-large'.

```typescript
import { OpenAIEmbeddings } from "@langchain/openai";

const embeddings = new OpenAIEmbeddings({
  model: "text-embedding-3-large"
});
```

--------------------------------

### Agent with Provider Strategy (JSON Schema)

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Demonstrates creating an agent with a JSON schema for structured output using `providerStrategy`. This is an alternative to Zod schemas for models supporting native structured output.

```typescript
import { createAgent, providerStrategy } from "langchain";

const contactInfoSchema = {
    "type": "object",
    "description": "Contact information for a person.",
    "properties": {
        "name": {"type": "string", "description": "The name of the person"},
        "email": {"type": "string", "description": "The email address of the person"},
        "phone": {"type": "string", "description": "The phone number of the person"}
    },
    "required": ["name", "email", "phone"]
}

const agent = createAgent({
    model: "gpt-5",
    tools: tools,
    responseFormat: providerStrategy(contactInfoSchema)
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "Extract contact info from: John Doe, john@example.com, (555) 123-4567"}]
});

result.structuredResponse;
// { name: "John Doe", email: "john@example.com", phone: "(555) 123-4567" }
```

--------------------------------

### Embed Queries and Compare Vector Lengths in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Demonstrates how to use an initialized embeddings model to generate vectors for text and assert that the generated vectors have the same length. It then logs the first 10 elements of the first vector.

```typescript
const vector1 = await embeddings.embedQuery(allSplits[0].pageContent);
const vector2 = await embeddings.embedQuery(allSplits[1].pageContent);

assert vector1.length === vector2.length;
console.log(`Generated vectors of length ${vector1.length}\n`);
console.log(vector1.slice(0, 10));
```

--------------------------------

### Initialize Google VertexAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the VertexAIEmbeddings class from the @langchain/google-vertexai package. Requires specifying the Vertex AI model, such as 'gemini-embedding-001'.

```typescript
import { VertexAIEmbeddings } from "@langchain/google-vertexai";

const embeddings = new VertexAIEmbeddings({
  model: "gemini-embedding-001"
});
```

--------------------------------

### Create Sample LangChain Documents

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Demonstrates how to create custom Document objects in TypeScript for LangChain. Each Document contains page content, metadata (like source), and optionally an ID, representing a piece of text with associated information.

```typescript
import { Document } from "@langchain/core/documents";

const documents = [
  new Document({
    pageContent:
      "Dogs are great companions, known for their loyalty and friendliness.",
    metadata: { source: "mammal-pets-doc" },
  }),
  new Document({
    pageContent: "Cats are independent pets that often enjoy their own space.",
    metadata: { source: "mammal-pets-doc" },
  }),
];
```

--------------------------------

### Configure Human-in-the-loop Middleware in Langchain Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Demonstrates how to add and configure the `humanInTheLoopMiddleware` when creating a Langchain agent. It shows how to specify which tool calls should trigger an interrupt and what decisions are allowed for each. It also highlights the necessity of configuring a checkpointer for persistence, using `MemorySaver` for demonstration and suggesting persistent options like `AsyncPostgresSaver` for production. The `descriptionPrefix` option for interrupt messages is also illustrated.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const agent = createAgent({
    model: "gpt-4o",
    tools: [writeFileTool, executeSQLTool, readDataTool],
    middleware: [
        humanInTheLoopMiddleware({
            interruptOn: {
                write_file: true, // All decisions (approve, edit, reject) allowed
                execute_sql: {
                    allowedDecisions: ["approve", "reject"],
                    // No editing allowed
                    description: "🚨 SQL execution requires DBA approval",
                },
                // Safe operation, no approval needed
                read_data: false,
            },
            // Prefix for interrupt messages - combined with tool name and args to form the full message
            // e.g., "Tool execution pending approval: execute_sql with query='DELETE FROM..."'
            // Individual tools can override this by specifying a "description" in their interrupt config
            descriptionPrefix: "Tool execution pending approval",
        }),
    ],
    // Human-in-the-loop requires checkpointing to handle interrupts.
    // In production, use a persistent checkpointer like AsyncPostgresSaver.
    checkpointer: new MemorySaver(), // [!code highlight]
});

```

--------------------------------

### Binding User Tools with `bindTools()`

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet demonstrates how to define a custom tool using `tool()` and bind it to a ChatOpenAI model using `bindTools()`. The model can then be invoked to potentially call the bound tool, and the response is processed to log any tool calls made.

```typescript
import { tool } from "langchain";
import * as z from "zod";
import { ChatOpenAI } from "@langchain/openai";

const getWeather = tool(
  (input) => `It's sunny in ${input.location}.`,
  {
    name: "get_weather",
    description: "Get the weather at a location.",
    schema: z.object({
      location: z.string().describe("The location to get the weather for"),
    }),
  },
);

const model = new ChatOpenAI({ model: "gpt-4o" });
const modelWithTools = model.bindTools([getWeather]);

const response = await modelWithTools.invoke("What's the weather like in Boston?");
const toolCalls = response.tool_calls || [];
for (const tool_call of toolCalls) {
  console.log(`Tool: ${tool_call.name}`);
  console.log(`Args: ${tool_call.args}`);
}
```

--------------------------------

### Display First 500 Characters of Loaded Document Content

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Logs the first 500 characters of the page content from the first Document object loaded by the CheerioWebBaseLoader. This is useful for quickly inspecting the beginning of the loaded content.

```typescript
console.log(docs[0].pageContent.slice(0, 500));
```

--------------------------------

### Configure Agent for Human Review of SQL Queries (Python)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

This code configures a LangChain agent to pause before executing the `sql_db_query` tool using the `HumanInTheLoopMiddleware`. It also sets up an `InMemorySaver` for checkpoiniting. The middleware intercepts tool calls, pausing execution for human approval.

```python
from langchain.agents import create_agent
from langchain.agents.middleware import HumanInTheLoopMiddleware
from langgraph.checkpoint.memory import InMemorySaver


agent = create_agent(
    model,
    tools,
    system_prompt=system_prompt,
    middleware=[
        HumanInTheLoopMiddleware(
            interrupt_on={"sql_db_query": True},
            description_prefix="Tool execution pending approval",
        ),
    ],
    checkpointer=InMemorySaver(),
)
```

--------------------------------

### Basic Text Streaming with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to stream basic text output from a Langchain model. It uses a for-await-of loop to iterate over the stream and log each text chunk to the console. This is useful for displaying responses as they are generated, enhancing user experience for longer outputs.

```typescript
const stream = await model.stream("Why do parrots have colorful feathers?");
for await (const chunk of stream) {
  console.log(chunk.text)
}
```

--------------------------------

### Write to State using Command in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This snippet demonstrates how to write to an agent's State to track session-specific information using the `Command` class. It defines an `authenticateUser` tool that updates the 'authenticated' status in the State based on a provided password. Dependencies include `@langchain/core/tools`, `langchain`, and `@langchain/langgraph`.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";
import { Command } from "@langchain/langgraph";

const authenticateUser = tool(
  async ({ password }, { runtime }) => {
    // Perform authentication
    if (password === "correct") {
      // Write to State: mark as authenticated using Command
      return new Command({
        update: { authenticated: true },
      });
    } else {
      return new Command({ update: { authenticated: false } });
    }
  },
  {
    name: "authenticate_user",
    description: "Authenticate user and update State",
    schema: z.object({
      password: z.string(),
    }),
  }
);

```

--------------------------------

### TypeScript SQL Agent Implementation

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

A detailed TypeScript implementation for a SQL agent using Langchain. It includes functions for downloading and accessing a SQLite database, sanitizing SQL queries for safety, and executing read-only SELECT statements. The agent is configured with a model, tools, and a system prompt that enforces specific rules for interaction with the database.

```typescript
import fs from "node:fs/promises";
import path from "node:path";
import { SqlDatabase } from "@langchain/classic/sql_db";
import { DataSource } from "typeorm";
import { SystemMessage, createAgent, tool } from "langchain"
import * as z from "zod";

const url = "https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db";
const localPath = path.resolve("Chinook.db");

async function resolveDbPath() {
  if (await fs.exists(localPath)) {
    return localPath;
  }
  const resp = await fetch(url);
  if (!resp.ok) throw new Error(`Failed to download DB. Status code: ${resp.status}`);
  const buf = Buffer.from(await resp.arrayBuffer());
  await fs.writeFile(localPath, buf);
  return localPath;
}

let db: SqlDatabase | undefined;
async function getDb() {
  if (!db) {
    const dbPath = await resolveDbPath();
    const datasource = new DataSource({ type: "sqlite", database: dbPath });
    db = await SqlDatabase.fromDataSourceParams({ appDataSource: datasource });
  }
  return db;
}

async function getSchema() {
  const db = await getDb();
  return await db.getTableInfo();
}

const DENY_RE = /\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\b/i;
const HAS_LIMIT_TAIL_RE = /\blimit\b\s+\d+(\s*,\s*\d+)?\s*;?\s*$/i;

function sanitizeSqlQuery(q) {
  let query = String(q ?? "").trim();

  // block multiple statements (allow one optional trailing ;)
  const semis = [...query].filter((c) => c === ";").length;
  if (semis > 1 || (query.endsWith(";") && query.slice(0, -1).includes(";"))) {
    throw new Error("multiple statements are not allowed.")
  }
  query = query.replace(/;+\s*$/g, "").trim();

  // read-only gate
  if (!query.toLowerCase().startsWith("select")) {
    throw new Error("Only SELECT statements are allowed")
  }
  if (DENY_RE.test(query)) {
    throw new Error("DML/DDL detected. Only read-only queries are permitted.")
  }

  // append LIMIT only if not already present
  if (!HAS_LIMIT_TAIL_RE.test(query)) {
    query += " LIMIT 5";
  }
  return query;
}

const executeSql = tool(
  async ({ query }) => {
    const q = sanitizeSqlQuery(query);
    try {
      const result = await db.run(q);
      return typeof result === "string" ? result : JSON.stringify(result, null, 2);
    } catch (e) {
      throw new Error(e?.message ?? String(e))
    }
  },
  {
    name: "execute_sql",
    description: "Execute a READ-ONLY SQLite SELECT query and return results.",
    schema: z.object({
      query: z.string().describe("SQLite SELECT query to execute (read-only)."),
    }),
  }
);

const getSystemPrompt = async () => new SystemMessage(`You are a careful SQLite analyst.

Authoritative schema (do not invent columns/tables):
${await getSchema()}

Rules:
- Think step-by-step.
- When you need data, call the tool `execute_sql` with ONE SELECT query.
- Read-only only; no INSERT/UPDATE/DELETE/ALTER/DROP/CREATE/REPLACE/TRUNCATE.
- Limit to 5 rows unless user explicitly asks otherwise.
- If the tool returns 'Error:', revise the SQL and try again.
- Limit the number of attempts to 5.
- If you are not successful after 5 attempts, return a note to the user.
- Prefer explicit column lists; avoid SELECT *.
`);

export const agent = createAgent({
  model: "gpt-5",
  tools: [executeSql],
  systemPrompt: getSystemPrompt,
});

```

--------------------------------

### Dynamic System Prompt based on Conversation State (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Generates a system prompt that adapts based on the current conversation length. It reads the message count from the state and appends instructions to be concise if the conversation exceeds 10 messages. Requires the 'langchain' package.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    dynamicSystemPromptMiddleware((state) => {
      // Read from State: check conversation length
      const messageCount = state.messages.length;

      let base = "You are a helpful assistant.";

      if (messageCount > 10) {
        base += "\nThis is a long conversation - be extra concise.";
      }

      return base;
    }),
  ],
});
```

--------------------------------

### Initialize Bedrock Chat Model using Class

Source: https://docs.langchain.com/oss/javascript/langchain/models

Instantiates a chat model from AWS Bedrock directly using the `ChatBedrockConverse` class. This provides more explicit control over model configuration. Follow the provided AWS documentation link to configure necessary credentials before use. The `model` and `region` are required parameters.

```typescript
import { ChatBedrockConverse } from "@langchain/aws";

const model = new ChatBedrockConverse({
  model: "gpt-4.1",
  region: "us-east-2"
});
```

--------------------------------

### Streaming Tool Calls and Reasoning with Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/models

Illustrates how to stream advanced content types like tool calls and reasoning from a Langchain model. The code iterates through content blocks within each chunk, checking their type to handle reasoning, tool call chunks, and text separately. This allows for more complex interactions where the model might use tools or provide intermediate reasoning steps.

```typescript
const stream = await model.stream("What color is the sky?");
for await (const chunk of stream) {
  for (const block of chunk.contentBlocks) {
    if (block.type === "reasoning") {
      console.log(`Reasoning: ${block.reasoning}`);
    } else if (block.type === "tool_call_chunk") {
      console.log(`Tool call chunk: ${block}`);
    } else if (block.type === "text") {
      console.log(block.text);
    } else {
      ...
    }
  }
}
```

--------------------------------

### Execute and Stream Agentic RAG Chain - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Demonstrates how to execute the configured agentic RAG chain with a user input message and stream the response. It processes the streamed messages, printing each step and the final output, showing how retrieved context is incorporated into the model's prompt.

```typescript
let inputMessage = `What is Task Decomposition?`;

let chainInputs = { messages: [{ role: "user", content: inputMessage }] };

const stream = await agent.stream(chainInputs, {
  streamMode: "values",
})
for await (const step of stream) {
  const lastMessage = step.messages[step.messages.length - 1];
  prettyPrint(lastMessage);
  console.log("-----\n");
}

```

--------------------------------

### Create SQL Execution Tool (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Defines an `executeSql` tool using `langchain`'s `tool` function. This tool takes a SQL query, sanitizes it using `sanitizeSqlQuery`, executes it against a database (`db.run`), and returns the result as a JSON string. It includes error handling for execution failures.

```typescript
import { tool } from "langchain"
import * as z from "zod";

const executeSql = tool(
  async ({ query }) => {
    const q = sanitizeSqlQuery(query);
    try {
      const result = await db.run(q);
      return typeof result === "string" ? result : JSON.stringify(result, null, 2);
    } catch (e) {
      throw new Error(e?.message ?? String(e))
    }
  },
  {
    name: "execute_sql",
    description: "Execute a READ-ONLY SQLite SELECT query and return results.",
    schema: z.object({
      query: z.string().describe("SQLite SELECT query to execute (read-only)."),
    }),
  }
);

```

--------------------------------

### Initialize MongoDB Atlas Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes MongoDB Atlas Vector Search. This requires connection details to your MongoDB Atlas cluster, including database name, collection name, and index details.

```typescript
import { MongoDBAtlasVectorSearch } from "@langchain/mongodb"
import { MongoClient } from "mongodb";

const client = new MongoClient(process.env.MONGODB_ATLAS_URI || "");
const collection = client
  .db(process.env.MONGODB_ATLAS_DB_NAME)
  .collection(process.env.MONGODB_ATLAS_COLLECTION_NAME);

const vectorStore = new MongoDBAtlasVectorSearch(embeddings, {
  collection: collection,
  indexName: "vector_index",
  textKey: "text",
  embeddingKey: "embedding",
});
```

--------------------------------

### Fetch User Data using Runtime Context in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This tool fetches user data by utilizing configuration from the runtime context, such as API keys and database connection strings. It uses these details to perform a database query. The function returns a summary of the results found for the specified user.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
  apiKey: z.string(),
  dbConnection: z.string(),
});

const fetchUserData = tool(
  async ({ query }, { runtime }) => {
    // Read from Runtime Context: get API key and DB connection
    const { userId, apiKey, dbConnection } = runtime.context;

    // Use configuration to fetch data
    const results = await performDatabaseQuery(dbConnection, query, apiKey);

    return `Found ${results.length} results for user ${userId}`;
  },
  {
    name: "fetch_user_data",
    description: "Fetch data using Runtime Context configuration",
    schema: z.object({
      query: z.string(),
    }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [fetchUserData],
  contextSchema,
});

```

--------------------------------

### Initialize Memory Vector Store (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes a MemoryVectorStore with an embeddings object. This vector store keeps data in memory, making it fast but not persistent.

```typescript
import { MemoryVectorStore } from "@langchain/classic/vectorstores/memory";

const vectorStore = new MemoryVectorStore(embeddings);
```

--------------------------------

### Structured Output Generation with Zod in LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/agents

This snippet demonstrates how to configure a LangChain agent to return output in a specific format using the `responseFormat` parameter and Zod schemas. It takes user input, extracts contact information, and logs the structured response.

```typescript
import * as z from "zod";
import { createAgent } from "langchain";

const ContactInfo = z.object({
  name: z.string(),
  email: z.string(),
  phone: z.string(),
});

const agent = createAgent({
  model: "gpt-4o",
  responseFormat: ContactInfo,
});

const result = await agent.invoke({
  messages: [
    {
      role: "user",
      content: "Extract contact info from: John Doe, john@example.com, (555) 123-4567",
    },
  ],
});

console.log(result.structuredResponse);
// {
//   name: 'John Doe',
//   email: 'john@example.com',
//   phone: '(555) 123-4567'
// }
```

--------------------------------

### Stream Agent Execution and Collect Interrupts (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Executes an agent with a given query and configuration, streaming the execution steps. It captures and logs any interrupt events encountered during the stream, storing them in an `interrupts` array for later inspection. This allows for programmatic handling of user review steps.

```typescript
const query =
  "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
  "and send them an email reminder about reviewing the new mockups.";

const config = { configurable: { thread_id: "6" } };

const interrupts: any[] = [];
const stream = await supervisorAgent.stream(
  { messages: [{ role: "user", content: query }] },
  config
);

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    } else if (Array.isArray(update)) {
      const interrupt = update[0];
      interrupts.push(interrupt);
      console.log(`\nINTERRUPTED: ${interrupt.id}`);
    }
  }
}
```

--------------------------------

### Resume Agent Execution After Human Review (Python)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

This code shows how to resume the agent's execution after a human review interruption, specifically by sending an 'approve' command. It continues streaming the agent's output, processing messages and potential further interruptions.

```python
from langgraph.types import Command

for step in agent.stream(
    Command(resume={"decisions": [{"type": "approve"}]}),
    config,
    stream_mode="values",
):
    if "messages" in step:
        step["messages"][-1].pretty_print()
    elif "__interrupt__" in step:
        print("INTERRUPTED:")
        interrupt = step["__interrupt__"][0]
        for request in interrupt.value["action_requests"]:
            print(request["description"])
    else:
        pass
```

--------------------------------

### Integrate InMemoryStore into LangChain Agent Tools

Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory

Shows how to create a LangChain agent tool that reads long-term memory from an InMemoryStore. It defines a Zod schema for context, initializes the store, writes sample data, defines a `getUserInfo` tool that accesses the store via runtime context, and creates an agent that uses this tool and store, invoking it with specific context.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";
import { InMemoryStore, type Runtime } from "@langchain/langgraph";

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.
const store = new InMemoryStore(); // [!code highlight]
const contextSchema = z.object({
    userId: z.string(),
});

// Write sample data to the store using the put method
await store.put( // [!code highlight]
    ["users"], // Namespace to group related data together (users namespace for user data)
    "user_123", // Key within the namespace (user ID as key)
    {
        name: "John Smith",
        language: "English",
    } // Data to store for the given user
);

const getUserInfo = tool(
  // Look up user info.
  async (_, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    // Access the store - same as that provided to `createAgent`
    const userId = runtime.context?.userId;
    if (!userId) {
      throw new Error("userId is required");
    }
    // Retrieve data from store - returns StoreValue object with value and metadata
    const userInfo = await runtime.store.get(["users"], userId);
    return userInfo?.value ? JSON.stringify(userInfo.value) : "Unknown user";
  },
  {
    name: "getUserInfo",
    description: "Look up user info by userId from the store.",
    schema: z.object({}),
  }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getUserInfo],
    contextSchema,
    // Pass store to agent - enables agent to access store when running tools
    store, // [!code highlight]
});

// Run the agent
const result = await agent.invoke(
    { messages: [{ role: "user", content: "look up user information" }] },
    { context: { userId: "user_123" } } // [!code highlight]
);

console.log(result.messages.at(-1)?.content);
/**
 * Outputs:
 * User Information:
 * - Name: John Smith
 * - Language: English
 */

```

--------------------------------

### Configure Sub-Agents with Human-in-the-Loop Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Configures two sub-agents (`calendarAgent`, `emailAgent`) to use `humanInTheLoopMiddleware`. This middleware allows for manual review of specific tool calls (`create_calendar_event`, `send_email`) by permitting various response types. A `supervisorAgent` is also set up with a `MemorySaver` checkpointer, which is essential for pausing and resuming the agent's execution flow.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: CALENDAR_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { create_calendar_event: true },
      descriptionPrefix: "Calendar event pending approval",
    }),
  ],
});

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: EMAIL_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { send_email: true },
      descriptionPrefix: "Outbound email pending approval",
    }),
  ],
});

const supervisorAgent = createAgent({
  model: llm,
  tools: [scheduleEvent, manageEmail],
  systemPrompt: SUPERVISOR_PROMPT,
  checkpointer: new MemorySaver(),
});
```

--------------------------------

### Define a Tool to Create Calendar Events (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines a Langchain tool named 'create_calendar_event' using Zod for schema validation. This tool is a stub that simulates creating a calendar event and requires precise ISO datetime formatting for its input.

```typescript
import { tool } from "langchain";
import { z } from "zod";

const createCalendarEvent = tool(
  async ({ title, startTime, endTime, attendees, location }) => {
    // Stub: In practice, this would call Google Calendar API, Outlook API, etc.
    return `Event created: ${title} from ${startTime} to ${endTime} with ${attendees.length} attendees`;
  },
  {
    name: "create_calendar_event",
    description: "Create a calendar event. Requires exact ISO datetime format.",
    schema: z.object({
      title: z.string(),
      startTime: z.string().describe("ISO format: '2024-01-15T14:00:00'"),
      endTime: z.string().describe("ISO format: '2024-01-15T15:00:00'"),
      attendees: z.array(z.string()).describe("email addresses"),
      location: z.string().optional(),
    }),
  }
);

```

--------------------------------

### Initialize Azure OpenAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the AzureOpenAIEmbeddings class from the @langchain/openai package. Requires specifying the Azure OpenAI deployment name for embeddings.

```typescript
import { AzureOpenAIEmbeddings } from "@langchain/openai";

const embeddings = new AzureOpenAIEmbeddings({
  azureOpenAIApiEmbeddingsDeploymentName: "text-embedding-ada-002"
});
```

--------------------------------

### Defining Tools for Langchain Agents in Javascript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Shows how to define custom tools (e.g., 'search', 'get_weather') with Zod schemas for input validation and descriptions. These tools enable agents to perform actions. Requires 'langchain' and 'zod' packages.

```typescript
import * as z from "zod";
import { createAgent, tool } from "langchain";

const search = tool(
  ({ query }) => `Results for: ${query}`,
  {
    name: "search",
    description: "Search for information",
    schema: z.object({
      query: z.string().describe("The query to search for"),
    }),
  }
);

const getWeather = tool(
  ({ location }) => `Weather in ${location}: Sunny, 72°F`,
  {
    name: "get_weather",
    description: "Get weather information for a location",
    schema: z.object({
      location: z.string().describe("The location to get weather for"),
    }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [search, getWeather],
});
```

--------------------------------

### Initialize MistralAI Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the MistralAIEmbeddings class from the @langchain/mistralai package. Requires specifying the MistralAI model, such as 'mistral-embed'.

```typescript
import { MistralAIEmbeddings } from "@langchain/mistralai";

const embeddings = new MistralAIEmbeddings({
  model: "mistral-embed"
});
```

--------------------------------

### Using Tool or Provider Strategies for Response Format

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Demonstrates how to explicitly control the structured output strategy using `toolStrategy` or `providerStrategy` when creating an agent. This allows for specific enforcement of how the agent should handle structured data.

```typescript
import { toolStrategy, providerStrategy } from "langchain";

const agent = createAgent({
    // use a provider strategy if supported by the model
    responseFormat: providerStrategy(z.object({ ... }))
    // or enforce a tool strategy
    responseFormat: toolStrategy(z.object({ ... }))
})
```

--------------------------------

### Wrap Email Agent as a Tool in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Wraps the email agent as a reusable tool named `manage_email` for a supervisor agent. This tool takes a natural language email request, invokes the email agent, and returns the agent's final response. The description clearly outlines its purpose, usage, and input schema, specifying a natural language string for email requests.

```typescript
const manageEmail = tool(
  async ({ request }) => {
    const result = await emailAgent.invoke({
      messages: [{ role: "user", content: request }]
    });
    const lastMessage = result.messages[result.messages.length - 1];
    return lastMessage.text;
  },
  {
    name: "manage_email",
    description: `
Send emails using natural language.

Use this when the user wants to send notifications, reminders, or any email communication.
Handles recipient extraction, subject generation, and email composition.

Input: Natural language email request (e.g., 'send them a reminder about the meeting')
    `,
    schema: z.object({
      request: z.string().describe("Natural language email request"),
    }),
  }
);
```

--------------------------------

### Define Agent Context Schema and Invoke Agent (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/runtime

Demonstrates how to define a context schema using Zod for an agent created with `createAgent`. It shows how to pass contextual data, such as a user name, during agent invocation.

```typescript
import * as z from "zod";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userName: z.string(),
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [
    /* ... */
  ],
  contextSchema,
});

const result = await agent.invoke(
  { messages: [{ role: "user", content: "What's my name?" }] },
  { context: { userName: "John Smith" } }
);
```

--------------------------------

### Initialize AWS Bedrock Converse Chat Model in JavaScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Shows how to initialize an AWS Bedrock Converse chat model using LangChain.js. The 'initChatModel' function is used, and it requires AWS credentials to be configured following the official AWS documentation.

```typescript
import { initChatModel } from "langchain";

// Follow the steps here to configure your credentials:
// https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html

// Example initialization (model name might vary based on Bedrock support):
// const model = await initChatModel("bedrock-converse:anthropic.claude-3-sonnet-20240229-v1:0");
```

--------------------------------

### Initialize Bedrock Converse Chat Model in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates initializing an AWS Bedrock Converse chat model using `initChatModel`. This method is suitable for integrating with AWS Bedrock services. Further configuration might be required based on specific Bedrock model and region.

```typescript
import { initChatModel } from "langchain";

// AWS credentials and region should be configured in the environment or via AWS SDK configuration.

const model = await initChatModel("bedrock_converse");

```

--------------------------------

### Dynamic Response Format Selection based on Runtime Context

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Implements middleware to dynamically select response formats based on runtime context, such as user role and environment. It applies an 'adminResponse' schema for administrators in a 'production' environment and a 'userResponse' schema otherwise.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.string(),
  environment: z.string(),
});

const adminResponse = z.object({
  answer: z.string().describe("Answer"),
  debugInfo: z.record(z.any()).describe("Debug information"),
  systemStatus: z.string().describe("System status"),
});

const userResponse = z.object({
  answer: z.string().describe("Answer"),
});

const contextBasedOutput = createMiddleware({
  name: "ContextBasedOutput",
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: user role and environment
    const userRole = request.runtime.context.userRole;  // [!code highlight]
    const environment = request.runtime.context.environment;  // [!code highlight]

    if (userRole === "admin" && environment === "production") {
      responseFormat = adminResponse;  // [!code highlight]
    } else {
      responseFormat = userResponse;  // [!code highlight]
    }

    return handler({ ...request, responseFormat });
  },
});
```

--------------------------------

### Implement Human-in-the-Loop Approval Workflow with LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

Demonstrates using LangChain's humanInTheLoopMiddleware to pause agent execution for sensitive operations, requiring explicit human approval before proceeding. It covers configuring which actions require approval and how to resume the agent's execution with the approval decision.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver, Command } from "@langchain/langgraph";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, sendEmailTool, deleteDatabaseTool],
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: {
        // Require approval for sensitive operations
        send_email: { allowAccept: true, allowEdit: true, allowRespond: true },
        delete_database: { allowAccept: true, allowEdit: true, allowRespond: true },
        // Auto-approve safe operations
        search: false,
      }
    }),
  ],
  checkpointer: new MemorySaver(),
});

// Human-in-the-loop requires a thread ID for persistence
const config = { configurable: { thread_id: "some_id" } };

// Agent will pause and wait for approval before executing sensitive tools
let result = await agent.invoke(
  { messages: [{ role: "user", content: "Send an email to the team" }] },
  config
);

result = await agent.invoke(
  new Command({ resume: { decisions: [{ type: "approve" }] } }),
  config  // Same thread ID to resume the paused conversation
);

```

--------------------------------

### Create Agent with Static Model Identifier - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Initializes a Langchain.js agent using a static model identifier string. This approach is straightforward for common model configurations. It requires the `langchain` package and accepts a model identifier and a list of tools.

```typescript
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-5",
  tools: []
});
```

--------------------------------

### Define a Tool to Send Email (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Defines a Langchain tool named 'send_email' using Zod for schema validation. This tool is a stub that simulates sending an email and requires properly formatted email addresses for the 'to' and optional 'cc' fields.

```typescript
import { tool } from "langchain";
import { z } from "zod";

const sendEmail = tool(
  async ({ to, subject, body, cc }) => {
    // Stub: In practice, this would call SendGrid, Gmail API, etc.
    return `Email sent to ${to.join(', ')} - Subject: ${subject}`;
  },
  {
    name: "send_email",
    description: "Send an email via email API. Requires properly formatted addresses.",
    schema: z.object({
      to: z.array(z.string()).describe("email addresses"),
      subject: z.string(),
      body: z.string(),
      cc: z.array(z.string()).optional(),
    }),
  }
);

```

--------------------------------

### Server-Side Tool Use in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to use server-side tool invocation with Langchain JS. The model can bind to tools and interact with them within a single turn. The response's contentBlocks will contain a provider-agnostic format of tool calls and results, eliminating the need for separate ToolMessage objects.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel("gpt-4.1-mini");
const modelWithTools = model.bindTools([{ type: "web_search" }]);

const message = await modelWithTools.invoke("What was a positive news story from today?");
console.log(message.contentBlocks);
```

--------------------------------

### Handle Multimodal Output with LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to invoke a model that can generate multimodal output, such as images, and access these content blocks from the response. This requires a model with underlying multimodal capabilities and assumes the response is an AIMessage containing content blocks.

```typescript
const response = await model.invoke("Create a picture of a cat");
console.log(response.contentBlocks);
// [
//   { type: "text", text: "Here's a picture of a cat" },
//   { type: "image", data: "...", mimeType: "image/jpeg" },
// ]
```

--------------------------------

### Custom Output Formatting with Command in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent

This snippet demonstrates how to use the Command class in Langchain.js to format a subagent's output. It defines a tool that invokes a subagent, then wraps the subagent's response, including additional state keys, within a Command object. This allows for merging custom state with the subagent's response before returning it.

```typescript
import { tool, ToolMessage } from "langchain";
import { Command } from "@langchain/langgraph";
import * as z from "zod";

const callSubagent1 = tool(
  async ({ query }, config) => {
    const result = await subagent1.invoke({
      messages: [{ role: "user", content: query }]
    });

    // Return a Command to update multiple state keys
    return new Command({
      update: {
        // Pass back additional state from the subagent
        exampleStateKey: result.exampleStateKey,
        messages: [
          new ToolMessage({
            content: result.messages.at(-1)?.text,
            tool_call_id: config.toolCall?.id!
          })
        ]
      }
    });
  },
  {
    name: "subagent1_name",
    description: "subagent1_description",
    schema: z.object({
      query: z.string().describe("The query to send to subagent1")
    })
  }
);

```

--------------------------------

### Configure LangSmith Environment Variables

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Sets environment variables required for LangSmith tracing, enabling inspection of agent activity. This configuration can be done in bash or directly within a TypeScript Node.js environment.

```bash
export LANGSMITH_TRACING="true"
export LANGSMITH_API_KEY="..."
```

```typescript
process.env.LANGSMITH_TRACING = "true";
process.env.LANGSMITH_API_KEY = "...";
```

--------------------------------

### Select Model Based on User Preferences (Store Middleware)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware selects a language model based on user preferences stored in a database. It defines a Zod schema for context (userId) and uses the store to retrieve user preferences, specifically a 'preferredModel'. If a preference is found and the model exists in the MODEL_MAP, that model is used; otherwise, a default model is applied. This allows for personalized model selection.

```typescript
import * as z from "zod";
import { createMiddleware, initChatModel } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

// Initialize available models once
const MODEL_MAP = {
  "gpt-4o": initChatModel("gpt-4o"),
  "gpt-4o-mini": initChatModel("gpt-4o-mini"),
  "claude-sonnet": initChatModel("claude-sonnet-4-5-20250929"),
};

const storeBasedModel = createMiddleware({
  name: "StoreBasedModel",
  contextSchema,
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's preferred model
    const store = request.runtime.store;  // [!code highlight]
    const userPrefs = await store.get(["preferences"], userId);  // [!code highlight]

    let model = request.model;

    if (userPrefs) {
      const preferredModel = userPrefs.value?.preferredModel;
      if (preferredModel && MODEL_MAP[preferredModel]) {
        model = MODEL_MAP[preferredModel];  // [!code highlight]
      }
    }

    return handler({ ...request, model });  // [!code highlight]
  },
});
```

--------------------------------

### Store-Based Tool Selection Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware filters tools based on user preferences or feature flags stored in a 'Store'. It retrieves enabled features for a given user ID and filters the available tools accordingly, allowing for personalized toolsets.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const storeBasedTools = createMiddleware({
  name: "StoreBasedTools",
  contextSchema,
  wrapModelCall: async (request, handler) => {
    const userId = request.runtime.context.userId;  // [!code highlight]

    // Read from Store: get user's enabled features
    const store = request.runtime.store;  // [!code highlight]
    const featureFlags = await store.get(["features"], userId);  // [!code highlight]

    let filteredTools = request.tools;

    if (featureFlags) {
      const enabledFeatures = featureFlags.value?.enabledTools || [];
      filteredTools = request.tools.filter(t => enabledFeatures.includes(t.name));  // [!code highlight]
    }

    return handler({ ...request, tools: filteredTools });  // [!code highlight]
  },
});
```

--------------------------------

### Write to Store for Persistent Data in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This code illustrates how to write to the Store to persist data across user sessions. The `savePreference` tool retrieves existing preferences, merges a new preference, and then saves the updated preferences back to the Store using `store.put`. It requires `zod`, `@langchain/core/tools`, and `langchain`.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const contextSchema = z.object({
  userId: z.string(),
});

const savePreference = tool(
  async ({ preferenceKey, preferenceValue }, { runtime }) => {
    const userId = runtime.context.userId;

    // Read existing preferences
    const store = runtime.store;
    const existingPrefs = await store.get(["preferences"], userId);

    // Merge with new preference
    const prefs = existingPrefs?.value || {};
    prefs[preferenceKey] = preferenceValue;

    // Write to Store: save updated preferences
    await store.put(["preferences"], userId, prefs);

    return `Saved preference: ${preferenceKey} = ${preferenceValue}`;
  },
  {
    name: "save_preference",
    description: "Save user preference to Store",
    schema: z.object({
      preferenceKey: z.string(),
      preferenceValue: z.string(),
    }),
  }
);

```

--------------------------------

### Customize Supervisor Information Flow in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript code snippet demonstrates how to customize the information returned to the supervisor. It shows two options: returning a simple confirmation string or a JSON string with structured event data. This is useful for controlling the granularity of information passed up the agent hierarchy.

```typescript
import { tool } from "langchain/tools";
import { z } from "zod";

// Assuming calendarAgent is initialized elsewhere
// const calendarAgent = ...;

const scheduleEvent = tool(
  async ({ request }) => {
    const result = await calendarAgent.invoke({ // Replace with actual calendar agent invocation
      messages: [{ role: "user", content: request }]
    });

    const lastMessage = result.messages[result.messages.length - 1];

    // Option 1: Return just the confirmation message
    return lastMessage.text;

    // Option 2: Return structured data
    // return JSON.stringify({
    //   status: "success",
    //   event_id: "evt_123",
    //   summary: lastMessage.text
    // });
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### Invoke Model with Single Message (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates the simplest way to call a chat model using the `invoke()` method with a single string message. This is useful for straightforward, single-turn queries. No external dependencies beyond the Langchain model instance are required.

```typescript
const response = await model.invoke("Why do parrots have colorful feathers?");
console.log(response);
```

--------------------------------

### Process Tool Calls from AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to bind tools to a model and invoke it to potentially receive tool calls within the AIMessage response. It then iterates through the `tool_calls` array in the response to log the name, arguments, and ID of each tool call.

```typescript
const modelWithTools = model.bindTools([getWeather]);
const response = await modelWithTools.invoke("What's the weather in Paris?");

for (const toolCall of response.tool_calls) {
  console.log(`Tool: ${toolCall.name}`);
  console.log(`Args: ${toolCall.args}`);
  console.log(`ID: ${toolCall.id}`);
}
```

--------------------------------

### Initialize AWS Bedrock Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the BedrockEmbeddings class from the @langchain/aws package. Requires specifying the Bedrock model to use, such as 'amazon.titan-embed-text-v1'.

```typescript
import { BedrockEmbeddings } from "@langchain/aws";

const embeddings = new BedrockEmbeddings({
  model: "amazon.titan-embed-text-v1"
});
```

--------------------------------

### Initialize and Use InMemoryStore for Long-Term Memory

Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory

Demonstrates initializing an InMemoryStore for LangChain agents, putting data into the store with namespaces and keys, retrieving specific memories, and searching for memories based on content filters and vector similarity. This requires an embedding function and assumes a production environment would use a DB-backed store.

```typescript
import { InMemoryStore } from "@langchain/langgraph";

const embed = (texts: string[]): number[][] => {
    // Replace with an actual embedding function or LangChain embeddings object
    return texts.map(() => [1.0, 2.0]);
};

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.
const store = new InMemoryStore({ index: { embed, dims: 2 } }); // [!code highlight]
const userId = "my-user";
const applicationContext = "chitchat";
const namespace = [userId, applicationContext]; // [!code highlight]

await store.put( // [!code highlight]
    namespace,
    "a-memory",
    {
        rules: [
            "User likes short, direct language",
            "User only speaks English & TypeScript",
        ],
        "my-key": "my-value",
    }
);

// get the "memory" by ID
const item = await store.get(namespace, "a-memory"); // [!code highlight]

// search for "memories" within this namespace, filtering on content equivalence, sorted by vector similarity
const items = await store.search( // [!code highlight]
    namespace,
    {
        filter: { "my-key": "my-value" },
        query: "language preferences"
    }
);

```

--------------------------------

### Define Agent with Multiple Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript code illustrates how to create an agent using `createAgent` and pass an array of middleware functions (`middleware1`, `middleware2`, `middleware3`) to control its behavior. The order of middleware in the array determines their execution sequence, with specific rules for `before_*`, `after_*`, and `wrap_*` hooks.

```typescript
const agent = createAgent({
  model: "gpt-4o",
  middleware: [middleware1, middleware2, middleware3],
  tools: [...],
});
```

--------------------------------

### Run Vitest/Jest Evaluations

Source: https://docs.langchain.com/oss/javascript/langchain/test

Execute your LangSmith-integrated tests using your preferred test runner. This command will run the evaluation tests and log the results to LangSmith if configured.

```bash
vitest run test_trajectory.eval.ts
# or
jest test_trajectory.eval.ts
```

--------------------------------

### Load PDF Document with PDFLoader in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Loads a PDF file into a sequence of Document objects. Each Document represents a page from the PDF and includes its content and metadata. Dependencies include the '@langchain/community/document_loaders/fs/pdf' package.

```typescript
import { PDFLoader } from "@langchain/community/document_loaders/fs/pdf";

const loader = new PDFLoader("../../data/nke-10k-2023.pdf");

const docs = await loader.load();
console.log(docs.length);
```

--------------------------------

### Add Documents to Vector Store

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This code snippet demonstrates how to add a list of document splits to an instantiated vector store. It assumes 'vectorStore' and 'allSplits' have been previously defined. The 'addDocuments' method is used for this operation.

```typescript
await vectorStore.addDocuments(allSplits);
```

--------------------------------

### Access Runtime Context in a Tool - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/tools

Shows how to create a tool that accesses agent runtime context. The `getUserName` tool retrieves the user's name from the `config.context`. It also includes setting up an agent with a context schema and invoking it with context.

```typescript
import * as z from "zod"
import { ChatOpenAI } from "@langchain/openai"
import { createAgent } from "langchain"

const getUserName = tool(
  (_, config) => {
    return config.context.user_name
  },
  {
    name: "get_user_name",
    description: "Get the user's name.",
    schema: z.object({}),
  }
);

const contextSchema = z.object({
  user_name: z.string(),
});

const agent = createAgent({
  model: new ChatOpenAI({ model: "gpt-4o" }),
  tools: [getUserName],
  contextSchema,
});

const result = await agent.invoke(
  {
    messages: [{ role: "user", content: "What is my name?" }]
  },
  {
    context: { user_name: "John Smith" }
  }
);
```

--------------------------------

### Initialize Cohere Embeddings in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Initializes the CohereEmbeddings class from the @langchain/cohere package. Requires specifying the Cohere model, such as 'embed-english-v3.0'.

```typescript
import { CohereEmbeddings } from "@langchain/cohere";

const embeddings = new CohereEmbeddings({
  model: "embed-english-v3.0"
});
```

--------------------------------

### Configure AWS Bedrock Environment Variable

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Sets the AWS region environment variable required for AWS Bedrock embeddings.

```bash
BEDROCK_AWS_REGION=your-region
```

--------------------------------

### Dynamic Response Format Selection based on Conversation State

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

Implements middleware to dynamically select response formats based on the number of messages in a conversation. A simple format is used for early conversation stages (less than 3 messages), and a detailed format is used for established conversations.

```typescript
import { createMiddleware } from "langchain";
import { z } from "zod";

const simpleResponse = z.object({
  answer: z.string().describe("A brief answer"),
});

const detailedResponse = z.object({
  answer: z.string().describe("A detailed answer"),
  reasoning: z.string().describe("Explanation of reasoning"),
  confidence: z.number().describe("Confidence score 0-1"),
});

const stateBasedOutput = createMiddleware({
  name: "StateBasedOutput",
  wrapModelCall: (request, handler) => {
    // request.state is a shortcut for request.state.messages
    const messageCount = request.messages.length;  // [!code highlight]

    if (messageCount < 3) {
      // Early conversation - use simple format
      responseFormat = simpleResponse; // [!code highlight]
    } else {
      // Established conversation - use detailed format
      responseFormat = detailedResponse; // [!code highlight]
    }

    return handler({ ...request, responseFormat });
  },
});
```

--------------------------------

### Anthropic Prompt Caching Middleware for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Optimize Anthropic model usage by caching repetitive prompt prefixes to reduce costs and latency. This middleware is particularly beneficial for applications with consistent system prompts or agents that reuse context. It allows configuring the time-to-live (TTL) for cached content, with supported values like '5m' for 5 minutes or '1h' for 1 hour.

```typescript
import { createAgent, HumanMessage, anthropicPromptCachingMiddleware } from "langchain";

const LONG_PROMPT = `
Please be a helpful assistant.

<Lots more context ...>
`;

const agent = createAgent({
  model: "claude-sonnet-4-5-20250929",
  prompt: LONG_PROMPT,
  middleware: [anthropicPromptCachingMiddleware({ ttl: "5m" })],
});

// cache store
await agent.invoke({
  messages: [new HumanMessage("Hi, my name is Bob")]
});

// cache hit, system prompt is cached
const result = await agent.invoke({
  messages: [new HumanMessage("What's my name?")]
});
```

--------------------------------

### Manually Construct and Insert AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to manually create an AIMessage object and insert it into a list of messages. This is useful when you want to simulate a model response or control the message history explicitly. It also shows combining SystemMessage and HumanMessage with the manually created AIMessage for model invocation.

```typescript
import { AIMessage, SystemMessage, HumanMessage } from "langchain";

const aiMsg = new AIMessage("I'd be happy to help you with that question!");

const messages = [
  new SystemMessage("You are a helpful assistant"),
  new HumanMessage("Can you help me?"),
  aiMsg,  // Insert as if it came from the model
  new HumanMessage("Great! What's 2+2?")
]

const response = await model.invoke(messages);
```

--------------------------------

### Configure Tool Calling Strategy for Structured Response

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

The `toolStrategy` function configures LangChain to use tool calling for generating structured responses when native support is unavailable. It accepts a schema (Zod or JSON Schema) and optional configuration for tool messages and error handling. This function is crucial for enabling structured output with compatible language models.

```typescript
function toolStrategy<StructuredResponseT>(
    responseFormat:
        | JsonSchemaFormat
        | ZodSchema<StructuredResponseT>
        | (ZodSchema<StructuredResponseT> | JsonSchemaFormat)[]
    options?: ToolStrategyOptions
): ToolStrategy<StructuredResponseT>
```

--------------------------------

### Parsing AIMessage Content Blocks with Anthropic Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how an `AIMessage` generated with Anthropic's provider-native content (including 'thinking' and 'text' blocks) can be lazily parsed into LangChain's standard `contentBlocks` representation.

```typescript
import { AIMessage } from "@langchain/core/messages";

const message = new AIMessage({
  content: [
    {
      "type": "thinking",
      "thinking": "...",
      "signature": "WaUjzkyp...",
    },
    {
      "type":"text",
      "text": "...",
      "id": "msg_abc123",
    },
  ],
  response_metadata: { model_provider: "anthropic" },
});

console.log(message.contentBlocks);
```

--------------------------------

### ContentBlock.Reasoning

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents model reasoning steps.

```APIDOC
## ContentBlock.Reasoning

### Description
Model reasoning steps.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"reasoning"`
- **reasoning** (string) - Required - The reasoning content

### Request Example
```json
{
    "type": "reasoning",
    "reasoning": "The user is asking about..."
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"reasoning"`
- **reasoning** (string) - The reasoning content

#### Response Example
```json
{
    "type": "reasoning",
    "reasoning": "The user is asking about..."
}
```
```

--------------------------------

### Customize Tool Message Content in LangChain JavaScript

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Demonstrates how to use the 'toolMessageContent' parameter within the 'toolStrategy' function to customize the assistant's message when structured output is generated. This allows for more user-friendly feedback in the conversation history. It requires the 'langchain' library and 'zod' for schema definition.

```typescript
import * as z from "zod";
import { createAgent, toolStrategy } from "langchain";

const MeetingAction = z.object({
    task: z.string().describe("The specific task to be completed"),
    assignee: z.string().describe("Person responsible for the task"),
    priority: z.enum(["low", "medium", "high"]).describe("Priority level"),
});

const agent = createAgent({
    model: "gpt-5",
    tools: [],
    responseFormat: toolStrategy(MeetingAction, {
        toolMessageContent: "Action item captured and added to meeting notes!"
    })
});

const result = await agent.invoke({
    messages: [{"role": "user", "content": "From our meeting: Sarah needs to update the project timeline as soon as possible"}]
});

console.log(result);
/**
 * { 
 *   messages: [
 *     { role: "user", content: "From our meeting: Sarah needs to update the project timeline as soon as possible" },
 *     { role: "assistant", content: "Action item captured and added to meeting notes!", tool_calls: [ { name: "MeetingAction", args: { task: "update the project timeline", assignee: "Sarah", priority: "high" }, id: "call_456" } ] },
 *     { role: "tool", content: "Action item captured and added to meeting notes!", tool_call_id: "call_456", name: "MeetingAction" }
 *   ],
 *   structuredResponse: { task: "update the project timeline", assignee: "Sarah", priority: "high" }
 * }
 */
```

--------------------------------

### Inject Compliance Rules Middleware for Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware injects compliance rules into the LLM prompt based on user jurisdiction and industry. It reads `userJurisdiction`, `industry`, and `complianceFrameworks` from the runtime context. Specific rules for frameworks like GDPR and HIPAA, or for industries like finance, are generated and appended as a user message to the conversation. Dependencies include 'langchain' and 'zod'.

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userJurisdiction: z.string(),
  industry: z.string(),
  complianceFrameworks: z.array(z.string()),
});

type Context = z.infer<typeof contextSchema>;

const injectComplianceRules = createMiddleware<Context>({
  name: "InjectComplianceRules",
  contextSchema,
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: get compliance requirements
    const { userJurisdiction, industry, complianceFrameworks } = request.runtime.context;  // [!code highlight]

    // Build compliance constraints
    const rules = [];
    if (complianceFrameworks.includes("GDPR")) {
      rules.push("- Must obtain explicit consent before processing personal data");
      rules.push("- Users have right to data deletion");
    }
    if (complianceFrameworks.includes("HIPAA")) {
      rules.push("- Cannot share patient health information without authorization");
      rules.push("- Must use secure, encrypted communication");
    }
    if (industry === "finance") {
      rules.push("- Cannot provide financial advice without proper disclaimers");
    }

    if (rules.length > 0) {
      const complianceContext = `Compliance requirements for ${userJurisdiction}:
    ${rules.join("\n")}`;

      // Append at end - models pay more attention to final messages
      const messages = [
        ...request.messages,
        { role: "user", content: complianceContext }
      ];
      request = request.override({ messages });  // [!code highlight]
    }

    return handler(request);
  },
});
```

--------------------------------

### Create ToolMessage for Tool Calling in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates the process of creating an AIMessage with tool calls and a corresponding ToolMessage to return the result of a tool execution back to the model. This enables AI models to interact with external tools.

```typescript
import { AIMessage, ToolMessage, HumanMessage } from "langchain";

const aiMessage = new AIMessage({
  content: [],
  tool_calls: [{
    name: "get_weather",
    args: { location: "San Francisco" },
    id: "call_123"
  }]
});

const toolMessage = new ToolMessage({
  content: "Sunny, 72°F",
  tool_call_id: "call_123"
});

const messages = [
  new HumanMessage("What's the weather in San Francisco?"),
  aiMessage,  // Model's tool call
  toolMessage,  // Tool execution result
];

const response = await model.invoke(messages);  // Model processes the result
```

--------------------------------

### Create LLM-as-Judge Evaluator with Reference Trajectory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Configures the LLM-as-Judge evaluator to include a reference trajectory using `TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE`. This allows for comparison against a known correct path. Dependencies include `agentevals`.

```typescript
import { TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE } from "agentevals";

const evaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,
});

const evaluation = await evaluator({
  outputs: result.messages,
  referenceOutputs: referenceTrajectory,
});
```

--------------------------------

### LangGraph Middleware with Runtime Context - TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/runtime

This TypeScript code demonstrates how to use middleware in LangGraph to access runtime context. It includes dynamic prompt generation based on 'userName' and logging middleware to track request processing. The `runtime` parameter, typed with `z.infer<typeof contextSchema>`, allows access to context data like `userName`.

```typescript
import * as z from "zod";
import { createAgent, createMiddleware, type AgentState, SystemMessage } from "langchain";
import { type Runtime } from "@langchain/langgraph";

const contextSchema = z.object({
  userName: z.string(),
});

// Dynamic prompt middleware
const dynamicPromptMiddleware = createMiddleware({
  name: "DynamicPrompt",
  beforeModel: (state: AgentState, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    const userName = runtime.context?.userName;
    if (!userName) {
      throw new Error("userName is required");
    }

    const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`;
    return {
      messages: [new SystemMessage(systemMsg), ...state.messages]
    };
  }
});

// Logging middleware
const loggingMiddleware = createMiddleware({
  name: "Logging",
  beforeModel: (state: AgentState, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    console.log(`Processing request for user: ${runtime.context?.userName}`);
    return;
  },
  afterModel: (state: AgentState, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    console.log(`Completed request for user: ${runtime.context?.userName}`);
    return;
  }
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [
    /* ... */
  ],
  middleware: [dynamicPromptMiddleware, loggingMiddleware],
  contextSchema,
});

const result = await agent.invoke(
  { messages: [{ role: "user", content: "What's my name?" }] },
  { context: { userName: "John Smith" } }
);

```

--------------------------------

### Force Tool Usage in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Demonstrates how to bind tools to a model in Langchain.js, specifying whether to force the use of any available tool or a specific one. This is useful for controlling model behavior when multiple tools are present.

```typescript
const modelWithTools = model.bindTools([tool_1], { toolChoice: "any" })
```

```typescript
const modelWithTools = model.bindTools([tool_1], { toolChoice: "tool_1" })
```

--------------------------------

### Tool Message Usage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to create and use `ToolMessage` objects, which are essential for passing the results of tool executions back to an AI model that supports tool calling.

```APIDOC
## Tool Message Usage

### Description
This documentation covers the creation and utilization of `ToolMessage` objects within Langchain. `ToolMessage` is used to convey the output of a tool's execution back to the model, enabling conversational AI agents to interact with external tools and utilize their results.

### Method
N/A (Client-side object creation and model invocation)

### Endpoint
N/A

### Parameters
#### Request Body (for `ToolMessage` constructor)
- **content** (string) - Required - The stringified output of the tool call.
- **tool_call_id** (string) - Required - The ID of the tool call that this message is responding to. This must match the ID of the tool call in the `AIMessage`.
- **name** (string) - Required - The name of the tool that was called.
- **artifact** (dict) - Optional - Additional data not sent to the model but accessible programmatically. Useful for storing raw results or metadata.

### Request Example
```typescript
import { AIMessage, ToolMessage, HumanMessage } from "langchain";

const aiMessage = new AIMessage({
  content: [],
  tool_calls: [{
    name: "get_weather",
    args: { location: "San Francisco" },
    id: "call_123"
  }]
});

const toolMessage = new ToolMessage({
  content: "Sunny, 72°F",
  tool_call_id: "call_123",
  name: "get_weather",
  artifact: { "source": "weather_api" } // Example artifact
});

const messages = [
  new HumanMessage("What's the weather in San Francisco?"),
  aiMessage,  // Model's tool call
  toolMessage,  // Tool execution result
];

// Assuming 'model' is an initialized Langchain model instance
// const response = await model.invoke(messages);
```

### Response
#### Success Response (from `model.invoke`)
- The structure of the response depends on the model's capabilities and the context of the invocation. It typically includes the model's final answer or subsequent actions.

#### Response Example
(Example shows the `ToolMessage` creation, the subsequent `model.invoke` response would vary)
```json
{
  "tool_call_id": "call_123",
  "content": "Sunny, 72°F",
  "name": "get_weather",
  "artifact": {
    "source": "weather_api"
  }
}
```

### Attributes
- **content** (`string`) - Required - The stringified output of the tool call.
- **tool_call_id** (`string`) - Required - The ID of the tool call that this message is responding to. (this must match the ID of the tool call in the [`AIMessage`](https://v03.api.js.langchain.com/classes/_langchain_core.messages_ai_message.AIMessage.html))
- **name** (`string`) - Required - The name of the tool that was called.
- **artifact** (`dict`) - Optional - Additional data not sent to the model but can be accessed programmatically. Useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.

#### Example: Using artifact for retrieval metadata

```typescript
import { ToolMessage } from "langchain";

// Artifact available downstream
const artifact = { document_id: "doc_123", page: 0 };

const toolMessage = new ToolMessage({
  content: "It was the best of times, it was the worst of times.",
  tool_call_id: "call_123",
  name: "search_books",
  artifact
});
```
```

--------------------------------

### Streaming AIMessageChunk Handling

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to process and combine incoming AIMessageChunk objects during a streaming operation to form a complete message.

```APIDOC
## Streaming AIMessageChunk Handling

### Description
This section explains how to handle `AIMessageChunk` objects received during streaming. It shows a pattern for combining sequential chunks into a single, complete message.

### Method
N/A (Client-side processing of stream data)

### Endpoint
N/A

### Parameters
N/A

### Request Example
N/A

### Response
#### Success Response
- **finalChunk** (`AIMessageChunk` | `undefined`) - The final combined `AIMessageChunk` object after processing all incoming chunks.

#### Response Example
```typescript
import { AIMessageChunk } from "langchain";

let finalChunk: AIMessageChunk | undefined;
for (const chunk of chunks) {
  finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;
}
```
```

--------------------------------

### Set Custom LangSmith Project Name (Static and Dynamic)

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Configure a custom project name for your LangSmith traces. This can be done statically by setting the LANGSMITH_PROJECT environment variable or dynamically within your code using `tracing_context`.

```bash
export LANGSMITH_PROJECT=my-agent-project
```

```python
import langsmith as ls

with ls.tracing_context(project_name="email-agent-test", enabled=True):
    response = agent.invoke({
        "messages": [{"role": "user", "content": "Send a welcome email"}]
    })
```

--------------------------------

### Add Metadata and Tags to LangSmith Traces

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Annotate your LangSmith traces with custom metadata and tags for better organization and filtering. This can be done directly in the `invoke` call or within the `tracing_context` manager.

```python
response = agent.invoke(
    {"messages": [{"role": "user", "content": "Send a welcome email"}]},
    config={
        "tags": ["production", "email-assistant", "v1.0"],
        "metadata": {
            "user_id": "user_123",
            "session_id": "session_456",
            "environment": "production"
        }
    }
)
```

```python
with ls.tracing_context(
    project_name="email-agent-test",
    enabled=True,
    tags=["production", "email-assistant", "v1.0"],
    metadata={"user_id": "user_123", "session_id": "session_456", "environment": "production"}):
    response = agent.invoke(
        {"messages": [{"role": "user", "content": "Send a welcome email"}]}
    )
```

--------------------------------

### Invoke Model with Dictionary Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates invoking a chat model using a list of dictionaries, following the OpenAI chat completions format. This method is convenient for specifying messages directly in a structured way.

```typescript
const messages = [
  { role: "system", content: "You are a poetry expert" },
  { role: "user", content: "Write a haiku about spring" },
  { role: "assistant", content: "Cherry blossoms bloom..." },
];
const response = await model.invoke(messages);
```

--------------------------------

### Handling Agent Interrupts with Langchain (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Demonstrates how to invoke a Langchain agent that may be interrupted, inspect the interrupt details including action requests and review configurations, and then resume the execution with a decision. Requires `@langchain/core/messages` and `@langchain/langgraph`.

```typescript
import { HumanMessage } from "@langchain/core/messages";
import { Command } from "@langchain/langgraph";

// You must provide a thread ID to associate the execution with a conversation thread,
// so the conversation can be paused and resumed (as is needed for human review).
const config = { configurable: { thread_id: "some_id" } }; // [!code highlight]

// Run the graph until the interrupt is hit.
const result = await agent.invoke(
    {
        messages: [new HumanMessage("Delete old records from the database")],
    },
    config // [!code highlight]
);


// The interrupt contains the full HITL request with action_requests and review_configs
console.log(result.__interrupt__);
// > [
// >    Interrupt(
// >       value: {
// >          action_requests: [
// >             {
// >                name: 'execute_sql',
// >                arguments: { query: 'DELETE FROM records WHERE created_at < NOW() - INTERVAL \'30 days\';' },
// >                description: 'Tool execution pending approval\n\nTool: execute_sql\nArgs: {...}'
// >             }
// >          ],
// >          review_configs: [
// >             {
// >                action_name: 'execute_sql',
// >                allowed_decisions: ['approve', 'reject']
// >             }
// >          ]
// >       }
// >    )
// > ]

// Resume with approval decision
await agent.invoke(
    new Command({ // [!code highlight]
        resume: { decisions: [{ type: "approve" }] }, // or "edit", "reject" [!code highlight]
    }), // [!code highlight]
    config // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Augment Message with Retrieved Documents in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/rag

This JavaScript code snippet demonstrates a middleware function for a LangChain agent. It retrieves relevant documents using similarity search based on the last message's content and augments the message with the retrieved document content as context. This is a core part of implementing RAG.

```javascript
const lastMessage = state.messages[state.messages.length - 1].content;
      const retrievedDocs = await vectorStore.similaritySearch(lastMessage, 2);

      const docsContent = retrievedDocs
        .map((doc) => doc.pageContent)
        .join("\n\n");

      const augmentedMessageContent = [
          ...lastMessage.content,
          { type: "text", text: `Use the following context to answer the query:\n\n${docsContent}` }
      ]

      return {
        messages: [{
          ...lastMessage,
          content: augmentedMessageContent,
        }]
        context: retrievedDocs,
      }
```

--------------------------------

### Wrap-style Dynamic Model Selection Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Provides a Wrap-style middleware for dynamically selecting the LLM model based on conversation length. The `wrapModelCall` hook inspects the number of messages in the request and switches the model between 'gpt-4o' and 'gpt-4o-mini' accordingly. This optimizes cost and performance.

```typescript
import { createMiddleware, initChatModel } from "langchain";

const dynamicModelMiddleware = createMiddleware({
  name: "DynamicModelMiddleware",
  wrapModelCall: (request, handler) => {
    // Use different model based on conversation length
    const modifiedRequest = { ...request };
    if (request.messages.length > 10) {
      modifiedRequest.model = initChatModel("gpt-4o");
    } else {
      modifiedRequest.model = initChatModel("gpt-4o-mini");
    }
    return handler(modifiedRequest);
  },
});
```

--------------------------------

### Configure Human-in-the-Loop Middleware for Agents

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This code configures LangChain.js agents with human-in-the-loop middleware. It specifies which tool calls should trigger an interruption, allowing for manual approval, editing, or rejection. A checkpointer is added to the supervisor agent to enable pausing and resuming execution.

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";
import { MemorySaver } from "@langchain/langgraph";

const calendarAgent = createAgent({
  model: llm,
  tools: [createCalendarEvent, getAvailableTimeSlots],
  systemPrompt: CALENDAR_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { create_calendar_event: true },
      descriptionPrefix: "Calendar event pending approval",
    }),
  ],
});

const emailAgent = createAgent({
  model: llm,
  tools: [sendEmail],
  systemPrompt: EMAIL_AGENT_PROMPT,
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: { send_email: true },
      descriptionPrefix: "Outbound email pending approval",
    }),
  ],
});

const supervisorAgent = createAgent({
  model: llm,
  tools: [scheduleEvent, manageEmail],
  systemPrompt: SUPERVISOR_PROMPT,
  checkpointer: new MemorySaver(),
});
```

--------------------------------

### System Message: Detailed Persona (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates creating a SystemMessage to define a detailed persona for the model, including expertise and response guidelines. This allows for more specific and tailored interactions.

```typescript
import { SystemMessage, HumanMessage } from "langchain";

const systemMsg = new SystemMessage(`
You are a senior TypeScript developer with expertise in web frameworks.
Always provide code examples and explain your reasoning.
Be concise but thorough in your explanations.
`);

const messages = [
  systemMsg,
  new HumanMessage("How do I create a REST API?"),
];
const response = await model.invoke(messages);
```

--------------------------------

### Similarity Search with Scores

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This snippet illustrates how to search a vector store for documents similar to a string query and also return the similarity scores. The output contains a score and the corresponding Document object.

```typescript
const results2 = await vectorStore.similaritySearchWithScore(
  "What was Nike's revenue in 2023?"
);

console.log(results2[0]);
```

--------------------------------

### Multiple Streaming Modes in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

Shows how to enable multiple streaming modes simultaneously by passing an array to the 'streamMode' option. This allows for flexibility in receiving different types of streaming output, such as 'updates', 'messages', and 'custom' data.

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const [streamMode, chunk] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: ["updates", "messages", "custom"] }
)) {
    console.log(`${streamMode}: ${JSON.stringify(chunk, null, 2)}`);
}
```

--------------------------------

### Inject File Context Middleware for Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware injects context about uploaded files into the LLM's message history. It retrieves file details from the request state and formats them into a user message, prepended to the existing conversation. This is useful for agents that need to reference specific documents during a conversation. Dependencies include the 'langchain' library.

```typescript
import { createMiddleware } from "langchain";

const injectFileContext = createMiddleware({
  name: "InjectFileContext",
  wrapModelCall: (request, handler) => {
    // request.state is a shortcut for request.state.messages
    const uploadedFiles = request.state.uploadedFiles || [];  // [!code highlight]

    if (uploadedFiles.length > 0) {
      // Build context about available files
      const fileDescriptions = uploadedFiles.map(file =>
        `- ${file.name} (${file.type}): ${file.summary}`
      );

      const fileContext = `Files you have access to in this conversation:
    ${fileDescriptions.join("\n")} 

    Reference these files when answering questions.`;

      // Inject file context before recent messages
      const messages = [  // [!code highlight]
        ...request.messages  // Rest of conversation
        { role: "user", content: fileContext }
      ];
      request = request.override({ messages });  // [!code highlight]
    }

    return handler(request);
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [injectFileContext],
});
```

--------------------------------

### Parsing AIMessage Content Blocks with OpenAI Format (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how an `AIMessage` originating from the OpenAI provider, containing 'reasoning' and 'text' blocks, can be accessed via LangChain's standard `contentBlocks` property.

```typescript
import { AIMessage } from "@langchain/core/messages";

const message = new AIMessage({
  content: [
    {
      "type": "reasoning",
      "id": "rs_abc123",
      "summary": [
        {"type": "summary_text", "text": "summary 1"},
        {"type": "summary_text", "text": "summary 2"},
      ],
    },
    {"type": "text", "text": "..."},
  ],
  response_metadata: { model_provider: "openai" },
});

console.log(message.contentBlocks);
```

--------------------------------

### Inspect Captured Interrupt Events (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Iterates through a list of captured interrupt events and logs details about each action request. This includes the interrupt ID and a description of the request, along with the specific tool and arguments that triggered the interruption. This is useful for debugging and understanding why an agent was paused.

```typescript
for (const interrupt of interrupts) {
  for (const request of interrupt.value.actionRequests) {
    console.log(`INTERRUPTED: ${interrupt.id}`);
    console.log(`${request.description}\n`);
  }
}
```

--------------------------------

### Selective Tracing with Python's tracing_context

Source: https://docs.langchain.com/oss/javascript/langchain/observability

Use LangSmith's `tracing_context` manager in Python to selectively enable or disable tracing for specific parts of your application. Code within the `with ls.tracing_context(enabled=True):` block will be traced.

```python
import langsmith as ls

# This WILL be traced
with ls.tracing_context(enabled=True):
    agent.invoke({"messages": [{"role": "user", "content": "Send a test email to alice@example.com"}]})

# This will NOT be traced (if LANGSMITH_TRACING is not set)
agent.invoke({"messages": [{"role": "user", "content": "Send another email"}]})
```

--------------------------------

### Stream Reasoning Output from LangChain.js Models

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to stream reasoning steps from a LangChain.js model when processing a query. It iterates through the streamed chunks, filters for content blocks of type 'reasoning', and logs them or the chunk's text if no reasoning steps are present. This requires the underlying model to support and expose reasoning capabilities.

```typescript
const stream = model.stream("Why do parrots have colorful feathers?");
for await (const chunk of stream) {
    const reasoningSteps = chunk.contentBlocks.filter(b => b.type === "reasoning");
    console.log(reasoningSteps.length > 0 ? reasoningSteps : chunk.text);
}
```

--------------------------------

### Stream LLM Tokens with Messages Mode in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

This code snippet illustrates how to stream tokens as they are produced by the LLM using LangChain.js with `streamMode: 'messages'`. It captures the token content and associated metadata, such as the LangGraph node. Dependencies include `zod` and `langchain`.

```typescript
import z from "zod";
import { createAgent, tool } from "langchain";

const getWeather = tool(
    async ({ city }) => {
        return `The weather in ${city} is always sunny!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string(),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const [token, metadata] of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "messages" }
)) {
    console.log(`node: ${metadata.langgraph_node}`);
    console.log(`content: ${JSON.stringify(token.contentBlocks, null, 2)}`);
}
```

--------------------------------

### ContentBlock.Multimodal.File

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents generic file data (e.g., PDF), which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.File

### Description
Generic files (PDF, etc).

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"file"`
- **url** (string) - Optional - URL pointing to the file location.
- **data** (string) - Optional - Base64-encoded file data.
- **fileId** (string) - Optional - Reference ID to an externally stored file.
- **mimeType** (string) - Optional - File MIME type (e.g., `application/pdf`)

### Request Example
```json
{
    "type": "file",
    "url": "http://example.com/document.pdf",
    "mimeType": "application/pdf"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"file"`
- **url** (string) - URL pointing to the file location.
- **data** (string) - Base64-encoded file data.
- **fileId** (string) - Reference ID to an externally stored file.
- **mimeType** (string) - File MIME type

#### Response Example
```json
{
    "type": "file",
    "url": "http://example.com/document.pdf",
    "mimeType": "application/pdf"
}
```
```

--------------------------------

### Configuring Base URL for OpenAI-Compatible APIs in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to configure a custom base URL for chat model integrations in Langchain JS, enabling the use of OpenAI-compatible APIs or proxy servers. This is done by passing the `baseUrl` parameter to `initChatModel`.

```python
model = initChatModel(
    "MODEL_NAME",
    {
        modelProvider: "openai",
        baseUrl: "BASE_URL",
        apiKey: "YOUR_API_KEY",
    }
)
```

--------------------------------

### Define schedule_event tool with customizable return format in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

Creates a tool for scheduling calendar events using LangChain agents. The function invokes a calendar agent, extracts the last message, and can return either a plain text confirmation or a JSON payload. Dependencies include the LangChain tool utilities and Zod for schema validation; inputs are a natural‑language request string, and the output is either a string or JSON string.

```TypeScript
const scheduleEvent = tool(
  async ({ request }) => {
    const result = await calendarAgent.invoke({
      messages: [{ role: "user", content: request }]
    });

    const lastMessage = result.messages[result.messages.length - 1];

    // Option 1: Return just the confirmation message
    return lastMessage.text;

    // Option 2: Return structured data
    // return JSON.stringify({
    //   status: "success",
    //   event_id: "evt_123",
    //   summary: lastMessage.text
    // });
  },
  {
    name: "schedule_event",
    description: "Schedule calendar events using natural language.",
    schema: z.object({
      request: z.string().describe("Natural language scheduling request"),
    }),
  }
);

```

--------------------------------

### LangSmith Evaluate Function for Dataset-Based Evaluation

Source: https://docs.langchain.com/oss/javascript/langchain/test

Utilize the `evaluate` function from LangSmith for running evaluations on datasets stored in LangSmith. This approach simplifies the process of evaluating an agent against a predefined dataset and automatically logs results. It requires the `langsmith/evaluation` and `agentevals` packages.

```typescript
import { evaluate } from "langsmith/evaluation";
import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";

const trajectoryEvaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT,
});

async function runAgent(inputs: any) {
  const result = await agent.invoke(inputs);
  return result.messages;
}

await evaluate(
  runAgent,
  {
    data: "your_dataset_name",
    evaluators: [trajectoryEvaluator],
  }
);
```

--------------------------------

### Invoke Model with Conversation History (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/models

Shows how to invoke a chat model with a list of messages representing a conversation history. The model uses message roles (system, user, assistant) to understand context. This requires constructing a message array, either as objects or specific message classes.

```typescript
const conversation = [
  { role: "system", content: "You are a helpful assistant that translates English to French." },
  { role: "user", content: "Translate: I love programming." },
  { role: "assistant", content: "J'adore la programmation." },
  { role: "user", content: "Translate: I love building applications." },
];

const response = await model.invoke(conversation);
console.log(response);  // AIMessage("J'adore créer des applications.")
```

```typescript
import { HumanMessage, AIMessage, SystemMessage } from "langchain";

const conversation = [
  new SystemMessage("You are a helpful assistant that translates English to French."),
  new HumanMessage("Translate: I love programming."),
  new AIMessage("J'adore la programmation."),
  new HumanMessage("Translate: I love building applications."),
];

const response = await model.invoke(conversation);
console.log(response);  // AIMessage("J'adore créer des applications.")
```

--------------------------------

### Runtime Context-Based Tool Selection Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware filters tools based on user roles or permissions defined in the Runtime Context. It allows for role-based access control, granting different sets of tools to different user roles (e.g., admin, editor).

```typescript
import * as z from "zod";
import { createMiddleware } from "langchain";

const contextSchema = z.object({
  userRole: z.string(),
});

const contextBasedTools = createMiddleware({
  name: "ContextBasedTools",
  contextSchema,
  wrapModelCall: (request, handler) => {
    // Read from Runtime Context: get user role
    const userRole = request.runtime.context.userRole;  // [!code highlight]

    let filteredTools = request.tools;

    if (userRole === "admin") {
      // Admins get all tools
    } else if (userRole === "editor") {
      filteredTools = request.tools.filter(t => t.name !== "delete_data");  // [!code highlight]
    } else {
      filteredTools = request.tools.filter(t => t.name.startsWith("read_"));  // [!code highlight]
    }

    return handler({ ...request, tools: filteredTools });  // [!code highlight]
  },
});
```

--------------------------------

### Customizing Agent Memory with Extended State Schema (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This snippet illustrates how to extend the default agent state schema to include custom fields like `userId` and `preferences`. It uses Zod for schema definition and `createMiddleware` to integrate the custom state into the agent.

```typescript
import * as z from "zod";
import { createAgent, createMiddleware } from "langchain";
import { MessagesZodState, MemorySaver } from "@langchain/langgraph";

const customStateSchema = z.object({
    messages: MessagesZodState.shape.messages,
    userId: z.string(),
    preferences: z.record(z.string(), z.any()),
});

const stateExtensionMiddleware = createMiddleware({
    name: "StateExtension",
    stateSchema: customStateSchema,
});

const checkpointer = new MemorySaver();
const agent = createAgent({
    model: "gpt-5",
    tools: [],
    middleware: [stateExtensionMiddleware] as const,
    checkpointer,
});

// Custom state can be passed in invoke
const result = await agent.invoke({
    messages: [{ role: "user", content: "Hello" }],
    userId: "user_123",
    preferences: { theme: "dark" },
});
```

--------------------------------

### Approve Tool Call with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Use the 'approve' decision type to execute a tool call as-is. This method is suitable when the agent's proposed tool call requires no modifications. It takes a configuration object that includes the 'decisions' array with an 'approve' type.

```typescript
await agent.invoke(
    new Command({
        // Decisions are provided as a list, one per action under review.
        // The order of decisions must match the order of actions
        // listed in the `__interrupt__` request.
        resume: {
            decisions: [
                {
                    type: "approve",
                }
            ]
        }
    }),
    config  // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Create LLM-as-Judge Evaluator without Reference Trajectory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Creates an LLM-as-Judge evaluator using `createTrajectoryLLMAsJudge` without providing a reference trajectory. It takes an LLM model and a prompt as input and evaluates agent outputs. Dependencies include `langchain`, `@langchain/core/tools`, `@langchain/core/messages`, `agentevals`, and `zod`.

```typescript
import { createAgent } from "langchain"
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryLLMAsJudge, TRAJECTORY_ACCURACY_PROMPT } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather]
});

const evaluator = createTrajectoryLLMAsJudge({
  model: "openai:o3-mini",
  prompt: TRAJECTORY_ACCURACY_PROMPT,
});

async function testTrajectoryQuality() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in Seattle?")]
  });

  const evaluation = await evaluator({
    outputs: result.messages,
  });
  // {
  //     'key': 'trajectory_accuracy',
  //     'score': true,
  //     'comment': 'The provided agent trajectory is reasonable...'
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Invoke Model with Simple Text Prompt (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to invoke a chat model with a single string, suitable for straightforward generation tasks without requiring conversation history.

```typescript
const response = await model.invoke("Write a haiku about spring");
```

--------------------------------

### Representing Text Content in Langchain

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the structure for documenting text-based content, supporting plain text and markdown. It includes fields for content type, the text itself, an optional title, and MIME type.

```typescript
{
  type: "text-plain",
  text: "The text content",
  title?: "Title of the text content",
  mimeType?: "text/plain" | "text/markdown"
}
```

--------------------------------

### Define Agent with Response Format Type

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This snippet shows the basic type definition for `responseFormat` in LangChain's `createAgent`. It accepts either a Zod schema or a JSON schema, allowing agents to handle structured data.

```typescript
type ResponseFormat = (
    | ZodSchema<StructuredResponseT> // a Zod schema
    | Record<string, unknown> // a JSON Schema
)

const agent = createAgent({
    // ...
    responseFormat: ResponseFormat | ResponseFormat[]
})
```

--------------------------------

### ContentBlock.Text

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents standard text output with optional annotations.

```APIDOC
## ContentBlock.Text

### Description
Standard text output.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"text"`
- **text** (string) - Required - The text content
- **annotations** (Citation[]) - Optional - List of annotations for the text

### Request Example
```json
{
    "type": "text",
    "text": "Hello world",
    "annotations": []
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"text"`
- **text** (string) - The text content
- **annotations** (Citation[]) - List of annotations for the text

#### Response Example
```json
{
    "type": "text",
    "text": "Hello world",
    "annotations": []
}
```
```

--------------------------------

### Custom Tool Error Handling Middleware in Javascript

Source: https://docs.langchain.com/oss/javascript/langchain/agents

Illustrates creating custom middleware to handle errors that occur during tool execution. It catches exceptions and returns a specific `ToolMessage` with a user-friendly error to the model. Requires 'langchain' package.

```typescript
import { createAgent, createMiddleware, ToolMessage } from "langchain";

const handleToolErrors = createMiddleware({
  name: "HandleToolErrors",
  wrapToolCall: (request, handler) => {
    try {
      return handler(request);
    } catch (error) {
      // Return a custom error message to the model
      return new ToolMessage({
        content: `Tool error: Please check your input and try again. (${error})`,
        tool_call_id: request.toolCall.id!,
      });
    }
  },
});

const agent = createAgent({
  model: "gpt-4o",
  tools: [
    /* ... */
  ],
  middleware: [handleToolErrors] as const,
});
```

--------------------------------

### Similarity Search with Embedded Query

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

This code demonstrates searching a vector store using an embedded query (vector representation of a question). It requires an embeddings object to generate the query vector and then uses 'similaritySearchVectorWithScore' to find the most similar document.

```typescript
const embedding = await embeddings.embedQuery(
  "How were Nike's margins impacted in 2023?"
);

const results3 = await vectorStore.similaritySearchVectorWithScore(
  embedding,
  1
);

console.log(results3[0]);
```

--------------------------------

### Import and Use Text and Image Content Blocks in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to import and instantiate Text and Image ContentBlock types from the 'langchain' library in TypeScript. These blocks are used to represent different types of content within messages.

```typescript
import { ContentBlock } from "langchain";

// Text block
const textBlock: ContentBlock.Text = {
    type: "text",
    text: "Hello world",
}

// Image block
const imageBlock: ContentBlock.Multimodal.Image = {
    type: "image",
    url: "https://example.com/image.png",
    mimeType: "image/png",
}
```

--------------------------------

### Implement Early Exit Middleware with Jump Targets in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This TypeScript snippet shows how to implement middleware that can exit the agent's execution early using the `jumpTo` property. The `earlyExitMiddleware` uses the `beforeModel` hook to check a condition; if `shouldExit(state)` is true, it returns an object with `jumpTo: "end"` and potentially modified messages. Available jump targets include `"end"`, `"tools"`, and `"model"`.

```typescript
import { createMiddleware, AIMessage } from "langchain";

const earlyExitMiddleware = createMiddleware({
  name: "EarlyExitMiddleware",
  beforeModel: (state) => {
    // Check some condition
    if (shouldExit(state)) {
      return {
        messages: [new AIMessage("Exiting early due to condition.")],
        jumpTo: "end",
      };
    }
    return;
  },
});
```

--------------------------------

### Select Model Based on Conversation Length (State Middleware)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware dynamically selects a language model based on the number of messages in the conversation state. It initializes three models (large, standard, efficient) and chooses one based on whether the message count exceeds 20, 10, or falls below. This is useful for managing costs and performance in long-running conversations.

```typescript
import { createMiddleware, initChatModel } from "langchain";

// Initialize models once outside the middleware
const largeModel = initChatModel("claude-sonnet-4-5-20250929");
const standardModel = initChatModel("gpt-4o");
const efficientModel = initChatModel("gpt-4o-mini");

const stateBasedModel = createMiddleware({
  name: "StateBasedModel",
  wrapModelCall: (request, handler) => {
    // request.messages is a shortcut for request.state.messages
    const messageCount = request.messages.length;  // [!code highlight]
    let model;

    if (messageCount > 20) {
      model = largeModel;
    } else if (messageCount > 10) {
      model = standardModel;
    } else {
      model = efficientModel;
    }

    return handler({ ...request, model });  // [!code highlight]
  },
});
```

--------------------------------

### Enabling Log Probabilities in Langchain JS ChatOpenAI

Source: https://docs.langchain.com/oss/javascript/langchain/models

Illustrates how to enable token-level log probabilities for the ChatOpenAI model in Langchain JS. This is achieved by setting the `logprobs` parameter to `true` during model initialization. The log probabilities can then be accessed from the response metadata.

```typescript
const model = new ChatOpenAI({
    model: "gpt-4o",
    logprobs: true,
});

const responseMessage = await model.invoke("Why do parrots talk?");

responseMessage.response_metadata.logprobs.content.slice(0, 5);
```

--------------------------------

### Human Message: Using String Shortcut (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates a shortcut for using HumanMessage by passing a simple string directly to the model's invoke method. This is equivalent to using a HumanMessage object with text content.

```typescript
const response = await model.invoke("What is machine learning?");
```

--------------------------------

### Executing Tool Calls with Arguments

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates how to represent a function call to a tool, including the tool's name, arguments, and a unique identifier for the call. This is used for invoking tools within the Langchain ecosystem.

```typescript
{
  type: "tool_call",
  name: "search",
  args: { query: "weather" },
  id: "call_123"
}
```

--------------------------------

### Wrap-style Tool Call Monitoring Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

A Wrap-style middleware for monitoring tool calls. The `wrapToolCall` hook logs the tool name and arguments before executing the tool, and logs success or failure messages afterwards. This is useful for debugging and auditing tool usage within an agent.

```typescript
import { createMiddleware } from "langchain";

const toolMonitoringMiddleware = createMiddleware({
  name: "ToolMonitoringMiddleware",
  wrapToolCall: (request, handler) => {
    console.log(`Executing tool: ${request.toolCall.name}`);
    console.log(`Arguments: ${JSON.stringify(request.toolCall.args)}`);

    try {
      const result = handler(request);
      console.log("Tool completed successfully");
      return result;
    } catch (e) {
      console.log(`Tool failed: ${e}`);
      throw e;
    }
  },
});
```

--------------------------------

### Node-style Logging Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implements Node-style hooks for logging model calls. It uses `createMiddleware` to define `beforeModel` and `afterModel` hooks that log the number of messages before a model call and the content of the last model response after the call. This is useful for monitoring and debugging agent behavior.

```typescript
import { createMiddleware } from "langchain";

const loggingMiddleware = createMiddleware({
  name: "LoggingMiddleware",
  beforeModel: (state) => {
    console.log(`About to call model with ${state.messages.length} messages`);
    return;
  },
  afterModel: (state) => {
    const lastMessage = state.messages[state.messages.length - 1];
    console.log(`Model returned: ${lastMessage.content}`);
    return;
  },
});
```

--------------------------------

### Configure PII Redaction and Masking Middleware in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

This snippet demonstrates how to configure LangChain.js middleware for PII detection. It shows how to redact emails, mask credit card numbers, and block API keys using different strategies. The middleware processes both user input and AI output.

```typescript
import { createAgent, piiRedactionMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [customerServiceTool, emailTool],
  middleware: [
    // Redact emails in user input before sending to model
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToInput: true,
    }),
    // Mask credit cards in user input
    piiRedactionMiddleware({
      piiType: "credit_card",
      strategy: "mask",
      applyToInput: true,
    }),
    // Block API keys - raise error if detected
    piiRedactionMiddleware({
      piiType: "api_key",
      detector: /sk-[a-zA-Z0-9]{32}/,
      strategy: "block",
      applyToInput: true,
    }),
  ],
});

// When user provides PII, it will be handled according to the strategy
const result = await agent.invoke({
  messages: [{
    role: "user",
    content: "My email is john.doe@example.com and card is 4532-1234-5678-9010"
  }]
});

```

--------------------------------

### ContentBlock.Tools.ServerToolCall

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents a tool call intended for server-side execution, including its identifier, name, and arguments.

```APIDOC
## ServerToolCall Content Block

### Description
Represents a tool call intended for server-side execution. Includes an identifier, the tool's name, and its arguments.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"server_tool_call"`
- **id** (string) - Required - An identifier associated with the tool call
- **name** (string) - Required - The name of the tool to be called
- **args** (string) - Required - Partial tool arguments (may be incomplete JSON)

### Request Example
```json
{
  "type": "server_tool_call",
  "id": "server_call_abc",
  "name": "database_query",
  "args": "{\"sql\": \"SELECT * FROM users\""
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`server_tool_call`)
- **id** (string) - Identifier for the tool call
- **name** (string) - Name of the tool called
- **args** (string) - Partial tool arguments

#### Response Example
```json
{
  "type": "server_tool_call",
  "id": "server_call_abc",
  "name": "database_query",
  "args": "{\"sql\": \"SELECT * FROM users\""
}
```
```

--------------------------------

### Provider Strategy Function Signature

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

Illustrates the function signature for `providerStrategy`, which is used to define the structured output schema for models that support native structured output. It accepts a Zod schema or a JSON schema.

```typescript
function providerStrategy<StructuredResponseT>(
    schema: ZodSchema<StructuredResponseT> | JsonSchemaFormat
): ProviderStrategy<StructuredResponseT>
```

--------------------------------

### Handle Multiple Tool Call Decisions in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

When multiple actions are pending review, provide a decision for each in the order they appear in the interrupt request. This ensures that each tool call is addressed correctly, whether it's approved, edited, or rejected.

```typescript
{
    decisions: [
        { type: "approve" },
        {
            type: "edit",
            editedAction: {
                name: "tool_name",
                args: { param: "new_value" }
            }
        },
        {
            type: "reject",
            message: "This action is not allowed"
        }
    ]
}
```

--------------------------------

### State-Based Tool Selection Middleware (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This middleware dynamically filters tools based on the conversation state, such as authentication status and message count. It ensures sensitive tools are only available after authentication and limits access to certain tools during the initial conversation stages.

```typescript
import { createMiddleware } from "langchain";

const stateBasedTools = createMiddleware({
  name: "StateBasedTools",
  wrapModelCall: (request, handler) => {
    // Read from State: check authentication and conversation length
    const state = request.state;  // [!code highlight]
    const isAuthenticated = state.authenticated || false;  // [!code highlight]
    const messageCount = state.messages.length;

    let filteredTools = request.tools;

    // Only enable sensitive tools after authentication
    if (!isAuthenticated) {
      filteredTools = request.tools.filter(t => t.name.startsWith("public_"));  // [!code highlight]
    } else if (messageCount < 5) {
      filteredTools = request.tools.filter(t => t.name !== "advanced_search");  // [!code highlight]
    }

    return handler({ ...request, tools: filteredTools });  // [!code highlight]
  },
});
```

--------------------------------

### Custom Tool Updates Streaming in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/streaming

Demonstrates how to stream custom updates from tools during execution using the 'writer' parameter in Langchain.js. This allows for real-time feedback on tool operations within a LangGraph context. Ensure a writer function is provided when invoking tools outside of LangGraph.

```typescript
import z from "zod";
import { tool, createAgent } from "langchain";
import { LangGraphRunnableConfig } from "@langchain/langgraph";

const getWeather = tool(
    async (input, config: LangGraphRunnableConfig) => {
        // Stream any arbitrary data
        config.writer?.(`Looking up data for city: ${input.city}`);
        // ... fetch city data
        config.writer?.(`Acquired data for city: ${input.city}`);
        return `It's always sunny in ${input.city}!`;
    },
    {
        name: "get_weather",
        description: "Get weather for a given city.",
        schema: z.object({
        city: z.string().describe("The city to get weather for."),
        }),
    }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [getWeather],
});

for await (const chunk of await agent.stream(
    { messages: [{ role: "user", content: "what is the weather in sf" }] },
    { streamMode: "custom" }
)) {
    console.log(chunk);
}
```

--------------------------------

### Audio Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Outlines the structure for an audio content block. The 'type' must be 'audio', and it supports specifying the audio source through 'url', 'data' (Base64), or 'fileId', along with the 'mimeType'.

```typescript
{
  type: "audio",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Configure Model Fallback Middleware in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This middleware enables automatic fallback to alternative LLMs when the primary model fails. It accepts a variable number of fallback model strings in order. This is useful for building resilient agents and cost optimization.

```typescript
import { createAgent, modelFallbackMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o", // Primary model
  tools: [...],
  middleware: [
    modelFallbackMiddleware(
      "gpt-4o-mini", // Try first on error
      "claude-3-5-sonnet-20241022" // Then this
    ),
  ],
});
```

--------------------------------

### Invoke Model with Message Objects (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Illustrates invoking a chat model with a list of message objects, including SystemMessage, HumanMessage, and AIMessage. This approach is ideal for managing multi-turn conversations and multimodal content.

```typescript
import { SystemMessage, HumanMessage, AIMessage } from "langchain";

const messages = [
  new SystemMessage("You are a poetry expert"),
  new HumanMessage("Write a haiku about spring"),
  new AIMessage("Cherry blossoms bloom..."),
];
const response = await model.invoke(messages);
```

--------------------------------

### ContentBlock.Multimodal.Video

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents video data, which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.Video

### Description
Video data.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"video"`
- **url** (string) - Optional - URL pointing to the video location.
- **data** (string) - Optional - Base64-encoded video data.
- **fileId** (string) - Optional - Reference ID to an externally stored video file.
- **mimeType** (string) - Optional - Video MIME type (e.g., `video/mp4`, `video/webm`)

### Request Example
```json
{
    "type": "video",
    "url": "http://example.com/video.mp4",
    "mimeType": "video/mp4"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"video"`
- **url** (string) - URL pointing to the video location.
- **data** (string) - Base64-encoded video data.
- **fileId** (string) - Reference ID to an externally stored video file.
- **mimeType** (string) - Video MIME type

#### Response Example
```json
{
    "type": "video",
    "url": "http://example.com/video.mp4",
    "mimeType": "video/mp4"
}
```
```

--------------------------------

### Access PDF Document Content and Metadata in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Demonstrates how to access the page content and metadata from a loaded PDF Document. The metadata includes source file, page number, and PDF specific information. This is useful for inspecting individual document chunks.

```typescript
console.log(docs[0].pageContent.slice(0, 200));
```

```typescript
console.log(docs[0].metadata);
```

--------------------------------

### Edit Email Subject in LangGraph.js using Command

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This snippet demonstrates how to intercept and modify agent actions, specifically editing the subject of an outbound email, by utilizing the `Command` class to specify decisions for interrupts. It processes a list of interrupts, identifies the 'send_email' action, modifies its subject, and then resumes the agent's execution with the specified edits. Dependencies include `@langchain/langgraph`.

```typescript
import { Command } from "@langchain/langgraph"; // [!code highlight]

const resume: Record<string, any> = {};
for (const interrupt of interrupts) {
  const actionRequest = interrupt.value.actionRequests[0];
  if (actionRequest.name === "send_email") {
    // Edit email
    const editedAction = { ...actionRequest };
    editedAction.arguments.subject = "Mockups reminder";
    resume[interrupt.id] = {
      decisions: [{ type: "edit", editedAction }]
    };
  } else {
    resume[interrupt.id] = { decisions: [{ type: "approve" }] };
  }
}

const resumeStream = await supervisorAgent.stream(
  new Command({ resume }), // [!code highlight]
  config
);

for await (const step of resumeStream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Stream Agent Execution and Capture Interrupts

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript code snippet shows how to stream agent execution with a user query and collect any interrupt events that occur. It iterates through the streamed updates, logs messages, and stores interrupt details for later inspection. The `config` object includes `thread_id` for managing conversational state.

```typescript
const query =
  "Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, " +
  "and send them an email reminder about reviewing the new mockups.";

const config = { configurable: { thread_id: "6" } };

const interrupts: any[] = [];
const stream = await supervisorAgent.stream(
  { messages: [{ role: "user", content: query }] },
  config
);

for await (const step of stream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    } else if (Array.isArray(update)) {
      const interrupt = update[0];
      interrupts.push(interrupt);
      console.log(`\nINTERRUPTED: ${interrupt.id}`);
    }
  }
}
```

--------------------------------

### Generate Structured Output with JSON Schema in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet shows how to use a raw JSON Schema to define the desired output structure for a LangChain model. This method offers maximum control and interoperability. It requires specifying the 'jsonSchema' method.

```typescript
const jsonSchema = {
  "title": "Movie",
  "description": "A movie with details",
  "type": "object",
  "properties": {
    "title": {
      "type": "string",
      "description": "The title of the movie",
    },
    "year": {
      "type": "integer",
      "description": "The year the movie was released",
    },
    "director": {
      "type": "string",
      "description": "The director of the movie",
    },
    "rating": {
      "type": "number",
      "description": "The movie's rating out of 10",
    },
  },
  "required": ["title", "year", "director", "rating"],
}

const modelWithStructure = model.withStructuredOutput(
  jsonSchema,
  { method: "jsonSchema" },
)

const response = await modelWithStructure.invoke("Provide details about the movie Inception")
console.log(response)  // {'title': 'Inception', 'year': 2010, ...}
```

--------------------------------

### LangChain.js Handle Multiple Exception Types in Structured Output

Source: https://docs.langchain.com/oss/javascript/langchain/structured-output

This snippet demonstrates how to handle multiple distinct exception types within the `handleError` function for structured output. It allows you to define specific responses for different error categories, such as validation errors and custom user errors, providing tailored feedback for each scenario.

```typescript
const responseFormat = toolStrategy(ProductRating, {
    handleError: (error: ToolStrategyError) => {
        if (error instanceof ToolInputParsingException) {
        return "Please provide a valid rating between 1-5 and include a comment.";
        }
        if (error instanceof CustomUserError) {
        return "This is a custom user error.";
        }
        return error.message;
    }
)
```

--------------------------------

### ContentBlock.Multimodal.Audio

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents audio data, which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.Audio

### Description
Audio data.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"audio"`
- **url** (string) - Optional - URL pointing to the audio location.
- **data** (string) - Optional - Base64-encoded audio data.
- **fileId** (string) - Optional - Reference ID to an externally stored audio file.
- **mimeType** (string) - Optional - Audio MIME type (e.g., `audio/mpeg`, `audio/wav`)

### Request Example
```json
{
    "type": "audio",
    "url": "http://example.com/audio.mp3",
    "mimeType": "audio/mpeg"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"audio"`
- **url** (string) - URL pointing to the audio location.
- **data** (string) - Base64-encoded audio data.
- **fileId** (string) - Reference ID to an externally stored audio file.
- **mimeType** (string) - Audio MIME type

#### Response Example
```json
{
    "type": "audio",
    "url": "http://example.com/audio.mp3",
    "mimeType": "audio/mpeg"
}
```
```

--------------------------------

### Custom Agent State Schema for Memory in LangChain

Source: https://docs.langchain.com/oss/javascript/langchain/agents

This code illustrates how to define a custom state schema for a LangChain agent using Zod, enabling it to remember additional information beyond standard messages. It integrates `MessagesZodState` and defines `userPreferences` as a string-to-string record.

```typescript
import * as z from "zod";
import { MessagesZodState } from "@langchain/langgraph";
import { createAgent, type BaseMessage } from "langchain";

const customAgentState = z.object({
  messages: MessagesZodState.shape.messages,
  userPreferences: z.record(z.string(), z.string()),
});

const CustomAgentState = createAgent({
  model: "gpt-4o",
  tools: [],
  stateSchema: customAgentState,
});
```

--------------------------------

### Access Runtime in Tool for Context and Memory (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/runtime

Illustrates how to access the `Runtime` object within a LangChain tool. This allows tools to read contextual information (like `userName`) and interact with the long-term memory store.

```typescript
import * as z from "zod";
import { tool } from "langchain";
import { type Runtime } from "@langchain/langgraph";

const contextSchema = z.object({
  userName: z.string(),
});

const fetchUserEmailPreferences = tool(
  async (_, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    const userName = runtime.context?.userName;
    if (!userName) {
      throw new Error("userName is required");
    }

    let preferences = "The user prefers you to write a brief and polite email.";
    if (runtime.store) {
      const memory = await runtime.store?.get(["users"], userName);
      if (memory) {
        preferences = memory.value.preferences;
      }
    }
    return preferences;
  },
  {
    name: "fetch_user_email_preferences",
    description: "Fetch the user's email preferences.",
    schema: z.object({}),
  }
);
```

--------------------------------

### Define Rate Limiting Middleware with Context Schema in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This snippet demonstrates how to create a middleware function in TypeScript that enforces rate limiting. It defines a context schema using Zod for validating required configuration like `maxRequestsPerMinute` and `apiKey`. The `beforeModel` hook accesses this context from the `runtime` object to perform the rate-limiting check and can optionally jump to 'END' if the limit is exceeded. Context is provided during agent invocation.

```typescript
import * as z from "zod";
import { createMiddleware, HumanMessage } from "langchain";

const rateLimitMiddleware = createMiddleware({
  name: "RateLimitMiddleware",
  contextSchema: z.object({
    maxRequestsPerMinute: z.number(),
    apiKey: z.string(),
  }),
  beforeModel: async (state, runtime) => {
    // Access context through runtime
    const { maxRequestsPerMinute, apiKey } = runtime.context;

    // Implement rate limiting logic
    const allowed = await checkRateLimit(apiKey, maxRequestsPerMinute);
    if (!allowed) {
      return { jumpTo: "END" };
    }

    return state;
  },
});

// Context is provided through config
await agent.invoke(
  { messages: [new HumanMessage("Process data")] },
  {
    context: {
      maxRequestsPerMinute: 60,
      apiKey: "api-key-123",
    },
  }
);
```

--------------------------------

### ContentBlock.Text

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents plain text content, including markdown files, with options for specifying a title and MIME type.

```APIDOC
## Text Content Block

### Description
Represents plain text content, including markdown files. Allows specifying a title and MIME type for the text.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"text-plain"`
- **text** (string) - Required - The text content
- **title** (string) - Optional - Title of the text content
- **mimeType** (string) - Optional - MIME type of the text (e.g., `text/plain`, `text/markdown`)

### Request Example
```json
{
  "type": "text-plain",
  "text": "# Hello World\nThis is a markdown document.",
  "title": "Greeting",
  "mimeType": "text/markdown"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`text-plain`)
- **text** (string) - The text content
- **title** (string) - Optional - Title of the text content
- **mimeType** (string) - Optional - MIME type of the text

#### Response Example
```json
{
  "type": "text-plain",
  "text": "# Hello World\nThis is a markdown document.",
  "title": "Greeting",
  "mimeType": "text/markdown"
}
```
```

--------------------------------

### Human Message: Using Message Object (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to create a HumanMessage object to represent user input. This method is part of constructing more complex message lists for conversational context.

```typescript
const response = await model.invoke([
  new HumanMessage("What is machine learning?"),
]);
```

--------------------------------

### Unordered Trajectory Match Evaluation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's trajectory contains the same tool calls as a reference trajectory, irrespective of their order. This is useful when the sequence of actions is not critical. It utilizes the 'langchain' and 'agentevals' libraries.

```typescript
import { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from "langchain"
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const getEvents = tool(
  async ({ city }: { city: string }) => {
    return `Concert at the park in ${city} tonight.`;
  },
  {
    name: "get_events",
    description: "Get events happening in a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather, getEvents]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "unordered",
});

async function testMultipleToolsAnyOrder() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's happening in SF today?")]
  });

  // Reference shows tools called in different order than actual execution
  const referenceTrajectory = [
    new HumanMessage("What's happening in SF today?"),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_1", name: "get_events", args: { city: "SF" } },

```

--------------------------------

### ContentBlock.Tools.ToolCall

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents a function call to a tool, including the tool's name, arguments, and a unique identifier.

```APIDOC
## ToolCall Content Block

### Description
Represents a function call to a tool, including the tool's name, arguments, and a unique identifier.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"tool_call"`
- **name** (string) - Required - Name of the tool to call
- **args** (object) - Required - Arguments to pass to the tool
- **id** (string) - Required - Unique identifier for this tool call

### Request Example
```json
{
  "type": "tool_call",
  "name": "search",
  "args": {
    "query": "weather"
  },
  "id": "call_123"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`tool_call`)
- **name** (string) - Name of the tool called
- **args** (object) - Arguments passed to the tool
- **id** (string) - Unique identifier for the tool call

#### Response Example
```json
{
  "type": "tool_call",
  "name": "search",
  "args": {
    "query": "weather"
  },
  "id": "call_123"
}
```
```

--------------------------------

### Exact Trajectory Match Evaluation in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's output messages exactly match a reference trajectory, including the order and content of tool calls and messages. This is useful for ensuring deterministic agent behavior. It requires defining the expected sequence of messages and tool interactions.

```typescript
import { createAgent } from "langchain";
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "exact",
});

async function testAgentExactMatch() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in SF?")]
  });

  const referenceTrajectory = [
    new HumanMessage({ content: "What's the weather in SF?" }),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_2", name: "get_weather", args: { city: "SF" } },
      ]
    }),
    new ToolMessage({
      content: "It's 75 degrees and sunny in SF.",
      tool_call_id: "call_2"
    }),
    new AIMessage("Today in SF: 75 degrees and sunny."),
  ];

  const evaluation = await evaluator({
    outputs: result.messages,
    referenceOutputs: referenceTrajectory,
  });
  // {
  //     'key': 'trajectory_exact_match',
  //     'score': true,
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Sanitize SQL Query (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent

Defines a function `sanitizeSqlQuery` to validate and sanitize SQL SELECT queries. It prevents multiple statements, enforces read-only operations (SELECT only), blocks DML/DDL commands, and appends a LIMIT clause if not present. This ensures query safety before execution.

```typescript
const DENY_RE = /\b(INSERT|UPDATE|DELETE|ALTER|DROP|CREATE|REPLACE|TRUNCATE)\b/i;
const HAS_LIMIT_TAIL_RE = /\blimit\b\s+\d+(\s*,\s*\d+)?\s*;?\s*$/i;

function sanitizeSqlQuery(q) {
  let query = String(q ?? "").trim();

  // block multiple statements (allow one optional trailing ;)
  const semis = [...query].filter((c) => c === ";").length;
  if (semis > 1 || (query.endsWith(";") && query.slice(0, -1).includes(";"))) {
    throw new Error("multiple statements are not allowed.")
  }
  query = query.replace(/;+\s*$/g, "").trim();

  // read-only gate
  if (!query.toLowerCase().startsWith("select")) {
    throw new Error("Only SELECT statements are allowed")
  }
  if (DENY_RE.test(query)) {
    throw new Error("DML/DDL detected. Only read-only queries are permitted.")
  }

  // append LIMIT only if not already present
  if (!HAS_LIMIT_TAIL_RE.test(query)) {
    query += " LIMIT 5";
  }
  return query;
}
```

--------------------------------

### Video Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Details the properties for a video content block. It requires 'type' to be 'video' and allows for specifying the video source via 'url', 'data' (Base64), or 'fileId', including its 'mimeType'.

```typescript
{
  type: "video",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Streaming Tool Call Fragments

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the structure for receiving streaming fragments of tool calls. This is useful for real-time updates during tool execution, providing partial arguments and chunk information.

```typescript
{
  type: "tool_call_chunk",
  name?: "search",
  args?: "{ query: \"weather\" }",
  id?: "call_123",
  index: 0
}
```

--------------------------------

### Wrap-style Model Retry Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implements a Wrap-style middleware for retrying model calls. The `wrapModelCall` hook attempts to execute the handler multiple times, catching errors and retrying up to a specified limit. If all retries fail, the last error is re-thrown. This enhances robustness by handling transient model errors.

```typescript
import { createMiddleware } from "langchain";

const createRetryMiddleware = (maxRetries: number = 3) => {
  return createMiddleware({
    name: "RetryMiddleware",
    wrapModelCall: (request, handler) => {
      for (let attempt = 0; attempt < maxRetries; attempt++) {
        try {
          return handler(request);
        } catch (e) {
          if (attempt === maxRetries - 1) {
            throw e;
          }
          console.log(`Retry ${attempt + 1}/${maxRetries} after error: ${e}`);
        }
      }
      throw new Error("Unreachable");
    },
  });
};
```

--------------------------------

### Define and Invoke a Subagent Tool in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent

This snippet demonstrates how to define a tool that calls a subagent within a Langchain agent. It uses `createAgent` and `tool` from 'langchain', along with Zod for schema definition. The tool takes a query, invokes the subagent, and returns the last message's text. It's designed for direct subagent invocation.

```typescript
import { createAgent, tool } from "langchain";
import * as z from "zod";

const subagent1 = createAgent({...});

const callSubagent1 = tool(
  async ({ query }) => {
    const result = await subagent1.invoke({
      messages: [{ role: "user", content: query }]
    });
    return result.messages.at(-1)?.text;
  },
  {
    name: "subagent1_name",
    description: "subagent1_description",
    schema: z.object({
      query: z.string().describe("The query to to send to subagent1."),
    }),
  }
);

const agent = createAgent({
  model,
  tools: [callSubagent1]
});
```

--------------------------------

### Handling Invalid Tool Calls

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents malformed tool calls that failed during processing. It includes the tool name, raw arguments, and a specific error message detailing the failure reason, such as invalid JSON or missing fields.

```typescript
{
  type: "invalid_tool_call",
  name: "search",
  args: "{ query: weather }",
  error: "Invalid JSON"
}
```

--------------------------------

### Inspect Captured Interrupt Events Details

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This TypeScript code iterates through a list of captured interrupt events and logs the description and arguments of each action request. This allows developers to review the details of interrupted tool calls, such as calendar event creation or email sending, providing context for manual intervention or automated handling.

```typescript
for (const interrupt of interrupts) {
  for (const request of interrupt.value.actionRequests) {
    console.log(`INTERRUPTED: ${interrupt.id}`);
    console.log(`${request.description}\n`);
  }
}
```

--------------------------------

### ContentBlock.Multimodal.Image

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents image data, which can be provided via URL, base64 data, or a file ID.

```APIDOC
## ContentBlock.Multimodal.Image

### Description
Image data.

### Method
N/A (Object structure)

### Endpoint
N/A

### Parameters
#### Path Parameters
N/A

#### Query Parameters
N/A

#### Request Body
- **type** (string) - Required - Always `"image"`
- **url** (string) - Optional - URL pointing to the image location.
- **data** (string) - Optional - Base64-encoded image data.
- **fileId** (string) - Optional - Reference ID to an externally stored image.
- **mimeType** (string) - Optional - Image MIME type (e.g., `image/jpeg`, `image/png`)

### Request Example
```json
{
    "type": "image",
    "url": "http://example.com/image.jpg",
    "mimeType": "image/jpeg"
}
```

### Response
#### Success Response (200)
- **type** (string) - Always `"image"`
- **url** (string) - URL pointing to the image location.
- **data** (string) - Base64-encoded image data.
- **fileId** (string) - Reference ID to an externally stored image.
- **mimeType** (string) - Image MIME type

#### Response Example
```json
{
    "type": "image",
    "url": "http://example.com/image.jpg",
    "mimeType": "image/jpeg"
}
```
```

--------------------------------

### Streaming Server-Side Tool Call Fragments

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Handles streaming fragments of server-side tool calls. It provides the tool call ID, name, partial arguments, and the chunk's position in the stream.

```typescript
{
  type: "server_tool_call_chunk",
  id: "server_call_abc",
  name: "search",
  args: "{ \"query\": \"weather\" }",
  index: 0
}
```

--------------------------------

### File Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Describes the generic file content block, identified by 'type' set to 'file'. Similar to other multimodal types, it can reference files using 'url', 'data' (Base64), or 'fileId', and specifies the 'mimeType'.

```typescript
{
  type: "file",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Split Documents with RecursiveCharacterTextSplitter (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/rag

Splits a large document into smaller chunks using RecursiveCharacterTextSplitter. This is useful for models with context window limitations. It takes an array of Document objects and returns an array of split Document objects. The chunkSize and chunkOverlap parameters control the size and overlap of the chunks.

```typescript
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const splitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});
const allSplits = await splitter.splitDocuments(docs);
console.log(`Split blog post into ${allSplits.length} sub-documents.`);
```

--------------------------------

### Superset Trajectory Match Evaluation in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's output trajectory contains at least all the tool calls specified in the reference trajectory, allowing for additional tool calls. This is useful when the exact sequence or number of tool calls is not critical, but essential steps must be present. The `trajectoryMatchMode` is set to 'superset'.

```typescript
import { createAgent } from "langchain";
import { tool } from "@langchain/core/tools";
import { HumanMessage, AIMessage, ToolMessage } from "@langchain/core/messages";
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const getDetailedForecast = tool(
  async ({ city }: { city: string }) => {
    return `Detailed forecast for ${city}: sunny all week.`;
  },
  {
    name: "get_detailed_forecast",
    description: "Get detailed weather forecast for a city.",
    schema: z.object({ city: z.string() }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather, getDetailedForecast]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "superset",
});

async function testAgentCallsRequiredToolsPlusExtra() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in Boston?")]
  });

  // Reference only requires getWeather, but agent may call additional tools
  const referenceTrajectory = [
    new HumanMessage("What's the weather in Boston?"),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_1", name: "get_weather", args: { city: "Boston" } },
      ]
    }),
    new ToolMessage({
      content: "It's 75 degrees and sunny in Boston.",
      tool_call_id: "call_1"
    }),
    new AIMessage("The weather in Boston is 75 degrees and sunny."),
  ];

  const evaluation = await evaluator({
    outputs: result.messages,
    referenceOutputs: referenceTrajectory,
  });
  // {
  //     'key': 'trajectory_superset_match',
  //     'score': true,
  //     'comment': null,
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Combine AIMessageChunk Objects in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to iterate over a stream of AIMessageChunk objects and combine them into a single, complete AIMessage object. This is useful for processing streaming responses from AI models.

```typescript
import { AIMessageChunk } from "langchain";

let finalChunk: AIMessageChunk | undefined;
for (const chunk of chunks) {
  finalChunk = finalChunk ? finalChunk.concat(chunk) : chunk;
}
```

--------------------------------

### ContentBlock.Tools.ServerToolCallChunk

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents streaming fragments of a server-side tool call.

```APIDOC
## ServerToolCallChunk Content Block

### Description
Represents streaming fragments of a server-side tool call. Used for partial updates during server tool execution.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"server_tool_call_chunk"`
- **id** (string) - Optional - An identifier associated with the tool call
- **name** (string) - Optional - Name of the tool being called
- **args** (string) - Optional - Partial tool arguments (may be incomplete JSON)
- **index** (number | string) - Optional - Position of this chunk in the stream

### Request Example
```json
{
  "type": "server_tool_call_chunk",
  "id": "server_call_abc",
  "index": 1,
  "args": ", \"limit\": 10\""
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`server_tool_call_chunk`)
- **id** (string) - Identifier for the tool call chunk
- **name** (string) - Name of the tool being called
- **args** (string) - Partial tool arguments
- **index** (number | string) - Position of this chunk in the stream

#### Response Example
```json
{
  "type": "server_tool_call_chunk",
  "id": "server_call_abc",
  "index": 1,
  "args": ", \"limit\": 10\""
}
```
```

--------------------------------

### Implement Safety Guardrail with After Agent Hook

Source: https://docs.langchain.com/oss/javascript/langchain/guardrails

This middleware validates the final AI response after the agent has finished processing. It uses a separate model to check for safety and appropriateness, returning a canned response if the output is deemed unsafe. This is useful for preventing harmful or inappropriate content from reaching the user.

```typescript
import { createMiddleware, AIMessage, initChatModel } from "langchain";

const safetyGuardrailMiddleware = () => {
  const safetyModel = initChatModel("gpt-4o-mini");

  return createMiddleware({
    name: "SafetyGuardrailMiddleware",
    afterAgent: async (state) => {
      // Get the final AI response
      if (!state.messages || state.messages.length === 0) {
        return;
      }

      const lastMessage = state.messages[state.messages.length - 1];
      if (lastMessage._getType() !== "ai") {
        return;
      }

      // Use a model to evaluate safety
      const safetyPrompt = `Evaluate if this response is safe and appropriate.
      Respond with only 'SAFE' or 'UNSAFE'.

      Response: ${lastMessage.content.toString()}`;

      const result = await safetyModel.invoke([
        { role: "user", content: safetyPrompt }
      ]);

      if (result.content.toString().includes("UNSAFE")) {
        return {
          messages: [
            new AIMessage(
              "I cannot provide that response. Please rephrase your request."
            )
          ],
          jumpTo: "end",
        };
      }

      return;
    },
  });
};

// Use the safety guardrail
import { createAgent } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [searchTool, calculatorTool],
  middleware: [safetyGuardrailMiddleware()],
});

const result = await agent.invoke({
  messages: [{ role: "user", content: "How do I make explosives?" }]
});
```

--------------------------------

### Access Usage Metadata from AIMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to invoke a chat model and access the `usage_metadata` field of the returned AIMessage. This field can contain information such as input tokens, output tokens, and total tokens used for the model interaction.

```typescript
import { initChatModel } from "langchain";

const model = await initChatModel("gpt-5-nano");

const response = await model.invoke("Hello!");
console.log(response.usage_metadata);
```

--------------------------------

### Trim Messages Utility in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

Demonstrates how to use the `trimMessages` utility from LangChain.js to manage message history within an agent's state. This function helps prevent exceeding LLM context window limits by selectively removing messages based on specified strategies and token counts. It's typically used with a `stateModifier` before calling the LLM.

```typescript
import {
    createAgent,
    trimMessages,
    type AgentState,
} from "langchain";
import { MemorySaver } from "@langchain/langgraph";

// This function will be called every time before the node that calls LLM
const stateModifier = async (state: AgentState) => {
    return {
        messages: await trimMessages(state.messages, {
        strategy: "last",
        maxTokens: 384,
        startOn: "human",
        endOn: ["human", "tool"],
        tokenCounter: (msgs) => msgs.length,
        }),
    };
};

const checkpointer = new MemorySaver();
const agent = createAgent({
    model: "gpt-5",
    tools: [],
    preModelHook: stateModifier,
    checkpointer,
});
```

--------------------------------

### Summarization Middleware for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implement automatic summarization of conversation history to manage token limits. This middleware helps preserve context in long-running conversations by summarizing older messages when a token threshold is reached. It requires specifying the summarization model, the token limit for triggering summarization, and the number of recent messages to retain.

```typescript
import { createAgent, summarizationMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [weatherTool, calculatorTool],
  middleware: [
    summarizationMiddleware({
      model: "gpt-4o-mini",
      maxTokensBeforeSummary: 4000, // Trigger summarization at 4000 tokens
      messagesToKeep: 20, // Keep last 20 messages after summary
      summaryPrompt: "Custom prompt for summarization...", // Optional
    }),
  ],
});
```

--------------------------------

### ContentBlock.Tools.ToolCallChunk

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents streaming fragments of a tool call, used for partial updates during tool execution.

```APIDOC
## ToolCallChunk Content Block

### Description
Represents streaming fragments of a tool call, used for partial updates during tool execution.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"tool_call_chunk"`
- **name** (string) - Optional - Name of the tool being called
- **args** (string) - Optional - Partial tool arguments (may be incomplete JSON)
- **id** (string) - Optional - Tool call identifier
- **index** (number | string) - Required - Position of this chunk in the stream

### Request Example
```json
{
  "type": "tool_call_chunk",
  "id": "call_123",
  "index": 0,
  "args": "{\"query\": \"weather\""
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`tool_call_chunk`)
- **name** (string) - Name of the tool being called
- **args** (string) - Partial tool arguments
- **id** (string) - Tool call identifier
- **index** (number | string) - Position of this chunk in the stream

#### Response Example
```json
{
  "type": "tool_call_chunk",
  "id": "call_123",
  "index": 0,
  "args": "{\"query\": \"weather\""
}
```
```

--------------------------------

### Save User Info Tool for Long-Term Memory in LangChain.js

Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory

This TypeScript code defines a tool to save user information, utilizing an in-memory store for persistence. It includes input validation via Zod schemas and demonstrates how to store and retrieve data associated with a user ID. This pattern is crucial for applications requiring persistent user data or chat history.

```typescript
import * as z from "zod";
import { tool, createAgent, type AgentRuntime } from "langchain";
import { InMemoryStore, type Runtime } from "@langchain/langgraph";

// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production.
const store = new InMemoryStore(); // [!code highlight]

const contextSchema = z.object({
    userId: z.string(),
});

// Schema defines the structure of user information for the LLM
const UserInfo = z.object({
    name: z.string(),
});

// Tool that allows agent to update user information (useful for chat applications)
const saveUserInfo = tool(
  async (userInfo: z.infer<typeof UserInfo>, runtime: Runtime<z.infer<typeof contextSchema>>) => {
    const userId = runtime.context?.userId;
    if (!userId) {
      throw new Error("userId is required");
    }
    // Store data in the store (namespace, key, data)
    await runtime.store.put(["users"], userId, userInfo);
    return "Successfully saved user info.";
  },
  {
    name: "save_user_info",
    description: "Save user info",
    schema: UserInfo,
  }
);

const agent = createAgent({
    model: "gpt-4o-mini",
    tools: [saveUserInfo],
    contextSchema,
    store, // [!code highlight]
});

// Run the agent
await agent.invoke(
    { messages: [{ role: "user", content: "My name is John Smith" }] },
    // userId passed in context to identify whose information is being updated
    { context: { userId: "user_123" } } // [!code highlight]
);

// You can access the store directly to get the value
const result = await store.get(["users"], "user_123");
console.log(result?.value); // Output: { name: "John Smith" }
```

--------------------------------

### ContentBlock.Tools.InvalidToolCall

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents an invalid or malformed tool call, including details about the error.

```APIDOC
## InvalidToolCall Content Block

### Description
Represents an invalid or malformed tool call, including details about the error. Common errors include invalid JSON or missing required fields.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"invalid_tool_call"`
- **name** (string) - Optional - Name of the tool that failed to be called
- **args** (string) - Optional - Raw arguments that failed to parse
- **error** (string) - Required - Description of what went wrong

### Request Example
```json
{
  "type": "invalid_tool_call",
  "name": "search",
  "args": "{\"query\": \"weather",
  "error": "Invalid JSON: Unexpected end of JSON input"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`invalid_tool_call`)
- **name** (string) - Name of the tool that failed
- **args** (string) - Raw arguments provided
- **error** (string) - Description of the error

#### Response Example
```json
{
  "type": "invalid_tool_call",
  "name": "search",
  "args": "{\"query\": \"weather",
  "error": "Invalid JSON: Unexpected end of JSON input"
}
```
```

--------------------------------

### Read Authentication Status from State in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering

This tool reads the current authentication status from the runtime state. It checks if the user is authenticated and returns a corresponding message. This is useful for tools that require user authentication to perform their actions.

```typescript
import * as z from "zod";
import { tool } from "@langchain/core/tools";
import { createAgent } from "langchain";

const checkAuthentication = tool(
  async (_, { runtime }) => {
    // Read from State: check current auth status
    const currentState = runtime.state;
    const isAuthenticated = currentState.authenticated || false;

    if (isAuthenticated) {
      return "User is authenticated";
    } else {
      return "User is not authenticated";
    }
  },
  {
    name: "check_authentication",
    description: "Check if user is authenticated",
    schema: z.object({}),
  }
);

```

--------------------------------

### ContentBlock.Tools.ServerToolResult

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents the result of a server-side tool execution, linked to a specific tool call.

```APIDOC
## ServerToolResult Content Block

### Description
Represents the result of a server-side tool execution. It is linked to a specific server tool call via `tool_call_id`.

### Method
N/A (Represents a data structure)

### Endpoint
N/A (Represents a data structure)

### Parameters
#### Request Body
- **type** (string) - Required - Always `"server_tool_result"`
- **tool_call_id** (string) - Required - Identifier of the corresponding server tool call
- **id** (string) - Optional - Identifier associated with the server tool result

### Request Example
```json
{
  "type": "server_tool_result",
  "tool_call_id": "server_call_abc",
  "id": "result_xyz",
  "content": "[\n  {\"id\": 1, \"name\": \"Alice\"},\n  {\"id\": 2, \"name\": \"Bob\"}\n]"
}
```

### Response
#### Success Response (200)
- **type** (string) - Type of the content block (`server_tool_result`)
- **tool_call_id** (string) - Identifier of the corresponding server tool call
- **id** (string) - Identifier associated with the server tool result
- **content** (string) - The actual result content from the tool execution

#### Response Example
```json
{
  "type": "server_tool_result",
  "tool_call_id": "server_call_abc",
  "id": "result_xyz",
  "content": "[\n  {\"id\": 1, \"name\": \"Alice\"},\n  {\"id\": 2, \"name\": \"Bob\"}\n]"
}
```
```

--------------------------------

### Strict Trajectory Match Evaluation (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/test

Evaluates if an agent's trajectory exactly matches a reference trajectory in terms of messages and tool calls, maintaining the same order. This is useful for testing specific operational sequences. It requires the 'langchain' and 'agentevals' libraries.

```typescript
import { createAgent, tool, HumanMessage, AIMessage, ToolMessage } from "langchain"
import { createTrajectoryMatchEvaluator } from "agentevals";
import * as z from "zod";

const getWeather = tool(
  async ({ city }: { city: string }) => {
    return `It's 75 degrees and sunny in ${city}.`;
  },
  {
    name: "get_weather",
    description: "Get weather information for a city.",
    schema: z.object({
      city: z.string(),
    }),
  }
);

const agent = createAgent({
  model: "gpt-4o",
  tools: [getWeather]
});

const evaluator = createTrajectoryMatchEvaluator({
  trajectoryMatchMode: "strict",
});

async function testWeatherToolCalledStrict() {
  const result = await agent.invoke({
    messages: [new HumanMessage("What's the weather in San Francisco?")]
  });

  const referenceTrajectory = [
    new HumanMessage("What's the weather in San Francisco?"),
    new AIMessage({
      content: "",
      tool_calls: [
        { id: "call_1", name: "get_weather", args: { city: "San Francisco" } }
      ]
    }),
    new ToolMessage({
      content: "It's 75 degrees and sunny in San Francisco.",
      tool_call_id: "call_1"
    }),
    new AIMessage("The weather in San Francisco is 75 degrees and sunny."),
  ];

  const evaluation = await evaluator({
    outputs: result.messages,
    referenceOutputs: referenceTrajectory
  });
  // {
  //     'key': 'trajectory_strict_match',
  //     'score': true,
  //     'comment': null,
  // }
  expect(evaluation.score).toBe(true);
}
```

--------------------------------

### Configure PII Redaction Middleware in Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

This middleware detects and handles Personally Identifiable Information (PII) in conversations. It supports various strategies like redaction, masking, blocking, and hashing, and can be applied to input, output, or tool results. Custom PII types can be defined using regular expressions.

```typescript
import { createAgent, piiRedactionMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    // Redact emails in user input
    piiRedactionMiddleware({
      piiType: "email",
      strategy: "redact",
      applyToInput: true,
    }),
    // Mask credit cards (show last 4 digits)
    piiRedactionMiddleware({
      piiType: "credit_card",
      strategy: "mask",
      applyToInput: true,
    }),
    // Custom PII type with regex
    piiRedactionMiddleware({
      piiType: "api_key",
      detector: /sk-[a-zA-Z0-9]{32}/,
      strategy: "block", // Throw error if detected
    }),
  ],
});
```

--------------------------------

### Human-in-the-Loop Middleware for Langchain.js

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Enable human oversight for agent actions by pausing execution for approval, editing, or rejection of tool calls. This middleware is crucial for high-stakes operations or compliance workflows. It allows granular control over which tool calls require human intervention and the type of interaction allowed (accept, edit, respond).

```typescript
import { createAgent, humanInTheLoopMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [readEmailTool, sendEmailTool],
  middleware: [
    humanInTheLoopMiddleware({
      interruptOn: {
        // Require approval, editing, or rejection for sending emails
        send_email: {
          allowAccept: true,
          allowEdit: true,
          allowRespond: true,
        },
        // Auto-approve reading emails
        read_email: false,
      }
    })
  ]
});
```

--------------------------------

### Split Documents using RecursiveCharacterTextSplitter in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base

Splits an array of Document objects into smaller chunks using the RecursiveCharacterTextSplitter. This splitter recursively divides text based on common separators, ensuring chunks are of a specified size with overlapping content to maintain context. Dependencies include the '@langchain/textsplitters' package.

```typescript
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";

const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 1000,
  chunkOverlap: 200,
});

const allSplits = await textSplitter.splitDocuments(docs);

console.log(allSplits.length);
```

--------------------------------

### Define Nested Structured Output Schema with Zod in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/models

This snippet illustrates how to define a Zod schema with nested objects and arrays for complex structured outputs. It shows a `MovieDetail` schema that includes a `cast` array of `Actor` objects and a `genres` array of strings.

```typescript
import * as z from "zod";

const Actor = z.object({
  name: z.string(),
  role: z.string(),
});

const MovieDetails = z.object({
  title: z.string(),
  year: z.number(),
  cast: z.array(Actor),
  genres: z.array(z.string()),
  budget: z.number().nullable().describe("Budget in millions USD"),
});

const modelWithStructure = model.withStructuredOutput(MovieDetails);
```

--------------------------------

### Image Content Block Structure (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the properties for an image content block. It includes 'type' set to 'image' and can optionally specify the image location via 'url', 'data' (Base64), or 'fileId', along with its 'mimeType'.

```typescript
{
  type: "image",
  url?: string,
  data?: string,
  fileId?: string,
  mimeType?: string
}
```

--------------------------------

### Edit Email Subject Using Command in LangGraphJS

Source: https://docs.langchain.com/oss/javascript/langchain/supervisor

This snippet shows how to use the `Command` class to specify decisions for interrupts, specifically editing the subject of an outbound email. It imports `Command` from `@langchain/langgraph` and processes a stream of agent events.

```typescript
import { Command } from "@langchain/langgraph"; // [!code highlight]

const resume: Record<string, any> = {};
for (const interrupt of interrupts) {
  const actionRequest = interrupt.value.actionRequests[0];
  if (actionRequest.name === "send_email") {
    // Edit email
    const editedAction = { ...actionRequest };
    editedAction.arguments.subject = "Mockups reminder";
    resume[interrupt.id] = {
      decisions: [{ type: "edit", editedAction }]
    };
  } else {
    resume[interrupt.id] = { decisions: [{ type: "approve" }] };
  }
}

const resumeStream = await supervisorAgent.stream(
  new Command({ resume }), // [!code highlight]
  config
);

for await (const step of resumeStream) {
  for (const update of Object.values(step)) {
    if (update && typeof update === "object" && "messages" in update) {
      for (const message of update.messages) {
        console.log(message.toFormattedString());
      }
    }
  }
}
```

--------------------------------

### Server-Side Tool Call Execution

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Represents a tool call intended for server-side execution. It includes a unique ID, the tool's name, and partial arguments, which may be streamed.

```typescript
{
  type: "server_tool_call",
  id: "server_call_abc",
  name: "search",
  args: "{ \"query\": \"weather\" }"
}
```

--------------------------------

### Representing Server Tool Results

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Defines the structure for results returned from server-side tool executions. It links the result to the corresponding server tool call via `tool_call_id` and includes an optional identifier for the result itself.

```typescript
{
  type: "server_tool_result",
  tool_call_id: "server_call_abc",
  id: "result_xyz"
}
```

--------------------------------

### Node-style Conversation Length Limit Middleware in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Creates a Node-style middleware to enforce a maximum number of messages in a conversation. The `beforeModel` hook checks the message count and, if the limit is reached, returns a new AIMessage and jumps to the end of the agent's execution flow. This prevents excessively long conversations.

```typescript
import { createMiddleware, AIMessage } from "langchain";

const createMessageLimitMiddleware = (maxMessages: number = 50) => {
  return createMiddleware({
    name: "MessageLimitMiddleware",
    beforeModel: (state) => {
      if (state.messages.length === maxMessages) {
        return {
          messages: [new AIMessage("Conversation limit reached.")],
          jumpTo: "end",
        };
      }
      return;
    },
  });
};
```

--------------------------------

### Trim Message History Before Model Call in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This middleware trims the message history before a model call to manage token limits. It uses the `trimMessages` function from Langchain, with a strategy to keep the last messages and specifies token counting. This prevents exceeding context window limits for the model.

```typescript
import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware, trimMessages, type AgentState } from "langchain";

const trimMessageHistory = createMiddleware({
  name: "TrimMessages",
  beforeModel: async (state) => {
    const trimmed = await trimMessages(state.messages, {
      maxTokens: 384,
      strategy: "last",
      startOn: "human",
      endOn: ["human", "tool"],
      tokenCounter: (msgs) => msgs.length,
    });
    return { messages: trimmed };
  },
});

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [],
    middleware: [trimMessageHistory],
});
```

--------------------------------

### Limit Model Calls with LangChain.js Middleware

Source: https://docs.langchain.com/oss/javascript/langchain/middleware

Implement model call limits in your LangChain.js agents using the modelCallLimitMiddleware. This middleware helps prevent excessive API calls and manage costs by setting maximum calls per thread and per run. It supports graceful termination or throwing an error when limits are reached.

```typescript
import { createAgent, modelCallLimitMiddleware } from "langchain";

const agent = createAgent({
  model: "gpt-4o",
  tools: [...],
  middleware: [
    modelCallLimitMiddleware({
      threadLimit: 10, // Max 10 calls per thread (across runs)
      runLimit: 5, // Max 5 calls per run (single invocation)
      exitBehavior: "end", // Or "error" to throw exception
    }),
  ],
});
```

--------------------------------

### Edit Tool Call with Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop

Utilize the 'edit' decision type to modify a tool call before execution. This allows for changing the tool name or its arguments. The 'editedAction' field within the decision object specifies the new tool name and arguments.

```typescript
await agent.invoke(
    new Command({
        // Decisions are provided as a list, one per action under review.
        // The order of decisions must match the order of actions
        // listed in the `__interrupt__` request.
        resume: {
            decisions: [
                {
                    type: "edit",
                    // Edited action with tool name and args
                    editedAction: {
                        // Tool name to call.
                        // Will usually be the same as the original action.
                        name: "new_tool_name",
                        // Arguments to pass to the tool.
                        args: { key1: "new_value", key2: "original_value" },
                    }
                }
            ]
        }
    }),
    config  // Same thread ID to resume the paused conversation
);
```

--------------------------------

### Use Artifact in ToolMessage for Metadata in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Shows how to include an 'artifact' field within a ToolMessage to pass supplementary data, such as retrieval metadata, that is not sent to the model but can be accessed programmatically. This is useful for storing debugging information or downstream processing data.

```typescript
import { ToolMessage } from "langchain";

// Artifact available downstream
const artifact = { document_id: "doc_123", page: 0 };

const toolMessage = new ToolMessage({
  content: "It was the best of times, it was the worst of times.",
  tool_call_id: "call_123",
  name: "search_books",
  artifact
});
```

--------------------------------

### Add Metadata to HumanMessage

Source: https://docs.langchain.com/oss/javascript/langchain/messages

Demonstrates how to add custom metadata, such as a 'name' field, to a HumanMessage object. The behavior of the 'name' field can vary depending on the model provider, so refer to the provider's documentation for specifics.

```typescript
const humanMsg = new HumanMessage({
  content: "Hello!",
  name: "alice",
  id: "msg_123",
});
```

--------------------------------

### Validate Model Response After Model Call in TypeScript

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This middleware runs after a model call to validate the response. It checks if the last message contains the word 'confidential' (case-insensitive). If it does, the entire message history is removed using `RemoveMessage`. This helps in filtering out sensitive information.

```typescript
import { RemoveMessage } from "@langchain/core/messages";
import { createAgent, createMiddleware, type AgentState } from "langchain";

const validateResponse = createMiddleware({
  name: "ValidateResponse",
  afterModel: (state) => {
    const lastMessage = state.messages.at(-1)?.content;
    if (typeof lastMessage === "string" && lastMessage.toLowerCase().includes("confidential")) {
      return {
        messages: [new RemoveMessage({ id: "all" }), ...state.messages],
      };
    }
    return;
  },
});

const agent = createAgent({
    model: "gpt-5-nano",
    tools: [],
    middleware: [validateResponse],
});
```

--------------------------------

### Control max concurrency for batch requests in Langchain JS

Source: https://docs.langchain.com/oss/javascript/langchain/models

When processing a large number of inputs using `batch()`, you can control the maximum number of parallel calls by setting the `maxConcurrency` attribute within the `RunnableConfig` dictionary. This prevents overwhelming the model or exceeding rate limits.

```typescript
model.batch(
  listOfInputs,
  {
    maxConcurrency: 5,  // Limit to 5 parallel calls
  }
)
```

--------------------------------

### Remove Specific Messages from State (TypeScript)

Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory

This code snippet demonstrates how to remove the earliest two messages from a state object using the `RemoveMessage` class. It checks if there are more than two messages before performing the removal, ensuring the operation is only executed when applicable. This is useful for managing message history length.

```typescript
import { RemoveMessage } from "@langchain/core/messages";

const deleteMessages = (state) => {
    const messages = state.messages;
    if (messages.length > 2) {
        // remove the earliest two messages
        return {
        messages: messages
            .slice(0, 2)
            .map((m) => new RemoveMessage({ id: m.id })),
        };
    }
};
```

in this course Lance Martin will teach you how to implement rag from scratch Lance is a software engineer at Lang chain and Lang chain is one of the most common ways to implement rag Lance will help you understand how to use rag to combine custom data with llms hi this is Lance Martin I'm a software engineer at Lang chain I'm going to be giving a short course focused on rag or retrieval augmented generation which is one of the most popular kind of ideas and in llms today so really the motivation for this

is that most of the world's data is private um whereas llms are trained on publicly available data so you can kind of see on the bottom on the x-axis the number of tokens using pre-training various llms so it kind of varies from say 1.5 trillion tokens in the case of smaller models like 52 out to some very large number that we actually don't know for proprietary models like GPT 4 CLA three but what's really interesting is that the context window or the ability to feed external information into these

LMS is actually getting larger so about a year ago context windows were between 4 and 8,000 tokens you know that's like maybe a dozen pages of text we've recently seen models all the way out to a million tokens which is thousands of pages of text so while these llms are trained on large scale public data it's increasingly feasible to feed them this huge mass of private data that they've never seen that private data can be your kind of personal data it can be corporate data or you know other

information that you want to pass to an LM that's not natively in his training set and so this is kind of the main motivation for rag it's really the idea that llms one are kind of the the center of a new kind of operating system and two it's increasingly critical to be able to feed information from external sources such as private data into llms for processing so that's kind of the overarching motivation for Rag and now rag refers to retrieval augmented generation and you can think

of it in three very general steps there's a process of indexing of external data so you can think about this as you know building a database for example um many companies already have large scale databases in different forms they could be SQL DBS relational DBS um they could be Vector Stores um or otherwise but the point is that documents are indexed such that they can be retrieved based upon some heuristics relative to an input like a question and those relevant documents can be passed to an llm and the llm can produce

answers that are grounded in that retrieved information so that's kind of the centerpiece or central idea behind Rag and why it's really powerful technology because it's really uniting the the knowledge and processing capacity of llms with large scale private external data source for which most of the important data in the world still lives and in the following short videos we're going to kind of build up a complete understanding of the rag landscape and we're going to be covering

a bunch of interesting papers and techniques that explain kind of how to do rag and I've really broken it down into a few different sections so starting with a question on the left the first kind of section is what I call query trans translation so this captures a bunch of different methods to take a question from a user and modify it in some way to make it better suited for retrieval from you know one of these indexes we've talked about that can use methods like query writing it can be decomposing the query

into you know constituent sub questions then there's a question of routing so taking that decomposed a Rewritten question and routing it to the right place you might have multiple Vector stores a relational DB graph DB and a vector store so it's the challenge of getting a question to the right Source then there's a there's kind of the challenge of query construction which is basically taking natural language and converting it into the DSL necessary for whatever data source you want to work with a classic example here

is text a SQL which is kind of a very kind of well studied process but text a cipher for graph DV is very interesting text to metadata filters for Vector DBS is also a very big area of study um then there's indexing so that's the process of taking your documents and processing them in some way so they can be easily retrieved and there's a bunch of techniques for that we'll talk through we'll talk through different embedding methods we'll talk about different indexing strategies after

retrieval there are different techniques to rerank or filter retrieve documents um and then finally we'll talk about generation and kind of an interesting new set of methods to do what we might call as active rag so in that retrieval or generation stage grade documents grade answers um grade for relevance to the question grade for faithfulness to the documents I.E check for hallucinations and if either fail feedback uh re- retrieve or rewrite the question uh regenerate the qu regenerate the answer and so forth so there's a

really interesting set of methods we're going to talk through that cover that like retrieval and generation with feedback and you know in terms of General outline we'll cover the basics first it'll go through indexing retrieval and generation kind of in the Bare Bones and then we'll talk through more advanced techniques that we just saw on the prior slide career Transformations routing uh construction and so forth hi this is Lance from Lang chain this the second video in our series rack from scratch focused on

indexing so in the past video you saw the main kind of overall components of rag pipelines indexing retrieval and generation and here we're going to kind of Deep dive on indexing and give like just a quick overview of it so the first aspect of indexing is we have some external documents that we actually want to load and put into what we're trying to call Retriever and the goal of this retriever is simply given an input question I want to fish out doents that are related to my question in some

way now the way to establish that relationship or relevance or similarity is typically done using some kind of numerical representation of documents and the reason is that it's very easy to compare vectors for example of numbers uh relative to you know just free form text and so a lot of approaches have been a developed over the years to take text documents and compress them down into a numerical rep presentation that then can be very easily searched now there's a few ways to do that so Google and others came up with

many interesting statistical methods where you take a document you look at the frequency of words and you build what they call sparse vectors such that the vector locations are you know a large vocabulary of possible words each value represents the number of occurrences of that particular word and it's sparse because there's of course many zeros it's a very large vocabulary relative to what's present in the document and there's very good search methods over this this type of numerical

representation now a bit more recently uh embedding methods that are machine learned so you take a document and you build a compressed fixed length representation of that document um have been developed with correspondingly very strong search methods over embeddings um so the intuition here is that we take documents and we typically split them because embedding models actually have limited context windows so you know on the order of maybe 512 tokens up to 8,000 tokens or Beyond but they're not infinitely large so

documents are split and each document is compressed into a vector and that Vector captures a semantic meaning of the document itself the vectors are indexed questions can be embedded in the exactly same way and then numerical kind of comparison in some form you know using very different types of methods can be performed on these vectors to fish out relevant documents relative to my question um and let's just do a quick code walk through on some of these points so I have my notebook here I've

installed here um now I've set a few API keys for lsmith which are very useful for tracing which we'll see shortly um previously I walked through this this kind of quick start that just showed overall how to lay out these rag pipelines and here what I'll do is I'll Deep dive a little bit more on indexing and I'm going to take a question and a document and first I'm just going to compute the number of tokens in for example the question and this is interesting because embedding models in

llms more generally operate on tokens and so it's kind of nice to understand how large the documents are that I'm trying to feed in in this case it's obviously a very small in this case question now I'm going to specify open eye embeddings I specify an embedding model here and I just say embed embed query I can pass my question my document and what you can see here is that runs and this is mapped to now a vector of length 1536 and that fixed length Vector representation will be computed for both

documents and really for any document so you're always is kind of computing this fix length Vector that encodes the semantics of the text that you've passed now I can do things like cosine similarity to compare them and as we'll see here I can load some documents this is just like we saw previously I can split them and I can index them here just like we did before but we can see under the hood really what we're doing is we're taking each split we're embedding it using open eye embeddings into this this

kind of this Vector representation and that's stored with a link to the rod document itself in our Vector store and next we'll see how to actually do retrieval using this Vector store hi this is Lance from Lang chain and this is the third video in our series rag from scratch building up a lot of the motivations for rag uh from the very basic components um so we're going to be talking about retrieval today in the last two uh short videos I outlined indexing and gave kind of an overview of

this flow which starts with indexing of our documents retrieval of documents relevant to our question and then generation of answers based on the retriev documents and so we saw that the indexing process basically makes documents easy to retrieve and it goes through a flow that basically looks like you take our documents you split them in some way into these smaller chunks that can be easily embedded um those embeddings are then numerical representations of those documents that are easily searchable and they're stored in an

index when given a question that's also embedded the index performs a similarity search and returns splits that are relevant to the question now if we dig a little bit more under the hood we can think about it like this if we take a document and embed it let's imagine that embedding just had three dimensions so you know each document is projected into some point in this 3D space now the point is that the location in space is determined by the semantic meaning or content in that document so

to follow that then documents in similar locations in space contain similar semantic information and this very simple idea is really the Cornerstone for a lot of search and retrieval methods that you'll see with modern Vector stores so in particular we take our documents we embed them into this in this case a toy 3D space we take our question do the same we can then do a search like a local neighborhood search you can think about in this 3D space around our question to say hey what documents are

nearby and these nearby neighbors are then retrieved because they can they have similar semantics relative to our question and that's really what's going on here so again we took our documents we split them we embed them and now they exist in this high dimensional space we've taken our question embedded it projected in that same space and we just do a search around the question from nearby documents and grab ones that are close and we can pick some number we can say we want one or two or three or n

documents close to my question in this embedding space and there's a lot of really interesting methods that implement this very effectively I I link one here um and we have a lot of really nice uh Integrations to play with this general idea so many different embedding models many different indexes lots of document loaders um and lots of Splitters that can be kind of recombined to test different ways of doing this kind of indexing or retrieval um so now I'll show a bit of a code walkth through so here we defined

um we kind of had walked through this previously this is our notebook we've installed a few packages we've set a few environment variables using lsmith and we showed this previously this is just an overview showing how to run rag like kind of end to end in the last uh short talk we went through indexing um and what I'm going to do very simply is I'm just going to reload our documents so now I have our documents I'm going to resplit them and we saw before how we can build our

index now here let's actually do the same thing but in the slide we actually showed kind of that notion of search in that 3D space and a nice parameter to think about in building your your retriever is K so K tells you the number of nearby neighbors to fetch when you do that retrieval process and we talked about you know in that 3D space do I want one nearby neighbor or two or three so here we can specify k equals 1 for example now we're building our index so we're taking every split embedding it storing

it now what's nice is I asked a a question what is Task decomposition this is related to the blog post and I'm going to run get relevant documents so I run that and now how many documents do I get back I get one as expected based upon k equals 1 so this retrieve document should be related to my question now I can go to lsmith and we can open it up and we can look at our Retriever and we can see here was our question here's the one document we got back and okay so that makes sense this

document pertains to task ke decomposition in particular and it kind of lays out a number of different approaches that can be used to do that this all kind of makes sense and this shows kind of in practice how you can implement this this NE this kind of KNN or k nearest neighbor search uh really easily uh just using a few lines of code and next we're going to talk about generation thanks hey this is Lance from Lang chain this is the fourth uh short video in our rack from scratch series that's going to be focused on

generation now in the past few videos we walked through the general flow uh for kind of basic rag starting with indexing Fall by retrieval then generation of an answer based upon the documents that we retrieved that are relevant to our question this is kind of the the very basic flow now an important consideration in generation is really what's happening is we're taking the documents you retrieve and we're stuffing them into the llm context window so if we kind of walk back through the process we take

documents we split them for convenience or embedding we then embed each split and we store that in a vector store as this kind of easily searchable numerical representation or vector and we take a question embed it to produce a similar kind of numerical representation we can then search for example using something like KN andn in this kind of dimensional space for documents that are similar to our question based on their proximity or location in this space in this case you can see 3D is a toy kind of toy

example now we've recovered relevant splits to our question we pack those into the context window and we produce our answer now this introduces the notion of a prompt so the prompt is kind of a you can think have a placeholder that has for example you know in our case B keys so those keys can be like context and question so they basically are like buckets that we're going to take those retrieve documents and Slot them in we're going to take our question and also slot it in and if you kind of walk

through this flow you can kind of see that we can build like a dictionary from our retrieve documents and from our question and then we can basically populate our prompt template with the values from the dict and then becomes a prompt value which can be passed to llm like a chat model resulting in chat messages which we then parse into a string and get our answer so that's like the basic workflow that we're going to see and let's just walk through that in code very quickly to kind of give you

like a Hands-On intuition so we had our notebook we walk through previously install a few packages I'm setting a few lsmith environment variables we'll see it's it's nice for uh kind of observing and debugging our traces um previously we did this quick start we're going to skip that over um and what I will do is I'm going to build our retriever so again I'm going to take documents and load them uh and then I'm going to split them here we've kind of done this previously so I'll go

through this kind of quickly and then we're going to embed them and store them in our index so now we have this retriever object here now I'm going to jump down here now here's where it's kind of fun this is the generation bit and you can see here I'm defining something new this is a prompt template and what my prompt template is something really simple it's just going to say answer the following question based on this context it's going to have this context variable and a question so now

I'm building my prompt so great now I have this prompt let's define an llm I'll choose 35 now this introdu the notion of a chain so in Lang chain we have an expression language called L Cel Lang chain expression language which lets you really easily compose things like prompts LMS parsers retrievers and other things but the very simple kind of you know example here is just let's just take our prompt which you defined right here and connect it to an LM which you defined right here into this chain so

there's our chain now all we're doing is we're invoking that chain so every L expression language chain has a few common methods like invoke bat stream in this case we just invoke it with a dict so context and question that maps to the expected Keys here in our template and so if we run invoke what we see is it's just going to execute that chain and we get our answer now if we zoom over to Langs Smith we should see that it's been populated so yeah we see a very simple runable

sequence here was our document um and here's our output and here is our prompt answer the following question based on the context here's the document we passed in here is the question and then we get our answer so that's pretty nice um now there's a lot of other options for rag prompts I'll pull one in from our prompt tub this one's like kind of a popular prompt so it just like has a little bit more detail but you know it's the main the main intuition is the same um you're

passing in documents you're asking them to reason about the documents given a question produce an answer and now here I'm going to find a rag chain which will automatically do the retrieval for us and all I have to do is specify here's my retriever which we defined before here's our question we which we invoke with the question gets passed through to the key question in our dict and it automatically will trigger the retriever which will return documents which get passed into our context so

it's exactly what we did up here except before we did this manually and now um this is all kind of automated for us we pass that dick which is autop populated into our prompt llm out to parser now let invoke it and that should all just run and great we get an answer and we can look at the trace and we can see everything that happened so we can see our retriever was run these documents were retrieved they get passed into our LM and we get our final answer so this kind of the end of our overview um where

we talked about I'll go back to the slide here quickly we talked about indexing retrieval and now generation and follow-up short videos we'll kind of dig into some of the more com complex or detailed themes that address some limitations that can arise in this very simple pipeline thanks hi my from Lang chain over the next few videos we're going to be talking about career translation um and in this first video we're going to cover the topic of multi-query so query translation sits kind of at the

first stage of an advanced rag Pipeline and the goal of career translation is really to take an input user question and to translate in some way in order to improve retrieval so the problem statement is pretty intuitive user queries um can be ambiguous and if the query is poorly written because we're typically doing some kind of semantic similarity search between the query and our documents if the query is poorly written or ill opposed we won't retrieve the proper documents from our index so there's a few approaches to

attack this problem and you can kind of group them in a few different ways so here's one way I like to think about it a few approaches has involveed query rewriting so taking a query and reframing it like writing from a different perspective um and that's what we're going to talk about a little bit here in depth using approaches like multi-query or rag Fusion which we'll talk about in the next video you can also do things like take a question and break it down to make it less abstract

like into sub questions and there's a bunch of interesting papers focused on that like least to most from Google you can also take the opposite approach of take a question to make it more abstract uh and there's actually approach we're going to talk about later in a future video called stepback prompting that focuses on like kind of higher a higher level question from the input so the intuition though for this multier approach is we're taking a question and we're going to break it

down into a few differently worded questions uh from different perspectives and the intuition here is simply that um it is possible that the way a question is initially worded once embedded it is not well aligned or in close proximity in this High dimensional embedding space to a document that we want to R that's actually related so the thinking is that by kind of rewriting it in a few different ways you actually increase the likel of actually retrieving the document that you really want to um because of nuances in the way

that documents and questions are embedded this kind of more shotgun approach of taking a question Fanning it out into a few different perspectives May improve and increase the reliability of retrieval that's like the intuition really um and of course we can com combine this with retrieval so we can take our our kind of fan out questions do retrieval on each one and combine them in some way and perform rag so that's kind of the overview and now let's what let's go over to um our code

so this is a notebook and we're going to share all this um we're just installing a few packages we're setting a lsmith API Keys which we'll see why that's quite useful here shortly there's our diagram now first I'm going to Index this blog post on agents I'm going to split it um well I'm going to load it I'm going to split it and then I'm going to index it in chroma locally so this is a vector store we've done this previously so now I have my index defined so here is where I'm

defining my prompt for multiquery which is your your assistant your task is to basically reframe this question into a few different sub questions um so there's our prompt um right here we'll pass that to an llm part it um into a string and then split the string by new lines and so we'll get a list of questions out of this chain that's really all we're doing here now all we're doing is here's a sample input question there's our generate queries chain which we defined

we're going to take that list and then simply apply each question to retriever so we'll do retrieval per question and this little function here is just going to take the unique Union of documents uh across all those retrievals so let's run this and see what happens so we're going to run this and we're going to get some set of questions uh or documents back so let's go to Langs Smith now we can actually see what happened under the hood so here's the key point we ran our initial chain to

generate a set of of reframed questions from our input and here was that prompt and here is that set of questions that we generated now what happened is for every one of those questions we did an independent retrieval that's what we're showing here so that's kind of the first step which is great now I can go back to the notebook and we can show this working end to end so now we're going to take that retrieval chain we'll pass it into context of our final rag prompt we'll also pass through the question

we'll pass that to our rag prompt here pass it to an LM and then Pary output now let's let's kind of see how that works so again that's okay there it is so let's actually go into langth and see what happened under the hood so this was our final chain so this is great we took our input question we broke it out to these like five rephrase questions for every one of those we did a retrieval that's all great we then took the unique Union of documents and you can see in our final llm prompt answer the

following cont following question based on the context this is the final set of unique documents that we retrieved from all of our sub questions um here's our initial question there's our answer so that kind of shows you how you can set this up really easily how you can use l Smith to kind of investigate what's going on and in particular use l Smith to investigate those intermediate questions that you generate in that like kind of question generation phase and in a future talks we're going to go through um some of

these other methods that we kind of introduced at the start of this one thank you last L chain this is the second video of our Deep dive on query translation in our rag from scratch series focused on a method called rag Fusion so as we kind of showed before career translation you can think of as the first stage in an advanced rag pipeline we're taking an input user question and We're translating it some way in order to improve retrievable now we showed this General mapping of approaches previously so

again you have kind of like rewriting so you can take a question and like kind of break it down into uh differently worded are different different perspectives of the same question so that's kind of rewriting there's sub questions where you take a question break it down into smaller problems solve each one independently and then there step back where you take a question and kind of go more abstract where you kind of ask a higher level question as a precondition to answer the user question so those are

the approaches and we're going to dig into one of the particular approaches for rewriting called rat Fusion now this is really similar to what we just saw with multiquery the difference being we actually apply a a kind of a clever rank ranking step of our retriev documents um which you call reciprocal rank Fusion that's really the only difference the the input stage of taking a question breaking it out into a few kind of differently worded questions retrieval on each one is all the same

and we're going to see that in the code here shortly so let's just hop over there and then look at this so again here is a notebook that we introduced previously here's the packages we've installed we've set a few API keys for lsmith which we see why is quite useful um and you can kind of go down here to a rag Fusion section and the first thing you'll note is what our prompt is so it looks really similar to The Prompt we just saw with multiquery and simply your helpful assistant that generates multiple search

queries based upon user input and here's the question output for queries so let's define our prompt and here was our query Generation chain again this looks a lot like we just saw we take our prompt Plum that into an llm and then basically parse by new lines and that'll basically split out these questions into a list that's all it's going to happen here so that's cool now here's where the novelty comes in each time we do retrieval from one of those questions we're going to get back

a list of documents from our Retriever and so we do it over that we generate four questions here based on our prompt we do the over four questions well like a list of lists basically now reciprocal rank Fusion is really well suited for this exact problem we want to take this list to list and build a single Consolidated list and really all that's going on is it's looking at the documents in each list and kind of aggregating them into a final output ranking um and that's really the intuition around what's

happening here um so let's go ahead and so let's so let's go ahead and look at that in some detail so we can see we run retrieval that's great now let's go over to Lang Smith and have a look at what's going on here so we can see that here was our prompt to your helpful assistant that generates multiple search queries based on a single input and here is our search queries and then here are our four retrievals so that's that's really good so we know that all is working um and then those retrievals

simply went into this rank function and our correspondingly ranked to a final list of six unique rank documents that's really all we did so let's actually put that all together into an a full rag chain that's going to run retrieval return that final list of rank documents and pass it to our context pass through our question send that to a rag prompt pass it to an LM parse it to an output and let's run all that together and see that working cool so there's our final answer now let's have a look in lsmith

we can see here was our four questions here's our retrievals and then our final rag prompt plumed through the final list of ranked six questions which we can see laid out here and our final answer so this can be really convenient particularly if we're operating across like maybe different Vector stores uh or we want to do like retrieval across a large number of of kind of differently worded questions this reciprocal rank Fusion step is really nice um for example if we wanted to only take the

top three documents or something um it can be really nice to build that Consolidated ranking across all these independent retrievals then pass that to for the final generation so that's really the intuition about what's happening here thanks hi this is Lance from Lang chain this is our third video focused on query translation in the rag from scratch series and we're going to be talking about decomposition so query translation in general is a set of approaches that sits kind of towards the front of this

overall rag Pipeline and the objective is to modify or rewrite or otherwise decompose an input question from a user in order improve retrieval so we can talk through some of these approaches previously in particular various ways to do query writing like rag fusion and multiquery there's a separate set of techniques that become pretty popular and are really interesting for certain problems which we might call like kind of breaking down or decomposing an input question into a set of sub questions um so some of the papers here

that are are pretty cool are for example this work from Google um and the objective really is first to take an input question and decompose it into a set of sub problems so this particular example from the paper was the problem of um last letter concatenation and so it took the inut question of three words think machine learning and broke it down into three sub problems think think machine think machine learning as the third sub problem and then you can see in this bottom panel it solves each one

individually so it shows for example in green solving the problem think machine where you can catenate the last letter of k with the last letter of machine or last letter think K less machine e can concatenate those to K and then for the overall problem taking that solution and then and basically building on it to get the overall solution of keg so that's kind of one concept of decomposing into sub problems solving them sequentially now a related work called IRC or in leap retrieval combines

retrieval with Chain of Thought reasoning and so you can kind of put these together into one approach which you can think of as kind of dynamically retrieval um to solve a set of sub problems kind of that retrieval kind of interleaving with Chain of Thought as noted in the second paper and a set of decomposed questions based on your initial question from the first work from Google so really the idea here is we're taking one sub question we're answering it we're taking that answer and using it to help answer the second

sub question and so forth so let's actually just walk through this in code to show how this might work so this is The Notebook we've been working with from some of the other uh videos you can see we already have a retriever to find uh up here at the top and what we're going to do is we're first going to find a prompt that's basically going to say given an input question let's break it down to set of sub problems or sub question which can be solved individually so we can do that and this blog post is

focused on agents so let's ask a question about what are the main components of an LM powerered autonomous agent system so let's run this and see what the decomposed questions are so you can see the decomposed questions are what is LM technology how does it work um what are components and then how the components interact so it's kind of a sane way to kind of break down this problem into a few sub problems which you might attack individually now here's where um we Define a prompt that very

simply is going to take our question we'll take any prior questions we've answered and we'll take our retrieval and basically just combine them and we can Define this very simple chain um actually let's go back and make sure retriever is defined up at the top so now we are building our retriever good we have that now so we can go back down here and let's run this so now we are running and what's happening is we're trying to solve each of these questions individually using retrieval and using

any prior question answers so okay very good looks like that's been done and we can see here's our answer now let's go over to langth and actually see what happened under the hood so here's what's kind of of interesting and helpful to see for the first question so here's our first one it looks like it just does retrieval which is we expect and then it uses that to answer this initial question now for the second question should be a little bit more interesting because if you look at our prompt here's

our question now here is our background available question answer pair so this was the answer question answer pair from the first question which we add to our prompt and then here's the retrieval for this particular question so we're kind of building up up the solution because we're pending the question answer pair from question one and then likewise with question three it should combine all of that so we can look at here here's our question here's question one here's question two great now here's additional

retrieval related to this particular question and we get our final answer so that's like a really nice way you can kind of build up Solutions um using this kind of interleaved uh retrieval and concatenating question answer pairs I do want to mention very briefly that we can also take a different approach where we can just answer these all individually and then just concatenate all those answers to produce a final answer and I'll show that really quickly here um it's like a little bit less interesting

maybe because you're not using answers from each uh question to inform the next one you're just answering them all in parallel this might be better for cases where it's not really like a sub question decomposition but maybe it's like like a set of set of several in independent questions whose answers don't depend on each other that might be relevant for some problems um and we can go ahead and run okay so this ran as well we can look at our trace and in this case um yeah we can see that this

actually just kind of concatenates all of our QA pairs to produce the final answer so this gives you a sense for how you can use quer decomposition employ IDE IDE from uh from two different papers that are pretty cool thanks hi this is Lance from Lang chain this is the fourth video uh in our Deep dive on queer translation in the rag from scratch series and we're going to be focused on step back prompting so queer translation as we said in some of the prior videos kind of sits at the the kind of first stage of

kind of a a a rag pipeline or flow and the main aim is to take an question and to translate it or modify in such a way that it improves retrieval now we talked through a few different ways to approach this problem so one General approach involves rewriting a question and we talk about two ways to do that rag fusion multiquery and again this is this is really about taking a question and modifying it to capture a few different perspectives um which may improve the retrieval process now another approach is to take

a question and kind of make it less abstract like break it down into sub questions um and then solve each of those independently so that's what we saw with like least to most prompting um and a bunch of other variants kind of in that in that vein of sub problem solving and then consolidating those Solutions into a final answer now a different approach presented um by again Google as well is stepback prompting so stepback prompting kind of takes the the the opposite approach where it tries to ask a more

abstract question so the paper talks a lot about um using F shot prompting to produce what they call the stepback or more abstract questions and the way it does it is it provides a number of examples of stepb back questions given your original question so like this is like this is for example they like for prompt temp you're an expert World Knowledge I asked you a question your response should be comprehensive not contradict with the following um and this is kind of where you provide your like original and then

step back so here's like some example um questions so like um like uh at year saw the creation of the region where the country is located which region of the country um is the county of of herir related um Janell was born in what country what is janell's personal history so that that's maybe a more intuitive example so it's like you ask a very specific question about like the country someone's born the more abstract question is like just give me the general history of this individual

without worrying about that particular um more specific question um so let's actually just walk through how this can be done in practice um so again here's kind of like a a diagram of uh the various approaches um from less abstraction to more abstraction now here is where we're formulating our prompt using a few of the few shot examples from the paper um so again like input um yeah something about like the police perform wful arrests and what what camp members of the police do so like it it basically

gives the model a few examples um we basically formulate this into a prompt that's really all going on here again we we repeat um this overall prompt which we saw from the paper your expert World Knowledge your test is to step back and paraphrase a question generate more a generic step back question which is easier to answer here are some examples so it's like a very intuitive prompt so okay let's start with the question what is Task composition for llm agents and we're going to say generate stack

question okay so this is pretty intuitive right what is a process of task compos I so like not worrying as much about agents but what is that process of task composition in general and then hopefully that can be independently um retrieved we we can independently retrieve documents related to the stepb back question and in addition retrieve documents related to the the actual question and combine those to produce kind of final answer so that's really all that's going on um and here's the response template where we're

Plumbing in the stepback context and our question context and so what we're going to do here is we're going to take our input question and perform retrieval on that we're also going to generate our stepb back question and perform retrieval on that we're going to plumb those into the prompt as here's our very here's our basically uh our prompt Keys normal question step back question um and our overall question again we formulate those as a dict we Plum those into our response prompt um and then we

go ahead and attempt to answer our overall question so we're going to run that that's running and okay we have our answer now I want to hop over to Langs Smith and attempt to show you um kind of what that looked like under the hood so let's see let's like go into each of these steps so here was our prompt right you're an expert World Knowledge your test to to step back and paraph as a question um so um here were our few shot prompts and this was our this was our uh stepb question so what is the process of task

composition um good from the input what is Tas composition for LM agents we perform retrieval on both what is process composition uh and what is for LM agents we perform both retrievals we then populate our prompt with both uh original question answer and then here's the context retrieve from both the question and the stepb back question here was our final answer so again this is kind of a nice technique um probably depends on a lot of the types of like the type of domain you want to perform

retrieval on um but in some domains where for example there's a lot of kind of conceptual knowledge that underpins questions you expect users to ask this stepback approach could be really convenient to automatically formulate a higher level question um to for example try to improve retrieval I can imagine if you're working with like kind of textbooks or like technical documentation where you make independent chapters focused on more highlevel kind of like Concepts and then other chapters

on like more detailed uh like implementations this kind of like stepb back approach and independent retrieval could be really helpful thanks hi this is Lance from Lang chain this is the fifth video focused on queer translation in our rack from scratch series we're going to be talking about a technique called hide so again queer translation sits kind of at the front of the overall rag flow um and the objective is to take an input question and translate it in some way that improves retrieval now hide is an interesting

approach that takes advantage of a very simple idea the basic rag flow takes a question and embeds it takes a document and embeds it and looks for similarity between an embedded document and embedded question but questions and documents are very different text objects so documents can be like very large chunks taken from dense um Publications or other sources whereas questions are short kind of tur potentially ill worded from users and the intuition behind hide is take questions and map them into document

space using a hypothetical document or by generating a hypothetical document um that's the basic intuition and the idea kind of shown here visually is that in principle for certain cases a hypothetical document is closer to a desired document you actually want to retrieve in this you know High dimensional embedding space than the sparse raw input question itself so again it's just kind of means of trans translating raw questions into these hypothetical documents that are better suited for

retrieval so let's actually do a Code walkthrough to see how this works and it's actually pretty easy to implement which is really nice so first we're just starting with a prompt and we're using the same notebook that we've used for prior videos we have a blog post on agents r index um so what we're going to do is Define a prompt to generate a hypothetical documents in this case we'll say write a write a paper passage uh to answer a given question so let's just run this and see what happens again

we're taking our prompt piping it to to open Ai chck gpte and then using string Opa parer and so here's a hypothetical document section related to our question okay and this is derived of course lm's kind of embedded uh kind of World Knowledge which is you know a sane place to generate hypothetical documents now let's now take that hypothetical document and basically we're going to pipe that into a retriever so this means we're going to fetch documents from our index related to this hypothetical

document that's been embedded and you can see we get a few qu a few retrieved uh chunks that are related to uh this hypothetical document that's all we've done um and then let's take the final step where we take those retrieve documents here which we defined and our question we're going to pipe that into this rag prompt and then we're going to run our kind of rag chain right here which you've seen before and we get our answer so that's really it we can go to lsmith and we can actually

look at what happened um so here for example this was our final um rag prompt answer the following question based on this context and here is the retrieve documents that we passed in so that part's kind of straightforward we can also look at um okay this is our retrieval okay now this is this is actually what we we generated a hypothetical document here um okay so this is our hypothetical document so we've run chat open AI we generated this passage with our hypothetical document and then we've run

retrieval here so this is basically showing hypothetical document generation followed by retrieval um so again here was our passage which we passed in and then here's our retrieve documents from the retriever which are related to the passage content so again in this particular index case it's possible that the input question was sufficient to retrieve these documents in fact given prior examples uh I know that some of these same documents are indeed retrieved just from the raw question but

in other context it may not be the case so folks have reported nice performance using Hyde uh for certain domains and the Really convenient thing is that you can take this this document generation prompt you can tune this arbitrarily for your domain of Interest so it's absolutely worth experimenting with it's a it's a need approach uh that can overcome some of the challenges with retrieval uh thanks very much hi this is Lance from Lang chain this is the 10th video in our rack from scratch series

focused on routing so we talk through query translation which is the process of taking a question and translating in some way it could be decomposing it using stepback prompting or otherwise but the idea here was take our question change it into a form that's better suited for retrieval now routing is the next step which is basically routing that potentially decomposed question to the right source and in many cases that could be a different database so let's say in this toy example we have a vector

store a relational DB and a graph DB the what we redo with routing is we simply route the question based upon the cont of the question to the relevant data source so there's a few different ways to do that one is what we call logical routing in this case we basically give an llm knowledge of the various data sources that we have at our disposal and we let the llm kind of Reason about which one to apply the question to so it's kind of like the the LM is applying some logic to determine you which which

data sour for example to to use alternatively you can use semantic routing which is where we take a question we embed it and for example we embed prompts we then compute the similarity between our question and those prompts and then we choose a prompt based upon the similarity so the general idea is in our diagram we talk about routing to for example a different database but it can be very general can be routing to different prompt it can be you know really arbitrarily taking this question and sending it at different

places be at different prompts be at different Vector stores so let's walk through the code a little bit so you can see just like before we've done a few pip installs we set up lsmith and let's talk through uh logical routing first so so in this toy example let's say we had for example uh three different docs like we had python docs we had JS docs we had goang docs what we want to do is take a question route it to one of those three so what we're actually doing is we're setting up

a data model which is basically going to U be bound to our llm and allow the llm to Output one of these three options as a structured object so you really think about this as like classification classification plus function calling to produce a structured output which is constrained to these three possibilities so the way we do that is let's just zoom in here a little bit we can Define like a structured object that we want to get out from our llm like in this case we want for example you know

one of these three data sources to be output we can take this and we can actually convert it into open like open for example function schema and then we actually pass that in and bind it to our llm so what happens is we ask a question our llm invokes this function on the output to produce an output that adheres to the schema that we specify so in this case for example um we output like you know in this toy example let's say we wanted like you know an output to be data source Vector store or SQL database the output will

contain a data source object and it'll be you know one of the options we specify as a Json string we also instantiate a parser from this object to parse that Json string to an output like a pantic object for example so that's just one toy example and let's show one up here so in this case again we had our three doc sources um we bind that to our llm so you can see we do with structured output basically under the hood that's taking that object definition turning into function schema and binding that

function schema to our llm and we call our prompt you're an expert at routing a user question based on you know programming language um that user referring to so let's define our router here now what we're going to do is we'll ask a question that is python code so we'll call that and now it's done and you see the object we get out is indeed it's a route query object so it's exactly it aderes to this data model we've set up and in this case it's it's it's correct so it's calling this python

doc so you can we can extract that right here as a string now once we have this you can really easily set up like a route so this could be like our full chain where we take this router we should defined here and then this choose route function can basically take that output and do something with it so for example if python docs this could then apply the question to like a retriever full of python information uh or JS same thing so this is where you would hook basically that question up to different

chains that are like you know retriever chain one for python retriever chain two for JS and so forth so this is kind of like the routing mechanism but this is really doing the heavy lifting of taking an input question and turning into a structured object that restricts the output to one of a few output types that we care about in our like routing problem so that's really kind of the way this all hooks together now semantic outing is actually maybe even a little bit more straightforward based on what we've seen

previously so in that case let's say we have two prompts we have a physics prompt we have a math prompt we can embed those prompts no problem we do that here now let's say we have an input question from a user like in this case what is a black hole we pass that through we then apply this runnable Lambda function which is defined right here what we're doing here is we're embedding the question we're Computing similarity between the question and the prompts uh we're taking

the most similar and then we're basically choosing the prompt based on that similarity and you can see let's run that and try it out and we're using the physics prompt and there we go black holes region and space so that just shows you kind of how you can use semantic routing uh to basically embed a question embed for example various prompts pick the prompt based on sematic similarity so that really gives you just two ways to do routing one is logical routing with function in uh can be used very

generally in this case we applied it to like different coding languages but imagine these could be swapped out for like you know my python uh my like vector store versus My Graph DB versus my relational DB and you could just very simply have some description of what each is and you know then not only will the llm do reasoning but it'll also return an object uh that can be parsed very cleanly to produce like one of a few very specific types which then you can reason over like we did here in your

routing function so that kind of gives you the general idea and these are really very useful tools and I encourage you to experiment with them thanks hi this is Lance from Lang chain this is the 11th part of our rag from scratch video series focused on query construction so we previously talked through uh query translation which is the process of taking a question and converting it or translating it into a question that's better optimized for retrieval then we talked about routing which is the process of going taking

that question routing it to the right Source be it a given Vector store graph DB um or SQL DB for example now we're going to talk about the process of query construction which is basically taking natural language and converting it into particular domain specific language uh for one of these sources now we're going to talk specifically about the process of going from natural language to uh meditated filters for Vector Stores um the problem statement is basically this let's imagine we had an

index of Lang Chain video transcripts um you might want to ask a question give me you know or find find me videos on chat Lang chain published after 2024 for example um the the process of query structuring basically converts this natural language question into a structured query that can be applied to the metadata uh filters on your vector store so most Vector stores will have some kind of meditative filters that can do kind of structur querying on top of uh the chunks that are indexed um so for

example this type of query will retrieve all chunks uh that talk about the topic of chat Lang chain uh published after the date 2024 that's kind of the problem statement and to do this we're going to use function calling um in this case you can use for example open AI or other providers to do that and we're going to do is at a high level take the metadata fields that are present in our Vector store and divide them to the model as kind of information and the model then can take those and produce queries that

adhere to the schema provided um and then we can parse those out to a structured object like a identic object which again which can then be used in search so that's kind of the problem statement and let's actually walk through code um so here's our notebook which we've kind of gone through previously and I'll just show you as an example let's take a example YouTube video and let's look at the metadata that you get with the transcript so you can see you get stuff like description uh URL um

yeah publish date length things like that now let's say we had an index that had um basically a that had a number of different metadata fields and filters uh that allowed us to do range filtering on like view count publication date the video length um or unstructured search on contents and title so those are kind of like the imagine we had an index that had uh those kind of filters available to us what we can do is capture that information about the available filters in an object so we're calling that this

tutorial search object kind of encapsulates that information about the available searches that we can do and so we basically enumerate it here content search and title search or semantic searches that can be done over those fields um and then these filters then are various types of structure searches we can do on like the length um The View count and so forth and so we can just kind of build that object now we can set this up really easily with a basic simple prompt that says you know you're

an expert can bring natural language into database queries you have access to the database tutorial videos um given a question return a database query optimize retrieval so that's kind of it now here's the key point though when you call this LM with structured output you're binding this pantic object which contains all the information about our index to the llm which is exactly what we talked about previously it's really this process right here you're taking this object you're converting it to a

function schema for example open AI you're binding that to your model and then you're going to be able to get um structured object out versus a Json string from a natural language question which can then be parsed into a pantic object which you get out so that's really the flow and it's taking advantage of function calling as we said so if we go back down we set up our query analyzer chain right here now let's try to run that just on a on a purely semantic input so rag from scratch let's run that and you can see

this just does like a Content search and a title search that's exactly what you would expect now if we pass a question that includes like a date filter let's just see if that would work and there we go so you kind of still get that semantic search um but you also get um search over for example publish date earliest and latest publish date kind of as as you would expect let's try another one here so videos focus on the topic of chat Lang chain they're published before 2024 this is just kind of a rewrite of

this question in slightly different way using a different date filter and then you can see we can get we get content search title search and then we can get kind of a date search so this is a very general strategy that can be applied kind of broadly to um different kinds of querying you want to do it's really the process of going from an unstructured input to a structured query object out following an arbitrary schema that you provide and so as noted really this whole thing we created here this

tutorial search is based upon the specifics of our Vector store of interest and if you want to learn more about this I link to some documentation here that talks a lot about different uh types of of Integrations we have with different Vector store providers to do exactly this so it's a very useful trick um it allows you to do kind of query uh uh say metadata filter filtering on the fly from a natural language question it's a very convenient trick uh that works with many different Vector DBS so

encourage you to play with it thanks this is Lance from Lang chain I'm going to talk about indexing uh and mulation indexing in particular for the 12th part of our rag from scratch series here so we previously talked about a few different major areas we talk about query translation which takes a question and translates it in some way to optimize for retrieval we talk about routing which is the process of taking a question routing it to the right data source be it a vector store graph DB uh SQL DB we talked about queer

construction we dug into uh basically queer construction for Vector stores but of course there's also text SQL text to Cipher um so now we're going to talk about indexing a bit in particular we're going to talk about indexing indexing techniques for Vector Stores um and I want to highlight one particular method today called multi-representation indexing so the high LEL idea here is derived a bit from a paper called proposition indexing which kind of makes a simple observation you can think about

decoupling raw documents and the unit you use for retrieval so in the typical case you take a document you split it up in some way to index it and then you embed the split directly um this paper talks about actually taking a document splitting it in some way but then using an llm to produce what they call a proposition which you can think of as like kind of a distillation of that split so it's kind of like using an llm to modify that split in some way to distill it or make it like a crisper uh like summary so to

speak that's better optimized for retrieval so that's kind of one highlight one piece of intuition so we actually taken that idea and we've kind of built on it a bit in kind of a really nice way that I think is very well suited actually for long context llms so the idea is pretty simple you take a document and you you actually distill it or create a proposition like they show in the prior paper I kind of typically think of this as just produce a summary of the document and you embed that

summary so that summary is meant to be optimized for retrieval so might contain a bunch of keywords from the document or like the big ideas such that when you embed the summary you embed a question you do search you basically can find that document based upon this highly optimized summary for retrieval so that's kind of represented here in your vector store but here's the catch you independently store the raw document in a dock store and when you when you basically retrieve the summary in the

vector store you return the full document for the llm to perform generation and this is a nice trick because at generation time now with long condex LMS for example the LM can handle that entire document you don't need to worry about splitting it or anything you just simply use the summary to prod like to create a really nice representation for fishing out that full dock use that full dock in generation there might be a lot of reasons you want to do that you want to make sure the LM has the full

context to actually answer the question so that's the big idea it's a nice trick and let's walk through some code here we have a notebook all set up uh just like before we done some pip installs um set to maybe I Keys here for lsmith um kind of here's a diagram now let me show an example let's just load two different uh blog posts uh one is about agents one is about uh you know human data quality um and what we're going to do is let's create a summary of each of those so this is kind of the

first step of that process where we're going from like the raw documents to summaries let's just have a look and make sure those ran So Okay cool so the first DOC discusses you know building autonomous agents the second doc contains the importance of high quality human data and training okay so that's pretty nice we have our summaries now we're going to go through a process that's pretty simple first we Define a vector store that's going to index those summaries now we're going to Define what

we call like our our document storage is going to store the full documents okay so this multiv Vector retriever kind of just pulls those two things together we basically add our Dock Store we had this bite store is basically the the the full document store uh the vector store is our Vector store um and now this ID is what we're going to use to reference between the chunks or the summaries and the full documents that's really it so now for every document we'll Define a new Doc ID um and then we're basically

going to like take our summary documents um and we're going to extract um for each of our summaries we're going to get the associated doc ID so we go um so let's go ahead and do that so we have our summary docs which we add to the vector store we have our full documents uh our doc IDs and the full raw documents which are added to our doc store and then let's just do a query Vector store like a similarity search on our Vector store so memory and agents and we can see okay so we can extract

you know from the summaries we can get for example the summary that pertains to um a agents so that's a good thing now let's go ahead and run a query get relevant documents on our retriever which basically combines the summaries uh which we use for retrieval then the doc store which we use to get the full doc back so we're going to apply our query we're going to basically run this and here's the key Point we've gotten back the entire article um and we can actually if you

want to look at the whole thing we we can just go ahead and do this here we go so this is the entire article that we get back from that search so it's a pretty nice trick again we query with just memory and agents um and we can kind of go back to our diagram here we quered for memory and agents it started our summaries it found the summary related to memory and agents it uses that doc ID to reference between the vector store and the doc store it fishes out the right full doc returns us the full document in this case the full web

page that's really it simple idea nice way to go from basically like nice simple proposition style or summary style indexing to full document retrieval which is very useful especially with long contact LMS thank you hi this is Lance from Lang chain this is the 13th part of our rag from scratch series focused on a technique called Raptor so Raptor sits within kind of an array of different indexing techniques that can be applied on Vector Stores um we just talked about multi-representation indexing um we I

priv a link to a video that's very good talking about the different means of chunking so I encourage you to look at that and we're going to talk today about a technique called Raptor which you can kind of think of it as a technique for hierarchical indexing so the highle intuition is this some questions require very detailed information from a corpus to answer like pertain to a single document or single chunk so like we can call those low-level questions some questions require consolidation across kind broad swast of

a document so across like many documents or many chunks within a document and you can call those like higher level questions and so there's kind of this challenge in retrieval and that typically we do like K nearest neighbors retrieval like we've been talking about you're fishing out some number of chunks but what if you have a question that requires information across like five six you know or a number of different chunks which may exceed you know the K parameter in your retrieval so again

when you typically do retrieval you might set a k parameter of three which means you're retrieving three chunks from your vector store um and maybe you have a high very high level question that could benefit from infation across more than three so this technique called raptor is basically a way to build a hierarchical index of document summaries and the intuition is this you start with a set of documents as your Leafs here on the left you cluster them and then you Summarize each cluster so each cluster

of similar documents um will consult information from across your context which is you know your context could be a bunch of different splits or could even be across a bunch of different documents you're basically capturing similar ones and you're consolidating the information across them in a summary and here's the interesting thing you do that recursively until either you hit like a limit or you end up with one single cluster that's a kind of very high level summary of all of your

documents and what the paper shows is that if you basically just collapse all these and index them together as a big pool you end up with a really nice array of chunks that span the abstraction hierarchy like you have a bunch of chunks from Individual documents that are just like more detailed chunks pertaining to that you know single document but you also have chunks from these summaries or I would say like you know maybe not chunks but in this case the summary is like a distillation so you know raw chunks on the left that

represent your leavs are kind of like the rawest form of information either raw chunks or raw documents and then you have these higher level summaries which are all indexed together so if you have higher level questions they should basically be more similar uh in sematic search for example to these higher level summary chunks if you have lower level questions then they'll retrieve these more lower level chunks and so you have better semantic coverage across like the abstraction hierarchy of question types

that's the intuition they do a bunch of nice studies to show that this works pretty well um I actually did a deep dive video just on this which I link below um I did want to cover it briefly just at a very high level um so let's actually just do kind of a code walkr and I've added it to this rack from scratch course notebook but I link over to my deep dive video as well as the paper and the the full code notebook which is already checked in is discussed at more length in the Deep dive the

technique is a little bit detailed so I only want to give you very high levels kind of overview here and you can look at the Deep dive video if you want to go in more depth again we talked through this abstraction hierarchy um I applied this to a large set of Lang chain documents um so this is me loading basically all of our Lang chain expression language docs so this is on the order of 30 documents you can see I do a histogram here of the token counts per document some are pretty big most are fairly small less than you know

4,000 tokens um and what I did is I indexed all of them um individually so the all those raw documents you can kind of Imagine are here on the left and then I do um I do embedding I do clustering summarization and I do that recursively um until I end up with in this case I believe I only set like three levels of recursion and then I save them all my Vector store so that's like the highle idea I'm applying this Raptor technique to a whole bunch of Lang chain documents um that have fairly large number of

tokens um so I do that um and yeah I use actually use both CLA as well as open AI here um this talks through the clustering method which they that they use which is pretty interesting you can kind of dig into that on your own if if you're really um interested this is a lot of their code um which I cite accordingly um this is basically implementing the clustering method that they use um and this is just simply the document embedding stage um this is like basically embedding uh and clustering

that's really it uh some text formatting um summarizing of the clusters right here um and then this is just running that whole process recursively that's really it um this is tree building so basically I have the RO the rod docs let's just go back and look at Doc texts so this should be all my raw documents uh so that's right you can see it here doc text is basically just the text in all those Lang chain documents that I pulled um and so I run this process on them right here uh so this is that recursive

embedding cluster basically runs and produces is that tree here's the results um this is me just going through the results and basically adding the result text to this list of uh texts um oh okay so here's what I do this Leaf text is all the raw documents and I'm appending to that all the summaries that's all it's going on and then I'm indexing them all together that's the key Point rag chain and there you have it that's really all you do um so anyway I encourage you to look at this in depth

it's a pretty interesting technique it works well long with long contexts so for example one of the arguments I made is that it's kind of a nice approach to consult information across like a span of large documents like in this particular case my individual documents were lch expression language docs uh each each being somewhere in the order of you know in this case like you know most of them are less than 4,000 tokens some pretty big but I index them all I cluster them without any splits uh embed them cluster

them build this tree um and go from there and it all works because we now have llms that can go out to you know 100 or 200,000 up to million tokens and Contex so you can actually just do this process for big swats of documents in place without any without any splitting uh it's a pretty nice approach so I encourage you to think about it look at it watch the deep that video If you really want to go deeper on this um thanks hi this is Lance from Lang chain this is the 14th part of our rag from

scratch series we're going to I'm going to be talking about an approach called cold bear um so we've talked about a few different approaches for indexing and just as kind of a refresher indexing Falls uh kind of right down here in our flow we started initially with career translation taking a question translating it in some way to optimize retrieval we talked about routing it to a particular database we then talked about query construction so going from natural language to the DSL or domain

specific language for E any of the databases that you want to work with those are you know metadata filters for Vector stores or Cipher for graph DB or SQL for relational DB so that's kind of the flow we talked about today we talked about some indexing approaches like multi-representation indexing we gave a small shout out to greet camer in the series on chunking uh we talked about hierarchical indexing and I want to include one Advanced kind embedding approach so we talked a lot about embeddings are obviously very

Central to semantic similarity search um and retrieval so one of the interesting points that's been brought up is that embedding models of course take a document you can see here on the top and embed it basically compress it to a vector so it's kind of a compression process you representing all the semantics of that document in a single Vector you're doing the same to your question you're doing similarity search between the question embedding and the document embedding um in order to

perform retrieval you're typically taking the you know K most similar um document abetting is given a question and that's really how you're doing it now a lot of people said well hey the compressing a full document with all this Nuance to single Vector seems a little bit um overly restrictive right and this is a fair question to ask um there's been some interesting approaches to try to address that and one is this this this approach method called Co bear so the intuition is actually pretty

straightforward there's a bunch of good articles I link down here this is my little cartoon to explain it which I think is hopefully kind of helpful but here's the main idea instead of just taking a document and compressing it down to a single Vector basically single uh what we might call embedding Vector we take the document we break it up into tokens so tokens are just like you know units of of content it depends on the token areas you use we talked about this earlier so you basically tokenize it and

you produce basically an embedding or vector for every token and there's some kind of positional uh waiting that occurs when you do this process so you obviously you look to look at the implementation understand the details but the intuition is that you're producing some kind of representation for every token okay and you're doing the same thing for your question so you're taking your question you're breaking into a tokens and you have some representation or vector per token

and then what you're doing is for every token in the question you're Computing the similarity across all the tokens in the document and you're finding the max you're taking the max you're storing that and you're doing that process for all the tokens in the question so again token two you compare it to every token in the in the document compute the Max and then the final score is in this case the sum of the max similarities uh between every question token and any document token so it's an interesting

approach uh it reports very strong performance latency is definitely a question um so kind of production Readiness is something you should look into but it's a it's an approach that's worth mentioning here uh because it's pretty interesting um and let's walk through the code so there's actually nice Library called rouille which makes it very easy to play with Co bear um she's pip install it here I've already done that and we can use one of their pre-train models to

mediate this process so I'm basically following their documentation this is kind of what they recommended um so I'm running this now hopefully this runs somewhat quickly I'm not sure I I previously have loaded this model so hopefully it won't take too long and yeah you can see it's pretty quick uh I'm on a Mac M2 with 32 gigs um so just as like a context in terms of my my system um this is from their documentation we're just grabbing a Wikipedia page this is getting a full

document on Miyazaki so that's cool we're going to grab that now this is just from their docs this is basically how we create an index so we provide the you know some index name the collection um the max document length and yeah you should look at their documentation for these flags these are just the defaults so I'm going to create my index um so I get some logging here so it it's working under the hood um and by the way I actually have their documentation open so you can kind of follow along um

so um let's see yeah right about here so you can kind of follow this indexing process to create an index you need to load a train uh a trained model this can be either your own pre-train model or one of ours from The Hub um and this is kind of the process we're doing right now create index is just a few lines of code and this is exactly what we're doing um so this is the you know my documents and this is the indexing step that we just we just kind of walk through and it looks like it's done um

so you get a bunch of logging here that's fine um now let's actually see if this works so we're going to run drag search what an emotion Studio did Miaki found set our K parameter and we get some results okay so it's running and cool we get some documents out so you know it seems to work now what's nice is you can run this within lighting chain as a liting chain retriever so that basically wraps this as a lighting chain Retriever and then you can use it freely as a retriever within Lang chain it

works with all the other different LMS and all the other components like rankers and so forth that we talk through so you can use this directly as a retriever let's try this out and boom nice and fast um and we get our documents again this is a super simple test example you should run this maybe on more complex cases but it's pretty pretty easy spin up it's a really interesting alternative indexing approach um using again like we talked through um a very different algorithm for computing do similarity that may

work better I think an interesting regime to consider this would be longer documents so if you want like longer um yeah if if you basically want kind of long context embedding I think you should look into for example the uh Max token limits for this approach because it partitions the document into into each token um I would be curious to dig into kind of what the overall context limits are for this approach of coar but it's really interesting to consider and it reports very strong performance so

again I encourage you to play with it and this is just kind of an intro to how to get set up and to start experimenting with it really quickly thanks hi this is Lance from Lang chain I'm going to be talking about using langra to build a diverse and sophisticated rag flows so just to set the stage the basic rag flow you can see here starts with a question retrieval of relevant documents from an index which are passed into the context window of an llm for generation of an answer ground in your documents

that's kind of the basic outline and we can see it's like a very linear path um in practice though you often encounter a few different types of questions like when do we actually want to retrieve based upon the context of the question um are the retrieve documents actually good or not and if they're not good should we discard them and then how do we loot back and retry retrieval with for example and improved question so these types of questions motivate an idea of active rag which is

a process where an llm actually decides when and where to retrieve based upon like existing retrievals or existing Generations now when you think about this there's a few different levels of control that you have over an llm in a rag application the base case like we saw with our chain is just use an llm to choose a single steps output so for example in traditional rag you feed it documents and it decides to generation so it's just kind of one step now a lot of rag workflows will use the idea of

routing so like given a question should I route it to a vector store or a graph DB um and we have seen this quite a bit now this newer idea that I want to introduce is how do we build more sophisticated logical flows um in a rag pipeline um that you let the llm choose between different steps but specify all the transitions that are available and this is known as we call a state machine now there's a few different architectures that have emerged uh to build different types of rag chains and of course chains are

traditionally used just for like very basic rag but this notion of State machine is a bit newer and Lang graph which we recently released provides a really nice way to build State machines for Rag and for other things and the general idea here is that you can lay out more diverse and complicated rag flows and then Implement them as graphs and it kind of motivates this more broad idea of of like flow engineering and thinking through the actual like workflow that you want and then implementing it um and we're gonna

actually do that right now so I'm GNA Pi a recent paper called CAG corrective rag which is really a nice method um for active rag that incorporates a few different ideas um so first you retrieve documents and then you grade them now if at least one document exceeds the threshold for relevance you go to generation you generate your answer um and it does this knowledge refinement stage after that but let's not worry about that for right now it's kind of not essential for understanding

the basic flow here so again you do a grade for relevance for every document if any is relevant you generate now if they're all ambiguous or incorrect based upon your grader you retrieve from an external Source they use web search and then they pass that as their context for answer generation so it's a really neat workflow where you're doing retrieval just like with basic rag but then you're reasoning about the documents if they're relevant go ahead and at least one is relevant go ahead and generate if

they're not retrieve from alternative source and then pack that into the context and generate your answer so let's see how we would implement this as a estate machine using Lang graph um we'll make a few simplifications um we're going to first decide if any documents are relevant we'll go ahead and do the the web search um to supplement the output so that's just like kind of one minor modification um we'll use tab search for web search um we use Query writing to optimize the search for uh to optimize

the web search but it follows a lot of the the intuitions of the main paper uh small note here we set the Tav API key and another small mode I've already set my lsmith API key um with which we'll see is useful a bit later for observing the resulting traces now I'm going to index three blog posts that I like um I'm going to use chroma DB I'm G use open ey embeddings I'm going to run this right now this will create a vector store for me from these three blog posts and then what I'm going to do is

Define State now this is kind of the core object that going to be passed around my graph that I'm going to modify and right here is where I Define it and the key point to note right now is it's just a dictionary and it can contain things that are relevant for rag like question documents generation and we'll see how we update that in in in a little bit but the first thing to note is we Define our state and this is what's going to be modified in every Noe of our graph now here's really the Crux of it

and this is the thing I want to zoom in on a little bit um so when you kind of move from just thinking about promps to thinking about overall flows it it's like kind of a fun and interesting exercise I kind of think about this as it's been mentioned on Twitter a little bit more like flow engineering so let's think through what was actually done in the paper and what modifications to our state are going to happen in each stage so we start with a question you can see that on the far left and this kind of state is represent

as a dictionary like we have we start with a question we perform retrieval from our Vector store which we just created that's going to give us documents so that's one node we made an an adjustment to our state by adding documents that's step one now we have a second node where we're going to grade the documents and in this node we might filter some out so we are making a modification to state which is why it's a node so we're going to have a greater then we're going to have what

we're going to call a conditional Edge so we saw we went from question to retrieval retrieval always goes to grading and now we have a decision if any document is irrelevant we're going to go ahead and do web search to supplement and if they're all relevant will go to generation it's a minor kind of a minor kind of logical uh decision ision that we're going to make um if any are not relevant we'll transform the query and we'll do web search and we'll use that for Generation

so that's really it and that's how we can kind of think about our flow and how our States can be modified throughout this flow now all we then need to do and I I kind of found spending 10 minutes thinking carefully through your flow engineering is really valuable because from here it's really implementation details um and it's pretty easy as you'll see so basically I'm going to run this code block but then we can like walk through some of it I won't show you everything so it'll get a little bit

boring but really all we're doing is we're finding functions for every node that take in the state and modify in some way that's all it's going on so think about retrieval we run retrieval we take in state remember it's a dict we get our state dick like this we extract one keyy question from our dick we pass that to a retriever we get documents and we write back out State now with documents key added that's all generate going to be similar we take in state now we have our question and

documents we pull in a prompt we Define an llm we do minor post processing on documents we set up a chain for retrieval uh or sorry for Generation which is just going to be take our prompt pump Plum that to an llm partially output a string and we run it right here invoking our documents in our question to get our answer we write that back to State that's it and you can kind of follow here for every node we just Define a function that performs the state modification that we want to do on that

node grading documents is going to be the same um in this case I do a little thing extra here because I actually Define a identic data model for my grader so that the output of that particular grading chain is a binary yes or no you can look at the code make sure it's all shared um and that just makes sure that our output is is very deterministic so that we then can down here perform logical filtering so what you can see here is um we Define this search value no and we iterate through our documents we grade them if any

document uh is graded as not relevant we flag this search thing to yes that means we're going to perform web search we then add that to our state dict at the end so run web search now that value is true that's it and you can kind of see we go through some other nodes here there's web search node um now here is where our one conditional Edge we Define right here this is where where we decide to generate or not based on that search key so we again get our state let's extract the various values so we have this

search value now if search is yes we return the next no that we want to go to so in this case it'll be transform query which will then go to web search else we go to generate so what we can see is we laid out our graph which you can kind of see up here and now we Define functions for all those nodes as well as the conditional Edge and now we scroll down all we have to do is just lay that out here again as our flow and this is kind of what you might think of as like kind of flow engineering where you're just laying out

the graph as you drew it where we have set our entry point as retrieve we're adding an edge between retrieve and grade documents so we went retrieval grade documents we add our conditional Edge depending on the grade either transform the query go to web search or just go to generate we create an edge between transform the query and web search then web search to generate and then we also have an edge generate to end and that's our whole graph that's it so we can just run this and now I'm going to ask a question

so let's just say um how does agent memory work for example let's just try that and what this is going to do is going to print out what's going on as we run through this graph so um first we going to see output from retrieve this is going to be all of our documents that we retrieved so that's that's fine this just from our our retriever then you can see that we're doing a relevance check across our documents and this is kind of interesting right you can see we grading them here one is grade as not

relevant um and okay you can see the documents are now filtered because we removed the one that's not relevant and because one is not relevant we decide okay we're going to just transform the query and run web search and um you can see after query transformation we rewrite the question slightly we then run web search um and you can see from web search it searched from some additional sources um which you can actually see here it's appended as a so here it is so here it's a new document appended from web search

which is from memory knowledge requirements so it it basically looked up some AI architecture related to memory uh web results so that's fine that's exactly what we want to do and then um we generate a response so that's great and this is just showing you everything in kind of gory detail but I'm going to show you one other thing that's that's really nice about this if I go to lsmith I have my AP I ke set so all my Generations are just logged to to lsmith and I can see my Lang graph run here now

what's really cool is this shows me all of my nodes so remember we had retrieve grade we evaluated the grade because one was irrelevant we then went ahead and transformed the query we did a web search we pended that to our context you can see all those steps are laid out here in fact you can even look at every single uh grader and its output I will move this up slightly um so you can see the the different scores for grades okay so this particular retrieval was graded as as not relevant so that's fine that that

can happen in some cases and because of that um we did a query transformation so we modified the question slightly how does memory how does the memory system an artificial agents function so it's just a minor rephrasing of the question we did this Tav web search this is where it queried from this particular blog post from medium so it's like a sing web query we can like sanity check it and then what's need is we can go to our generate step look at open Ai and here's our full prompt how does the memory

system in our official agents function and then here's all of our documents so this is the this is the web search as well as we still have the Rel chunks that were retrieved from our blog posts um and then here's our answer so that's really it you can see how um really moving from the notion of just like I'll actually go back to the original um moving from uh I will try to open this up a little bit um yeah I can see my face still um the transition from laying out simple chains to

flows is a really interesting and helpful way of thinking about why graphs are really interesting because you can encode more sophisticated logical reasoning workflows but in a very like clean and well-engineered way where you can specify all the transitions that you actually want to have executed um and I actually find this way of thinking and building kind of logical uh like workflows really intuitive um we have a blog post coming out uh tomorrow that discusses both implementing self rag as well as C rag

for two different active rag approaches using using uh this idea of of State machines and Lang graph um so I encourage you to play with it uh I found it really uh intuitive to work with um I also found uh inspection of traces to be quite intuitive using Lang graph because every node is enumerated pretty clearly for you which is not always the case when you're using other types of of more complex reasoning approaches for example like agents so in any case um I hope this was helpful and I definitely

encourage you to check out um kind of this notion of like flow engineering using Lang graph and in the context of rag it can be really powerful hopefully as you've seen here thank you hey this is Lance from Lang chain I want to talk to a recent paper that I saw called adaptive rag which brings together some interesting ideas that have kind of been covered in other videos but this actually ties them all together in kind of a fun way so the the two big ideas to talk about here are one of query analysis so we've actually done

kind of a whole rag from scratch series that walks through each of these things in detail but this is a very nice example of how this comes together um with some other ideas we've been talking about so query analysis is typically the process of taking an input question and modifying in some way uh to better optimize retrieval there's a bunch of different methods for this it could be decomposing it into sub questions it could be using some clever techniques like stepb back prompting um but that's

kind of like the first stage of query analysis then typically you can do routing so you route a question to one of multiple potential sources it could be one or two different Vector stores it could be relational DB versus Vector store it could be web search it could just be like an llm fallback right so this is like one kind of big idea query analysis right it's kind of like the front end of your rag pipeline it's taking your question it's modifying it in some way it's sending it to the right

place be it a web search be it a vector store be it a relational DB so that's kind of topic one now topic two is something that's been brought up in a few other videos um of what I kind of call Flow engineering or adaptive rag which is the idea of doing tests in your rag pipeline or in your rag inference flow uh to do things like check relevance documents um check whether or not the answer contains hallucinations so this recent blog post from Hamil Hussein actually covers evaluation in in

some really nice detail and one of the things he highlighted explicitly is actually this topic so he talks about unit tests and in particular he says something really interesting here he says you know unlike typical unit tests you want to organize these assertions in places Beyond typical unit testing such as data cleaning and here's the key Point automatic retries during model inference that's the key thing I want to like draw your attention to to it's a really nice approach we've talked

about some other papers that do that like corrective rag self rag but it's also cool to see it here and kind of encapsulated in this way the main idea is that you're using kind of unit tests in your flow to make Corrections like if your retrieval is bad you can correct from that if your generation has hallucinations you can correct from that so I'm going to kind of draw out like a cartoon diagram of what we're going to do here and you can kind of see it here we're starting with a question we talked

about query analysis we're going to take our question and we're going to decide where it needs to go and for this particular toy example I'm going to say either send it to a vector store send it to web search or just have the llm answer it right so that's like kind of my fallback Behavior then we're going to bring in that idea of kind of online flow engineering or unit testing where I'm going to have my retrieval either from the VOR store or web search I'm then going to ask is this actually relevant

to the question if it isn't I'm actually going to kick back to web sech so this is a little bit more relevant in the case if I've routed to to the vector store done retrieval documents aren't relevant I'll have a fallback mechanism um then I'm going to generate I check for hallucinations in my generation and then I check for um for whether or not the the generation actually answers the question then I return my answer so again we're tying together two ideas one is query analysis

like basically taking a question routing it to the right place modifying it as needed and then kind of online unit testing and iterative flow feedback so to do this I've actually heard a lot of people talk online about command r a new model release from gooh here it has some pretty nice properties that I was kind of reading about recently so one it has nice support for Tool use and it does support query writing in the context of tool use uh so this all rolls up in really nice capabilities for routing it's kind of

one now two it's small it's 35 billion parameter uh it's actually open weight so you can actually run this locally and I've tried that we can we can talk about that later uh so and it's also fast served via the API so it's kind of a small model and it's well tuned for rag so I heard a lot of people talking about using coher for Rag and it has a large context 120,000 tokens so this like a nice combination of properties it supports to and routing it's small and fast so it's like quick for grading and

it's well tuned for rag so it's actually a really nice fit for this particular workflow where I want to do query analysis and routing and I want to do kind of online checking uh and rag so kind of there you go now let's just get to the coding bit so I have a notebook kind of like usual I've done a few pip installs you can see it's nothing exotic I'm bringing Lang chain coh here I set my coher API key now I'm just going to set this Lang chain project within lsmith so all my traces for this go to

that project and I have enabled tracing so I'm using Langs Smith here so we're going to walk through this flow and let's do the first thing let's just build a vector store so I'm going to build a vector store using coherent beddings with chroma open source Vector DB runs locally from three different web pages on blog post that I like so it pertains to agents prompt engineering and adversarial attacks so now I have a retriever I can run retriever invoke and I can ask a question about you know

agent memory agent memory and there we go so we get documents back so there we go we have a retriever now now here's where I'm going to bring in coh here I also want a router so you look at our flow the first step is this routing stage right so what I'm going to do is I'm guess we going to find two tools a web search tool and a vector store tool okay in my Preamble is just going to say you're an expert routing user questions to Vector store web search now here's the key I tell it

what the vector store has so again my index my Vector store has agents prompt engineering adial tax I just repeat that here agents prompt adversarial tax so it knows what's in the vector store um so use it for questions on these topics otherwise use web search so that's it I use command R here now I'm going to bind these tools to the model and attach the Preamble and I have a structured LM router so let's give it a let's give this a few tests just to like kind of sandbox this a little bit

so I can inval here's my chain I have a router prompt I pass that to the structured LM router which I defined right here and um let's ask a few different questions like who will the Bears draft in the NFL draft with types of agent memory and Hi how are you so I'm going to kick that off and you can see you know it does web search it does it goes to Vector store and then actually returns this false so that's kind of interesting um this is actually just saying if it does not use either tool so

for that particular query web search or the vector store was inappropriate it'll just say hey I didn't call one of those tools so that's interesting we'll use that later so that's my router tool now the second thing is my grader and here's where I want to show you something really nice that is generally useful uh for many different problems you might encounter so here's what I'm doing I'm defining a data model uh for My Grade so basically grade documents it's going to

have this is a pantic object it is just basically a binary score here um field specified here uh documents are relevant to the question yes no I have a preamble your grer assessing relevance of retrieve documents to a user question um blah blah blah so you know and then basically give it a b score yes no I'm using command R but here's the catch I'm using this wi structured outputs thing and I'm passing my grade documents uh data model to that that so this is the key thing we can test this right now as

well it's going to return an object based on the schema I give it which is extremely useful for all sorts of use cases and let's actually Zoom back up so we're actually right here so this greater stage right I want to constrain the output to yes no I don't want any preambles I want anything because the logic I'm going to build in this graph is going to require a yes no binary response from this particular Edge in our graph so that's why this greater tool is really useful and I'm asking like a mock

question types of agent memory I do a retriever I do a retrieval from our Vector store I get the tuck and I test it um I invoke our greater retrieval grater chain with the question the doc text and it's relevant as we would expect so that's good but again let's just kind of look at that a little bit more closely what's actually happening under the hood here here's the pantic object we passed here's the document in question I'm providing basically it's converting this

object into coher function schema it's binding that to the llm we pass in the document question it returns an object basic a Json string per our pantic schema that's it and then it's just going to like parse that into a pantic object which we get at the end of the day so that's what's happening under the hood with this with structured output thing but it's extremely useful and you'll see we're going to use that a few different places um um because we want to ensure that in our in our flow

here we have three different grading steps and each time we want to constrain the output to yes no we're going to use that structured output more than once um this is just my generation so this is good Old Rag let's just make sure that works um I'm using rag chain typical rag prompt again I'm using cohere for rag pretty easy and yeah so the rag piece works that's totally fine nothing to it crazy there um I'm going to find this llm fallback so this is basically if you saw a router chain if

it doesn't use a tool I want to fall back and just fall back to the llm so I'm going to kind of build that as a little chain here so okay this is just a fallback I have my Preamble just you're you're an assistant answer the question based upon your internal knowledge so again that fallback behavior is what we have here so what we've done already is we defined our router piece we've defined our our basic retrieval our Vector store we already have here um we've defined our first logic or like

grade check and we defined our fallback and we're just kind of roll through the parts of our graph and Define each piece um so I'm going to have two other graders and they're going to use the same thing we just talked about slightly different data model I mean same output but actually just slightly different uh prompt um and you know descript destion this in this case is the aners grounded the facts yes no this is my hallucination grater uh and then I have an answer grader as well and I've also run a test

on each one and you can see I'm getting binary this this these objects out have a binary score so this a pantic object with a binary score uh and that's exactly what we want cool and I have a Search tool so that's really nice we've actually gone through and we've kind of laid out I have like a router I've tested it we have a vector story tested we've tested each of our graders here we've also tested generation of just doing rag so we have a bunch of pieces built here we have a

fallback piece we have web search now the question is how do I Stitch these together into this kind of flow and for that I I like to use Lang graph we'll talk a little about Lang graph versus agents a bit later but I want to show you why this is really easy to do using Lang graph so what's kind of nice is I've kind of laid out all my logic here we've tested individually and now all I'm going to do is I'm going to first lay out uh the parts of my graph so what you're going

to notice here is first there's a graph state so this state represents kind of the key parts of the graph or the key kind of objects within the graph that we're going to be modifying so this is basically a graph centered around rag we're going to have question generation and documents that's really kind of the main things we're going to be working with in our graph so then you're going to see something that's pretty intuitive I think what you're going to see is we're

going to basically walk through this flow and for each of these little circles we're just going to find a function and these uh little squares or these these you can think about every Circle as a node and every kind of diamond here as as an edge or conditional Edge so that's actually what we're going to do right now we're going to lay out all of our nodes and edges and each one of them are just going to be a function and you're going to see how we do that right now so I'm going to

go down here I def find my graph state so this is what's going to be kind of modified and propagated throughout my graph now all I'm going to do is I'm just going to find a function uh for each of those nodes so let me kind of go side by side and show you the diagram and then like kind of show the nodes next to it so here's the diagram so we have uh a retrieve node so that kind of represents our Vector store we have a fallback node that's this piece we have a generate node so that's

basically going to do our rag you can see there we have a grade documents node kind of right here um and we have a web search node so that's right here cool now here's where we're actually to find the edges so you can see our edges are the pieces of the graph that are kind of making different decisions so this route question Edge basic conditional Edge is basically going to take an input question and decide where it needs to go and that's all we're doing down here it kind of

follows what we did up at the top where we tested this individually so recall we basically just invoke that question router returns our source now remember if tool calls were not in the source we do our fall back so we show actually showed that all the way up here remember this if tool calls is not in the response this thing will just be false so that means we didn't either we didn't call web search and we didn't call uh our retriever tool so then we're just going to fall back um yep right here and this is just like

uh you know a catch just in case a tool could make a decision but most interestingly here's where we choose a data source basically so um this is the output of our tool call we're just going to fish out the name of the tool so that's data source and then here we go if the data source is web search I'm returning web search as basically the next node to go to um otherwise if it's Vector store we return Vector store as the next node to go to so what's this search thing well remember we right up

here Define this node web search that's it we're just going to go to that node um what's this Vector store um you'll see below how we can kind of tie these strings that we returned from the conditional Edge to the node we want to go to that's really it um same kind of thing here decide to generate that's going to roll in these two conditional edges into one um and basically it's going to do if there's no documents so basic basically if we filtered out all of our documents from this first test

here um then what we're going to do is we've decided all documents are not relevant to the question and we're going to kick back to web search exactly as we show here so that's this piece um otherwise we're going to go to generate so that's this piece so again in these conditional edges you're basically implementing the logic that you see in our diagram right here that's all that's going on um and again this is just implementing the final two checks uh for hallucinations and and answer

relevance um and um yep so here's our hallucination grader we then extract the grade if the if basically there are hallucinations um oh sorry in this case the grade actually yes means that the answer is grounded so we say answer is actually grounded and then we go to the next step we go to the next test that's all this is doing it's just basically wrapping this logic that we're implementing here in our graph so that's all that's going on and let's go ahead and Define all those things so nice we

have all that um now we can actually go down a little bit and we can pull um this is actually where we stitch together everything so all it's happening here is you see we defined all these functions up here we just add them as nodes in our graph here and then we build our graph here basically by by basically laying out the flow or the connectivity between our nodes and edges so you know you can look at this notebook to kind of study in a bit of detail what's going on but frankly what I like to do here typically just draw

out a graph kind of like we did up here and then Implement uh the Lo logical flow here in your graph as nodes and edges just like we're doing here that's all that's happening uh so again we have like our entry point is the router um this is like the output is this is basically directing like here's what the router is outputting and here's the next node to go to so that's it um and then for each node we're kind of applying like we're saying like what's what's the

flow so web search goes to generate after um and retrieve goes to grade documents grade documents um kind of is is like is a conditional Edge um depending on the results we either do web search or generate and then our second one we go from generate to uh basically this grade uh generation versus documents in question based on the output of that we either have hallucinations we regenerate uh we found that the answer is not useful we kick back to web search or we end um finally we have that llm fallback and that's

also if we go to the fallback we end so what you're seeing here is actually the the logic flow we're laying out in this graph matches the diagram that we laid out up top I'm just going to copy these over and I'll actually go then back to the diagram and and kind of underscore that a little bit more so here is the flow we've laid out again here is our diagram and you can kind of look at them side by side and see how they basically match up so here's kind of our flow diagram going from basically

query analysis that's this thing this route question and you can see web search Vector store LM fallback LM fallback web search vector store so those are like the three options that can come out of this conditional Edge and then here's where we connect so if we go to web search then basically we next go to generate so that's kind of this whole flow um now if we go to retrieve um then we're going to grade so that's it um and you know it follows kind of as you can see here that's really it uh so

it's just nice to draw the these diagrams out first and then it's pretty quick to implement each node and each Edge just as a function and then stitch them together in a graph just like I show here and of course we'll make sure this code's publ so you can use it as a reference um so there we go now let's try a few a few different test questions so like what player the Bears to draft and NFL draft right let's have a look at that and they should print everything it's doing as we go so okay this is

important route question it just decides to route to web search that's good it doesn't go to our Vector store this is a current event not related to our Vector store at all it goes to web search um and then it goes to generate so that's what we'd expect so basically web search goes through to generate um and we check hallucinations Generations ground the documents we check generation versus question the generation addresses the question the Chicago Bears expected to draft Caleb Williams that's right that's that's the

consensus so cool that works now let's ask a question related to our Vector store what are the types of agent memory we'll kick this off so we're routing okay we're routing to rag now look how fast this is that's really fast so we basically whip through that document grading determine they're all relevant uh we go to decision to generate um we check hallucinations we check answer versus question and there are several types of memory stored in the human brain memory can also be

stored in G of Agents you have LM agents memory stream retrieval model and and reflection mechanism so it's representing what's captured on the blog post pretty reasonably now let me show you something else is kind of nice I can go to Langs Smith and I can go to my projects we create this new project coher adaptive rag at the start and everything is actually logged there everything we just did so I can open this up and I can actually just kind of look through all the stages of my Lang graph to here's my retrieval stage um

here's my grade document stage and we can kind of audit the grading itself we kind of looked at this one by one previously but it's actually pretty nice we can actually audit every single individual document grade to see what's happening um we can basically go through um to this is going to be one of the other graders here um yep so this is actually going to be the hallucination grading right here uh and then this is going to be the answer grading right here so that's really it you can kind of walk through the entire

graph you can you can kind of study what's going on um which is actually very useful so it looks like this worked pretty well um and finally let's just ask a question that should go to that fallback uh path down at the bottom like not related at all to our Vector store current events and yeah hello I'm doing well so it's pretty neat we've seen in maybe 15 minutes we've from scratch built basically a semi- sophisticated rag system that has agentic properties we've done in Lang graph we've done with

coher uh command R you can see it's pretty darn fast in fact we can go to Langs Smith and look at so this whole thing took 7 seconds uh that is not bad let's look at the most recent one so this takes one second so the fallback mechanism to the LM is like 1 second um the let's just look here so 6 seconds for the initial uh land graph so this is not bad at all it's quite fast it done it does quite a few different checks we do routing uh and then we have kind of a bunch of nice fallback behavior and

inline checking uh for both relevance hallucinations and and answer uh kind of groundedness or answer usefulness so you know this is pretty nice I definitely encourage you to play with a notebook command R is a really nice option for this due to the fact that is tool use routing uh small and quite fast and it's really good for Rags it's a very nice kind of uh a very nice option for workflows like this and I think you're going to see more and more of this kind of like uh adaptive or self-reflective

rag um just because this is something that a lot systems can benefit from like a a lot of production rack systems kind of don't necessarily have fallbacks uh depending on for example like um you know if the documents retrieved are not relevant uh if the answer contains hallucinations and so forth so this opportunity to apply inline checking along with rag is like a really nice theme I think we're going to see more and more of especially as model inference gets faster and faster and these checks get cheaper and cheaper to

do kind of in the inference Loop now as a final thing I do want to bring up the a point about you know we've shown this Lang graph stuff what about agents you know how do you think about agents versus Lang graph right and and I think the way I like to frame this is that um Lang graph is really good for um flows that you have kind of very clearly defined that don't have like kind of open-endedness but like in this case we know the steps we want to take every time we want to do um basically

query analysis routing and then we want to do a three grading steps and that's it um Lang graph is really good for building very reliable flows uh it's kind of like putting an agent on guard rails and it's really nice uh it's less flexible but highly reliable and so you can actually use smaller faster models with langra so that's the thing I like about we saw here command R 35 billion parameter model works really well with langra quite quick we' were able to implement a pretty sophisticated rag

flow really quickly 15 minutes um in time is on the order of like less than you know around 5 to 6 seconds so so pretty good right now what about agents right so I think Agents come into play when you want more flexible workflows you don't want to necessarily follow a defined pattern a priori you want an agent to be able to kind of reason and make of open-end decisions which is interesting for certain like long Horizon planning problems you know agents are really interesting the catch is that

reliability is a bit worse with agents and so you know that's a big question a lot of people bring up and that's kind of where larger LMS kind of come into play with a you know there's been a lot of questions about using small LMS even open source models with agents and reliabilities kind of continuously being an issue whereas I've been able to run these types of land graphs with um with uh like mraw or you know command R actually is open weights you can run it locally um I've been able to run them

very reproducibly with open source models on my laptop um so you know I think there's a tradeoff and Comm actually there's a new coher model coming out uh believe command R plus which uh is a larger model so it's probably more suitable for kind of more open-ended agentic use cases and there's actually a new integration with Lang chain that support uh coher agents um which is quite nice so I think it's it's worth experimenting for certain problems in workflows you may need more

open-ended reasoning in which case use an agent with a larger model otherwise you can use like Lang graph for more uh a more reliable potential but con strain flow and it can also use smaller models faster LMS so those are some of the trade-offs to keep in mind but anyway encourage you play with a notebook explore for yourself I think command R is a really nice model um I've also been experimenting with running it locally with AMA uh currently the quantise model is like uh two bit quantise is like 13 billion uh or so uh

yeah 13 gigs it's it's a little bit too large to run quickly locally for me um inference for things like rag we're on the order of 30 seconds so again it's not great for a live demo but it does work it is available on a llama so I encourage you to play with that I have a Mac M2 32 gig um so you know if I if you're a larger machine then it absolutely could be worth working with locally so encourage you to play with that anyway hopefully this was useful and interesting I think this is a cool

paper cool flow um coher command R is a nice option for these types of like routing uh it's quick good with Lang graph good for rag good for Tool use so you know have a have a look and uh you know reply anything uh any feedback in the comments thanks hi this is Lance from Lang chain this is a talk I gave at two recent meetups in San Francisco called is rag really dead um and I figured since you know a lot of people actually weren't able to make those meetups uh I just record this and put this on YouTube and

see if this is of interest to folks um so we all kind of recognize that Contex windows are getting larger for llms so on the x-axis you can see the tokens used in pre-training that's of course you know getting larger as well um proprietary models are somewhere over the two trillion token regime we don't quite know where they sit uh and we've all the way down to smaller models like 52 trained on far fewer tokens um but what's really notable is on the y axis you can see about a year ago da the art

models were on the order of 4,000 to 8,000 tokens and that's you know dozens of pages um we saw Claude 2 come out with the 200,000 token model earlier I think it was last year um gbd4 128,000 tokens now that's hundreds of pages and now we're seeing Claud 3 and Gemini come out with million token models so this is hundreds to thousands of pages so because of this phenomenon people have been kind of wondering is rag dead if you can stuff you know many thousands of pages into the context window llm why do

you need a reteval system um it's a good question spoke sparked a lot of interesting debate on Twitter um and it's maybe first just kind of grounding on what is rag so rag is really the process of reasoning and retrieval over chunks of of information that have been retrieved um it's starting with you know documents that are indexed um they're retrievable through some mechanism typically some kind of semantic similarity search or keyword search other mechanisms retriev docs should then pass to an llm

and the llm reasons about them to ground response to the question in the retrieve document so that's kind of the overall flow but the important point to make is that typically it's multiple documents and involve some form of reasoning so one of the questions I asked recently is you know if long condex llms can replace rag it should be able to perform you know multia retrieval and reasoning from its own context really effectively so I teamed up with Greg Cameron uh to kind of pressure test this and he had done some

really nice needle the Haack analyses already focused on kind of single facts called needles placed in a Hy stack of Paul Graham essays um so I kind of extended that to kind of mirror the rag use case or kind of the rag context uh where I took multiple facts so I call it multi needle um I buil on a funny needle in the HTO challenge published by anthropic where they add they basically placed Pizza ingredients in the context uh and asked the LM to retrieve this combination of pizza ingredients I did I

kind of Rift on that and I basically split the pizza ingredients up into three different needles and place those three ingredients in different places in the context and then ask the um to recover those three ingredients um from the context so again the setup is the question is what the secret ingredients need to build a perfect Pizza the needles are the ingredients figs Pudo goat cheese um I place them in the context at some specified intervals the way this test works is you can basically set the percent of context you want to

place the first needle and the remaining two are placed at roughly equal intervals in the remaining context after the first so that's kind of the way the test is set up now it's all open source by the way the link is below so needs are placed um you ask a question you promp L them with with kind of um with this context and the question and then produces the answer and now the the framework will grade the response both one are you know all are all the the specified ingredients present in the

answer and two if not which ones are missing so I ran a bunch of analysis on this with GPD 4 and came kind of came up with some with some fun results um so you can see on the left here what this is looking at is different numbers of needles placed in 120,000 token context window for gbd4 and I'm asking um gbd4 to retrieve either one three or 10 needles now I'm also asking it to do reasoning on those needles that's what you can see in those red bars so green is just retrieve the ingredients red is reasoning and the

reasoning challenge here is just return the first letter of each ingredient so we find is basically two things the performance or the percentage of needles retrieved drops with respect to the number of needles that's kind of intuitive you place more facts performance gets worse but also it gets worse if you ask it to reason so if you say um just return the needles it does a little bit better than if you say return the needles and tell me the first letter so you overlay reasoning so this is the

first observation more facts is harder uh and reasoning is harder uh than just retrieval now the second question we ask is where are these needles actually present in the context that we're missing right so we know for example um retrieval of um 10 needles is around 60% so where are the missing needles in the context so on the right you can see results telling us actually which specific needles uh are are the model fails to retrieve so what we can see is as you go from a th000 tokens up to 120,000 tokens on the X here and you

look at needle one place at the start of the document to needle 10 placed at the end at a th000 token context link you can retrieve them all so again kind of match what we see over here small well actually sorry over here everything I'm looking at is 120,000 tokens so that's really not the point uh the point is actually smaller context uh better retrieval so that's kind of point one um as I increase the context window I actually see that uh there is increased failure to retrieve needles which you

see can see in red here towards the start of the document um and so this is an interesting result um and it actually matches what Greg saw with single needle case as well so the way to think about it is it appears that um you know if you for example read a book and I asked you a question about the first chapter you might have forgotten it same kind of phenomenon appears to happen here with retrieval where needles towards the start of the context are are kind of Forgotten or are not well retrieved

relative to those of the end so this is an effect we see with gbd4 it's been reproduced quite a bit so ran nine different trials here Greg's also seen this repeatedly with single needle so it seems like a pretty consistent result and there's an interesting point I put this on Twitter and a number of folks um you know replied and someone sent me this paper which is pretty interesting and it mentions recency bias is one possible reason so the most informative tokens for predicting the

next token uh you know are are are present close to or recent to kind of where you're doing your generation and so there's a bias to attend to recent tokens which is obviously not great for the retrieval problem as we saw here so again the results show us that um reasoning is a bit harder than retrieval more needles is more difficult and needles towards the start of your context are harder to retrieve than towards the end those are three main observations from this and it maybe indeed due to this recency bias so

overall what this kind of tells you is be wary of just context stuffing in large long context there are no retrieval guarantees and also there's some recent results that came out actually just today suggesting that single needle may be misleadingly easy um you know there's no reasoning it's retrieving a single needle um and also these guys I'm I showed this tweet here show that um the in a lot of these needle and Haack challenges including mine the facts that we look for are very different than um

the background kind of Hy stack of Paul Graham essays and so that may be kind of an interesting artifact they note that indeed if the needle is more subtle retrievals is worse so I think basically when you see these really strong performing needle and hyack analyses put up by model providers you should be skeptical um you shouldn't necessarily assume that you're going to get high quality retrieval from these long contact LMS uh for numerous reasons you need to think about retrieval of multiple facts um you need to think

about reasoning on top of retrieval you need need to think about the subtlety of the retrieval relative to the background context because for many of these needle and the Haack challenges it's a single needle no reasoning and the needle itself is very different from the background so anyway those may all make the challenge a bit easier than a real world scenario of fact retrieval so I just want to like kind of lay out that those cautionary notes but you know I think it is fair to say this will

certainly get better and I think it's also fair to say that rag will change and this is just like a nearly not a great joke but Frank zap a musician made the point Jazz isn't dead it just smells funny you know I think same for rag rag is not dead but it will change I think that's like kind of the key Point here um so just as a followup on that rag today's focus on precise retrieval of relevant doc chunks so it's very focused on typically taking documents chunking them in some particular way often using

very OS syncratic chunking methods things like chunk size are kind of picked almost arbitrarily embeding them storing them in an index taking a question embedding it doing K&N uh similarity search to retrieve relevant chunks you're often setting a k parameter which is the number of chunks you retrieve you often will do some kind of filtering or Pro processing on the retrieve chunks and then ground your answer in those retrieved chunks so it's very focused on precise retrieval of just the right chunks now in a world

where you have very long context models I think there's a fair question to ask is is this really kind of the most most reasonable approach so kind of on the left here you can kind of see this notion closer to today of I need the exact relevant chunk you can risk over engineering you can have you know higher complexity sensitivity to these odd parameters like chunk size k um and you can indeed suffer lower recall because you're really only picking very precise chunks you're beholden to very

particular embedding models so you know I think going forward as long context models get better and better there are definitely question you should certainly question the current kind of very precise chunking rag Paradigm but on the flip side I think just throwing all your docs into context probably will also not be the preferred approach you'll suffer higher latency higher token usage I should note that today 100,000 token GPD 4 is like $1 per generation I spent a lot of money on Lang Chain's account uh

on that multile analysis I don't want to tell Harrison how much I spent uh so it's it's you know it's not good right um You Can't audit retrieve um and security and and authentication are issues if for example you need different users different different access to certain kind of retriev documents or chunks in the Contex stuffing case you you kind of can't manage security as easily so there's probably some predo optimal regime kind of here in the middle and um you know I

I put this out on Twitter I think there's some reasonable points raised I think you know this inclusion at the document level is probably pretty sane documents are self-contained chunks of context um so you know what about document Centric rag so no chunking uh but just like operate on the context of full documents so you know if you think forward to the rag Paradigm that's document Centric you still have the problem of taking an input question routing it to the right document um this doesn't change so I think a lot of

methods that we think about for kind of query analysis um taking an input question rewriting it in a certain way to optimize retrieval things like routing taking a question routing to the right database be it a relational database graph database Vector store um and quer construction methods so for example text to SQL text to Cipher for graphs um or text to even like metadata filters for for Vector stores those are all still relevant in the world that you have long Contex llms um you're probably

not going to dump your entire SQL DB and feed that to the llm you're still going to have SQL queries you're still going to have graph queries um you may be more permissive with what you extract but it still is very reasonable to store the majority of your structured data in these in these forms likewise with unstructured data like documents like we said before it still probably makes sense to ENC to you know store documents independently but just simply aim to retrieve full documents rather than

worrying about these idiosyncratic parameters like like chunk size um and along those lines there's a lot of methods out there we've we've done a few of these that are kind of well optimized for document retrieval so one I want a flag is what we call multi repesent presentation indexing and there's actually a really nice paper on this called dense X retriever or proposition indexing but the main point is simply this would you do is you take your OD document you produce a representation

like a summary of that document you index that summary right and then um at retrieval time you ask your question you embed your question and you simply use a highle summary to just retrieve the right document you pass the full document to the LM for a kind of final generation so it's kind of a trick where you don't have to worry about embedding full documents in this particular case you can use kind of very nice descriptive summarization prompts to build descriptive summaries and the problem you're solving here is just get

me the right document it's an easier problem than get me the right chunk so this is kind of a nice approach it there's also different variants of it which I share below one is called parent document retriever where you could use in principle if you wanted smaller chunks but then just return full documents but anyway the point is preserving full documents for Generation but using representations like summaries or chunks for retrieval so that's kind of like approach one that I think is

really interesting approach two is this idea of raptor is a cool paper came out of Stamper somewhat recently and this solves the problem of what if for certain questions I need to integrate information across many documents so what this approach does is it takes documents and it it embeds them and clusters them and then it summarizes each cluster um and it does this recursively in up with only one very high level summary for the entire Corpus of documents and what they do is they take this kind of this abstraction

hierarchy so to speak of different document summarizations and they just index all of it and they use this in retrieval and so basically if you have a question that draws an information across numerous documents you probably have a summary present and and indexed that kind of has that answer captured so it's a nice trick to consolidate information across documents um they they paper actually reports you know these documents in their case or the leavs are actually document chunks or slices but I actually showed I have a

video on it and a notebook that this works across full documents as well um and this and I segue into to do this you do need to think about long context embedding models because you're embedding full documents and that's a really interesting thing to track um the you know hazy research uh put out a really nice um uh blog post on this using uh what the Monch mixer so it's kind of a new architecture that tends to longer context they have a 32,000 token embedding model that's pres that's

available on together AI absolutely worth experimenting with I think this is really interesting Trend so long long Contex and beddings kind of play really well with this kind of idea you take full documents embed them using for example long Contex embedding models and you can kind of build these document summarization trees um really effectively so I think this another nice trick for working with full documents in the long context kind of llm regime um one other thing I'll note I think there's also going to Mo be move away

from kind of single shot rag well today's rag we typically you know we chunk documents uh uh embed them store them in an index you know do retrieval and then do generation but there's no reason why you shouldn't kind of do reasoning on top of the generation or reasoning on top of the retrieval and feedback if there are errors so there's a really nice paper called selfrag um that kind of reports this we implemented this using Lang graph works really well and the simp the idea is simply to you

know grade the relevance of your documents relative to your question first if they're not relevant you rewrite the question you can do you can do many things in this case we do question rewriting and try again um we also grade for hallucinations we grade for answer relevance but anyway it kind of moves rag from like a single shot Paradigm to a kind of a cyclic flow uh in which you actually do various gradings Downstream and this is all relev in the long context llm regime as well in fact you know it you you

absolutely should take advantage of of for example increasingly fast and Performing LMS to do this grading um Frameworks like langra allow you to build these kind of these flows which build which allows you to kind of have a more performant uh kind of kind of self-reflective rag pipeline now I did get a lot of questions about latency here and I completely agree there's a trade-off between kind of performance accuracy and latency that's present here I think the real answer is you can opt

to use very fast uh for example models like grock where seeing um you know gp35 turbos very fast these are fairly easy grading challenges so you can use very very fast LMS to do the grading and for example um you you can also restrict this to only do one turn of of kind of cyclic iteration so you can kind of restrict the latency in that way as well so anyway I think it's a really cool approach still relevant in the world as we move towards longer context so it's kind of like building reasoning on top

of rag um in the uh generation and retrieval stages and a related point one of the challenges with rag is that your index for example you you may have a question that is that asks something that's outside the scope of your index and this is kind of always a problem so a really cool paper called c c rag or corrective rag came out you know a couple months ago that basically does grading just like we talked about before and then if the documents are not relevant you kick off and do a web search and basically return the search

results to the LM for final generation so it's a nice fallback in cases where um your you the questions out of the domain of your retriever so you know again nice trick overlaying reasoning on top of rag I think this trend you know continues um because you know it it just it makes rag systems you know more performant uh and less brittle to questions that are out of domain so you know you know that's another kind of nice idea this particular approach also we showed works really well with with uh

with open source models so I ran this with mraw 7B it can run locally on my laptop using a llama so again really nice approach I encourage you to look into this um and this is all kind of independent of the llm kind of context length these are reasoning you can add on top of the retrieval stage that that can kind of improve overall performance and so the overall picture kind of looks like this where you know I think that the the the the problem of routing your question to the right database Andor to

the right document kind of remains in place query analysis is still quite relevant routing is still relevant query construction is still relevant um in the long Contex regime I think there is less of an emphasis on document chunking working with full documents is probably kind of more parto optimal so to speak um there's some some clever tricks for IND indexing of documents like the multi-representation indexing we talked about the hierarchical indexing using Raptor that we talked about as well are

two interesting ideas for document Centric indexing um and then kind of reasoning in generation post retrieval on retrieval itself tog grade on the generations themselves checking for hallucinations those are all kind of interesting and relevant parts of a rag system that I think we'll probably will see more and more of as we move more away from like a more naive prompt response Paradigm more to like a flow Paradigm we're seeing that actually already in codenation it's probably going to carry over to rag as well where

we kind of build rag systems that have kind of a cyclic flow to them operate on documents use longc Comics llms um and still use kind of routing and query analysis so reasoning pre- retrieval reasoning post- retrieval so anyway that was kind of my talk um and yeah feel free to leave any comments on the video and I'll try to answer any questions but um yeah that's that's probably about it thank you

